{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !sudo pip install -U nltk\n",
    "# !sudo pip install wget\n",
    "# !sudo pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda, Inc.| (default, Oct 13 2017, 12:02:49) \n",
      "[GCC 7.2.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arunima/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "# Install a few python packages using pip\n",
    "#from common import utils\n",
    "from common import utils\n",
    "utils.require_package('nltk')\n",
    "utils.require_package(\"wget\")      # for fetching dataset\n",
    "#from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arunima/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys, time\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "#comment or uncomment based on anamika/ arunima\n",
    "# Helper libraries\n",
    "# from common import utils, vocabulary, glove_helper\n",
    "\n",
    "# from common import utils, vocabulary\n",
    "from common import utils, vocabulary\n",
    "from common import glove_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to read the amazon review data files\n",
    "def parse(path):\n",
    "  print('start parse')\n",
    "  start_parse = time.time()\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "  end_parse = time.time()\n",
    "  print('end parse with time for parse',end_parse - start_parse)\n",
    "\n",
    "def getDF(path):\n",
    "  print('start getDF')\n",
    "  start = time.time()\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  print('end getDF')\n",
    "  end = time.time()\n",
    "  print('time taken to load data = ',end-start)\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "#df = getDF('reviews_Toys_and_Games.json.gz') #old def function corresponding to the step bt step vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Using pretrained GLove embeddings\n",
    "hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available\n",
    "hands.shape\n",
    "print(hands.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please note that i had to comment out the path. Please uncomment before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 120.41323447227478\n",
      "end getDF\n",
      "time taken to load data =  120.41351127624512\n"
     ]
    }
   ],
   "source": [
    "#df_toys = getDF('/newvolume/reviews_Toys_and_Games.json.gz')\n",
    "df_toys = getDF('reviews_Toys_and_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 78.07821440696716\n",
      "end getDF\n",
      "time taken to load data =  78.07865905761719\n"
     ]
    }
   ],
   "source": [
    "#df_vid = getDF('/newvolume/reviews_Video_Games.json.gz')\n",
    "df_vid = getDF('reviews_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 67.88917350769043\n",
      "end getDF\n",
      "time taken to load data =  67.8895890712738\n"
     ]
    }
   ],
   "source": [
    "#df_aut = getDF('/newvolume/reviews_Automotive.json.gz')\n",
    "df_aut = getDF('reviews_Automotive.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 229.32517433166504\n",
      "end getDF\n",
      "time taken to load data =  229.3255169391632\n"
     ]
    }
   ],
   "source": [
    "df_hnk = getDF('reviews_Home_and_Kitchen.json.gz')\n",
    "#df_hnk = getDF('/newvolume/reviews_Home_and_Kitchen.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Home and Kitchen reviews examples\n",
      "\n",
      "A210NOCSTBT4OD\n",
      "Have you ever thought about how you met your best friend? Was it normal, or was it wacky - like how Elias met Shohei? Pulling a boa constrictor snake named Mathilda out of your backpack can make a remarkable first impression! This book is about three best friends Elias, Honoria, and Shohei, who are united against \"That Which Is The Peshtigo School\". Their goal is to make it through the annual school science fair, but things don't always go as planned.Elias is part of a family made up of science fanatics who would do anything to win a science fair. Elias isn't exactly what you'd call the ambitious type, especially when it comes to science fairs. So he becomes like Galileo and \"retests\" one of his sibling's past projects. Honoria loves to be ambitious, especially when it comes to being a legal counsel extraordinaire. But when she faces a bigger challenge than beating Goliath Reed or getting a piranha to become vegetarian, she doesn't know if she can make it. Shohei is an all around slacker who tries to mooch off Elias instead of creating something on his own. His adoptive parents are constantly encouraging him to start \"hearing\" his ancestors. His mom has even turned Shohei's room into what looks like a walk-in Japanese museum exhibit!This book is laugh out loud hilarious and the more you read, the more exciting and unexpected it gets. I love the title on this book because it really made me laugh and want to read the book. I also like how people so different from one another can be such close friends. There is not much excitement in the beginning, but it builds up very quickly. So if you like that type of story, then this is the book for you.\n",
      "A28ILV4TOG8BH2\n",
      "The butter dish is serving us well, and keeping the butter fresh and healthy. Couldn't be happier with it, and the color is a pleasing green.\n"
     ]
    }
   ],
   "source": [
    "#Looking at a few examples of review text\n",
    "# print('Toys reviews examples\\n')\n",
    "# for i in range(1):\n",
    "#     print(df_toys['reviewerID'].iloc[i])\n",
    "#     print(df_toys['reviewText'].iloc[i])\n",
    "\n",
    "# print('\\n Video games reviews examples\\n')\n",
    "# for i in range(1):\n",
    "#     print(df_vid['reviewerID'].iloc[i])\n",
    "#     print(df_vid['reviewText'].iloc[i])\n",
    "    \n",
    "# print('\\n Automobile reviews examples\\n')\n",
    "# for i in range(1):\n",
    "#     print(df_aut['reviewerID'].iloc[i])\n",
    "#     print(df_aut['reviewText'].iloc[i])\n",
    "    \n",
    "print('\\n Home and Kitchen reviews examples\\n')\n",
    "for i in range(2):\n",
    "    print(df_hnk['reviewerID'].iloc[i])\n",
    "    print(df_hnk['reviewText'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy reviews train, dev and test set dataframe shape: (1351662, 9) (450554, 9) (450555, 9)\n",
      "Video games reviews train, dev and test set dataframe shape: (794851, 9) (264951, 9) (264951, 9)\n",
      "Auto reviews train, dev and test set dataframe shape: (824260, 9) (274754, 9) (274754, 9)\n",
      "Home and Kitchen reviews train, dev and test set dataframe shape: (2552355, 9) (850785, 9) (850786, 9)\n"
     ]
    }
   ],
   "source": [
    "# Create train,dev,test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_toys,devtest = train_test_split(df_toys, test_size=0.4, random_state=42)\n",
    "dev_toys,test_toys = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Toy reviews train, dev and test set dataframe shape:',train_toys.shape,dev_toys.shape,test_toys.shape)\n",
    "\n",
    "#For Video games reviews\n",
    "train_vid,devtest = train_test_split(df_vid, test_size=0.4, random_state=42)\n",
    "dev_vid,test_vid = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Video games reviews train, dev and test set dataframe shape:',train_vid.shape,dev_vid.shape,test_vid.shape)\n",
    "\n",
    "#For Auto reviews\n",
    "train_aut,devtest = train_test_split(df_aut, test_size=0.4, random_state=42)\n",
    "dev_aut,test_aut = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Auto reviews train, dev and test set dataframe shape:',train_aut.shape,dev_aut.shape,test_aut.shape)\n",
    "\n",
    "#For Home and Kitchen reviews\n",
    "train_hnk,devtest = train_test_split(df_hnk, test_size=0.4, random_state=42)\n",
    "dev_hnk,test_hnk = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Home and Kitchen reviews train, dev and test set dataframe shape:',train_hnk.shape,dev_hnk.shape,test_hnk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to create a smaller sized train and dev data set. Enables testing accuracy for different sizes.\n",
    "#Also binarizes the labels. Ratings of 1,2 and to 0; Ratings of 4,5 to 1.\n",
    "\n",
    "def set_df_size(size,data_train,data_dev):\n",
    "    size_train = size\n",
    "    len_max_train = data_train[data_train.overall!=3].shape[0] #max possible length of train data set taking out the 3 ratings.\n",
    "    #print(\"Number of reviews with ratings != 3 in train set\",len_max_train)\n",
    "    temp_size_train = min(len_max_train,size_train)\n",
    "\n",
    "    len_max_dev = data_dev[data_dev.overall!=3].shape[0]\n",
    "    #print(\"Number of reviews with ratings != 3 in dev set\",len_max_dev)\n",
    "    temp_size_dev = min(len_max_dev,int(0.3*temp_size_train)) #making the dev set about 0.3 times the train set.\n",
    "\n",
    "    temp_train_data = data_train[data_train.overall != 3][:temp_size_train]\n",
    "    #print('Size of train data',temp_train_data.shape)\n",
    "    #print(temp_train_data.groupby('overall').count())\n",
    "    #print(temp_train_toys[:5])\n",
    "\n",
    "    temp_dev_data = data_dev[data_dev.overall!=3][:temp_size_dev]\n",
    "    #print('Size of dev data',temp_dev_data.shape)\n",
    "    #print(temp_dev_data.groupby('overall').count())\n",
    "    #print(temp_dev_data[:2])\n",
    "    \n",
    "    #Binarize ratings\n",
    "    temp_train_y = np.zeros(temp_size_train)\n",
    "    temp_train_y[temp_train_data.overall > 3] = 1\n",
    "    temp_dev_y = np.zeros(temp_size_dev)\n",
    "    temp_dev_y[temp_dev_data.overall>3] = 1\n",
    "    #print('binarized y shape',temp_train_y.shape,temp_dev_y.shape)\n",
    "    #print(temp_dev_y[:20],data_dev.overall[:20])\n",
    "    return temp_train_data,temp_dev_data,temp_train_y,temp_dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] #list of keys that refer to each dataframe. Adding a new dataframe would require updating this list\n",
    "dict_train_df = {} #Dict to store train input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_dev_df = {} #Dict to store dev input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_train_y = {} #Dict to store binarized train data label for each domain\n",
    "dict_dev_y = {} #Dict to store binarized dev data label for each domain\n",
    "#print(len(dict_train_df))\n",
    "\n",
    "size_initial = 10000\n",
    "def create_sized_data(size = 10000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "    key = list_df[0]\n",
    "    #print('Toys reviews\\n')\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "    #print('\\n Video games reviews\\n')\n",
    "    key = list_df[1]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_vid,dev_vid)\n",
    "    #print('\\n Auto reviews\\n')\n",
    "    key = list_df[2]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_aut,dev_aut)\n",
    "    #print('\\n Home and Kitchen reviews\\n')\n",
    "    key = list_df[3]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_hnk,dev_hnk)\n",
    "    \n",
    "create_sized_data(size_initial)\n",
    "#create_sized_data(500)\n",
    "#print(len(dict_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab_processor = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=0)\n",
    "#Note : Above function was used instead of the below, which is deprecated. \n",
    "# vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_length)\n",
    "\n",
    "def process_inputs(key, vocab_processor):\n",
    "    \n",
    "    start_vectorize = time.time()\n",
    "    x_train = dict_train_df[key].reviewText\n",
    "    y_train = dict_train_y[key]\n",
    "    x_dev = dict_dev_df[key].reviewText\n",
    "    y_dev = dict_dev_y[key]\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    # Train the vocab_processor from the training set\n",
    "    x_train = vocab_processor.fit_transform(x_train)\n",
    "    # Transform our test set with the vocabulary processor\n",
    "    x_dev = vocab_processor.transform(x_dev)\n",
    "\n",
    "    # We need these to be np.arrays instead of generators\n",
    "    x_train = np.array(list(x_train))\n",
    "    print(x_train.shape)\n",
    "    x_dev = np.array(list(x_dev))\n",
    "    y_train = np.array(y_train).astype(int)\n",
    "    y_dev = np.array(y_dev).astype(int)\n",
    "    \n",
    "#     y_train = tf.expand_dims(y_train,1)\n",
    "#     y_dev = tf.expand_dims(y_dev,1)\n",
    "    print('y train shape',y_train.shape)\n",
    "\n",
    "    V = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % V)\n",
    "    end_vectorize = time.time()\n",
    "    print('Time taken to vectorize %d size dataframe'%x_train.shape[0],end_vectorize-start_vectorize)\n",
    "\n",
    "    # Return the transformed data and the number of words\n",
    "    return x_train, y_train, x_dev, y_dev, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 13807\n",
      "Time taken to vectorize 10000 size dataframe 1.8203957080841064\n",
      "Number words in training corpus for toys 13807\n",
      "toys dataset id shapes (10000, 150) (3000, 150)\n",
      "sample review for domain toys It's just the model I was hoping for and more. It's challenging and has given me something more to learn in developing my model building skills. Revell as always makes good models to build! \n",
      "\n",
      "corresponding ids\n",
      " [ 111   38    1  319    6   14 1067    8    2   51  111  755    2   42  577\n",
      "   97  225   51    3  366   12 4337   15  319  535  858 5014   22  272  211\n",
      "   61  782    3  389    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0] \n",
      "\n",
      "vid\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 19754\n",
      "Time taken to vectorize 10000 size dataframe 2.826982259750366\n",
      "Number words in training corpus for vid 19754\n",
      "vid dataset id shapes (10000, 150) (3000, 150)\n",
      "sample review for domain vid First things first. I love the Lego games. So, my view on this game is skewed. This game follows the typical Lego quality game with a slight change in game play. When playing with two players, the screen splits, which is great, because you don't have to drop out all the time. However, watch changing players or using Jack's compass when the screen is split because it will sometimes freeze up on you. If you like Legos, the other Lego games, or Pirates of the Carribean, then this game is for you. \n",
      "\n",
      "corresponding ids\n",
      " [  570   179    78     5    92     1  1256    28   184    24   993    19\n",
      "    11     9     6 11592    37     9  2450     1  1794  1256   286     9\n",
      "    15     4  1918   431    13     9    34   279    77    15   159   277\n",
      "     1   245 14137    71     6    44    79    10    80    17     3  1150\n",
      "    40    29     1    52   314   601  1842   277    33   272  7527  7074\n",
      "    60     1   245     6  2572    79     8    42   443  2310    47    19\n",
      "    10    84    10    26  8263     1    67  1256    28    33  4397     7\n",
      "     1 14937   100    11     9     6    12    10     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0] \n",
      "\n",
      "aut\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 12472\n",
      "Time taken to vectorize 10000 size dataframe 1.850517749786377\n",
      "Number words in training corpus for aut 12472\n",
      "aut dataset id shapes (10000, 150) (3000, 150)\n",
      "sample review for domain aut THE OLD LIGHTS WERE FOGGED OVER WITH THE NEW ONES MAKES DRIVING AT NIGHT A LOT SAFER A LOT BRIGHTER AND THEY WERE EASY TO INSTALL. \n",
      "\n",
      "corresponding ids\n",
      " [ 344 5134 6508 4347    0 4678 1581  344 2548 6545 6522 7553 1809 9951  162\n",
      " 2451    0  162 2451    0  542 1816 4347 2645  548 3322    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0] \n",
      "\n",
      "hnk\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 13029\n",
      "Time taken to vectorize 10000 size dataframe 2.06183123588562\n",
      "Number words in training corpus for hnk 13029\n",
      "hnk dataset id shapes (10000, 150) (3000, 150)\n",
      "sample review for domain hnk These barstools are amazing. After scouring stores and the internet, I decided that these would be my best bet. I needed sturdy stools to deal with heavy use from kids, family, ect. They are 1000% solid. When they arrive you have to screw the seat onto the base swivel mechanism, however all the important parts that make it solid are already in place. You also have to attach the solid wooden seat onto the frame. The seat isn't pre-drilled so that is a bit annoying. Not hard, but annoying. These stools are going to last a very long time, without a doubt. I wouldn't hesitate to buy more. They feel stronger, more solid, and of higher quality than stools I saw for $300 each. I was a little nervous they would look clunky, but they are beautiful with very nice lines. I wish I would have ordered them two months ago when I first saw them.I forgot to mention the box was delivered to my door within two days. Fastest shipping ever. \n",
      "\n",
      "corresponding ids\n",
      " [  194  7166    24   721   268 12476   728     3     1  2632     2   408\n",
      "    12    58    34    29    13   175  3224     2   222   237  2272     4\n",
      "   446    14   247    28    47   560   422  8749   106    24  3489   465\n",
      "   275    36  1979    19    15     4   975     1  1203   979     1   557\n",
      "  3184  1753   528    42     1  1115   539    12    79     6   465    24\n",
      "   429    11   273   212    92    15     4  1940     1   465  1042  1203\n",
      "   979     1   632    20  1203   434  7593    23    12     7     5   176\n",
      "  1455   306   248    18  1455   194  2272    24   213     4   197     5\n",
      "    27   159    53   173     5  1524     2   574  3249     4   102    50\n",
      "   106   311  2093    50   465     3     8  1006    96    49  2272     2\n",
      "   648     9  1872   322     2    16     5    69  2726    36    34   150\n",
      "  6326    18    36    24   343    14    27    89  2263     2   406     2\n",
      "    34    15   221    41   114   220] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting reviews to ids for all domains and add padding.\n",
    "\n",
    "# Hyperparameters\n",
    "min_frequency = 1\n",
    "max_length = 150\n",
    "\n",
    "dict_vectorizers = {} #Dict to store the vocab_processor fit on each domain\n",
    "dict_train_ids = {} #Dict to store train data reviews as sparse matrix of word ids\n",
    "dict_dev_ids = {} #Dict to store dev data reviews as sparse matrix of word ids\n",
    "dict_cnn = {} #Dict to store cnn model developed on each domain. Assumes input features are developed using the corresponding count_vectorizer\n",
    "dict_dev_ypred = {} #Dict to store dev predictions\n",
    "dict_vocab_len = {} #Store vocab length of each domain\n",
    "for key in list_df:\n",
    "    \n",
    "    #Converting ratings to tokenized word id counts as a sparse matrix using count_vectorizer\n",
    "    dict_vectorizers[key] = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=min_frequency)\n",
    "    print(key)\n",
    "    dict_train_ids[key], dict_train_y[key],dict_dev_ids[key], dict_dev_ypred[key], dict_vocab_len[key] = process_inputs(key,dict_vectorizers[key])\n",
    "    \n",
    "    print(\"Number words in training corpus for\",key,(dict_vocab_len[key]))\n",
    "    print(key,'dataset id shapes',dict_train_ids[key].shape, dict_dev_ids[key].shape)\n",
    "\n",
    "    #Print a few examples for viewing\n",
    "    print('sample review for domain',key, dict_train_df[key].reviewText.iloc[3],'\\n')\n",
    "    print('corresponding ids\\n',dict_train_ids[key][3],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys (100000,)\n",
      "(array([58987, 24503,  8678,  3490,  1674,   922,   556,   370,   242,\n",
      "         159,    95,    68,    57,    43,    38,    31,    20,    10,\n",
      "           8,    49]), array([    0.,    50.,   100.,   150.,   200.,   250.,   300.,   350.,\n",
      "         400.,   450.,   500.,   550.,   600.,   650.,   700.,   750.,\n",
      "         800.,   850.,   900.,   950.,  1000.]))\n",
      "Number less than 100 83712\n",
      "Number less than 150 92223\n",
      "Number less than 175 94288\n",
      "Number less than 200 95652\n",
      "vid (100000,)\n",
      "(array([47996, 21688, 10292,  5920,  3620,  2440,  1692,  1282,   994,\n",
      "         680,   601,   459,   377,   307,   253,   194,   183,   143,\n",
      "         127,   752]), array([    0.,    50.,   100.,   150.,   200.,   250.,   300.,   350.,\n",
      "         400.,   450.,   500.,   550.,   600.,   650.,   700.,   750.,\n",
      "         800.,   850.,   900.,   950.,  1000.]))\n",
      "Number less than 100 69913\n",
      "Number less than 150 80111\n",
      "Number less than 175 83353\n",
      "Number less than 200 85960\n",
      "aut (100000,)\n",
      "(array([63741, 22809,  7245,  2918,  1404,   677,   456,   216,   166,\n",
      "         103,    87,    43,    27,    27,    20,    12,    10,     8,\n",
      "           5,    26]), array([   0.  ,   49.85,   99.7 ,  149.55,  199.4 ,  249.25,  299.1 ,\n",
      "        348.95,  398.8 ,  448.65,  498.5 ,  548.35,  598.2 ,  648.05,\n",
      "        697.9 ,  747.75,  797.6 ,  847.45,  897.3 ,  947.15,  997.  ]))\n",
      "Number less than 100 86739\n",
      "Number less than 150 93835\n",
      "Number less than 175 95556\n",
      "Number less than 200 96718\n",
      "hnk (100000,)\n",
      "(array([56142, 25500,  9051,  4115,  2030,  1087,   604,   433,   282,\n",
      "         185,   158,   116,    63,    47,    46,    22,    22,     9,\n",
      "          16,    72]), array([   0.  ,   49.95,   99.9 ,  149.85,  199.8 ,  249.75,  299.7 ,\n",
      "        349.65,  399.6 ,  449.55,  499.5 ,  549.45,  599.4 ,  649.35,\n",
      "        699.3 ,  749.25,  799.2 ,  849.15,  899.1 ,  949.05,  999.  ]))\n",
      "Number less than 100 81876\n",
      "Number less than 150 90765\n",
      "Number less than 175 93165\n",
      "Number less than 200 94837\n"
     ]
    }
   ],
   "source": [
    "#This code was used to pick max_length for all domains for the CNN, by using a sample of 100000, and a max_length of 10000 for analysis\n",
    "#it is not needed for running the CNN.\n",
    "# for key in list_df:\n",
    "#     length = np.count_nonzero(dict_train_ids[key],axis = 1)\n",
    "#     print(key,length.shape)\n",
    "#     print(np.histogram(length,bins = 20))\n",
    "#     print(\"Number less than 100\",np.count_nonzero(length[length <= 100]))\n",
    "#     print(\"Number less than 150\",np.count_nonzero(length[length <= 150]))\n",
    "#     print(\"Number less than 175\",np.count_nonzero(length[length <= 175]))\n",
    "#     print(\"Number less than 200\",np.count_nonzero(length[length <= 200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(y_train.shape)\n",
    "# print(y_dev.shape)\n",
    "# print(np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__( self, sequence_length, num_classes, vocab_size, learning_rate, momentum, embedding_size, \n",
    "                 gl_embed, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            #self.W = tf.get_variable(\"W_in\",[vocab_size, embedding_size],initializer =tf.random_uniform_initializer(0,1)) #from wildML\n",
    "            self.W=tf.get_variable(name=\"embedding_\",shape=gl_embed.shape,\n",
    "                                       initializer=tf.constant_initializer(gl_embed),trainable=True)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            #print('embedded_chars',self.embedded_chars.get_shape())\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            #print('embedded_chars_expanded',self.embedded_chars_expanded.get_shape())\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                #W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                Wname = \"w_%d\"%filter_size\n",
    "                W = tf.get_variable(Wname, shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "                b = tf.Variable(tf.constant(0.0, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d( self.embedded_chars_expanded, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "\n",
    "                # Apply nonlinearity\n",
    "                conv+= b\n",
    "                h = tf.nn.relu(conv, name=\"relu\")\n",
    "                #print('h',h.get_shape())\n",
    "\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1], strides=[1, 1, 1, 1],\n",
    "                    padding='VALID', name=\"pool\")\n",
    "                #print('pooled',pooled.get_shape())\n",
    "                pooled_outputs.append(pooled)\n",
    "                #print('pooled_outputs',type(pooled_outputs))\n",
    "                #print('pooled_outputs as array',type(np.array(pooled_outputs)),np.array(pooled_outputs).shape)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        #print('h_pool',self.h_pool.get_shape())\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        #print('h_pool_flat',self.h_pool_flat.get_shape())\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\"W\", shape=[num_filters_total, num_classes],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            #print('self.scores',self.scores.get_shape())\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            self.pred_proba = tf.nn.softmax(self.scores, name=\"pred_proba\")\n",
    "            #print('self.predictions',self.predictions.get_shape())\n",
    "            \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "            #self.loss = tf.losses.mean_squared_error(self.input_y, self.scores)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "#             correct_pred = tf.equal(tf.cast(tf.round(self.scores), tf.int32), self.input_y)\n",
    "#             self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "        # AUC\n",
    "#         with tf.name_scope(\"auc\"):\n",
    "#             false_pos_rate, true_pos_rate, _ = roc_curve(self.input_y, self.pred_proba[:,1])\n",
    "#             self.auc = auc(false_pos_rate, true_pos_rate)\n",
    "            \n",
    "            \n",
    "        with tf.name_scope('train'):\n",
    "            #self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            self.optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate,momentum=momentum,use_nesterov=True).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(ids, labels, batch_size=100, Trainable=False):\n",
    "            #ids is input, X_train\n",
    "            #need to fix this to shuffle between epochs\n",
    "            \n",
    "            n_batches = len(ids)//batch_size\n",
    "            ids, labels = ids[:n_batches*batch_size], labels[:n_batches*batch_size]\n",
    "            if Trainable:\n",
    "                shuffle = np.random.permutation(np.arange(n_batches*batch_size))\n",
    "                ids, labels = ids[shuffle], labels[shuffle]\n",
    "   \n",
    "            for ii in range(0, len(ids), batch_size):\n",
    "                yield ids[ii:ii+batch_size], labels[ii:ii+batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "#embed_dim = 50 #use when not using pre-trained embeddings\n",
    "embed_dim = hands.shape[1]\n",
    "filter_sizes= [3,4,5]\n",
    "num_filters = 256\n",
    "l2_reg_lambda = 0\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "keep_prob = 0.8\n",
    "evaluate_train = 2 # of epochs at which to print test accuracy\n",
    "evaluate_dev = 2 # of epochs at which to estimate and print dev accuracy\n",
    "time_print = 4 # of epochs at which to print time taken\n",
    "num_classes = 2\n",
    "num_epochs = 11\n",
    "#num_checkpoints = 2\n",
    "#batch_size = 64\n",
    "batch_size=128\n",
    "\n",
    "# out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnn\"))\n",
    "# print(\"Model saving  to {}\\n\".format(out_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Actual training loop:\n",
    "\n",
    "def train_cnn(key, size=5000):\n",
    "     \n",
    "    x_train = dict_train_ids[key]\n",
    "    y_train = dict_train_y[key]\n",
    "    x_dev = dict_dev_ids[key]\n",
    "    y_dev = dict_dev_ypred[key]\n",
    "    V = dict_vocab_len[key]\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "        \n",
    "            cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('completed cnn creation')\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            size_folder =  \"size_\" + str(size) \n",
    "            out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key, size_folder))\n",
    "            #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key))\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            model_name = key \n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, model_name  + \"_model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            # Write vocabulary\n",
    "            ## vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "            \n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                \n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=True), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                   # _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    #avg_auc = total_auc/(batch_size*(len(x_train)//batch_size))\n",
    "                   # print(\"Train epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    \n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                    \n",
    "                    print('y_dev.shape',y_dev.shape)\n",
    "                    print('y_shuffled.shape',y_shuffled.shape)\n",
    "                    \n",
    "                    if np.array_equal(y_shuffled,y_dev):\n",
    "                        print(\"Yes\")\n",
    "                    right_acc = total_batch_acc/(num_batches)\n",
    "                    #avg_auc = total_auc/(num_batches*batch_size)\n",
    "                    \n",
    "                    #Calculate Accuracy\n",
    "                    new_acc = accuracy_score(y_shuffled, y_pred, normalize=True ) \n",
    "                     \n",
    "                    \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "                    print(\"\\t\\tDev epoch {}, auc {:g}, new accuracy {:g}, right accuracy {:g},\".format(e,  roc_auc, new_acc, right_acc))\n",
    "                    #print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                    \n",
    "                    \n",
    "        # Save model weights for future use.\n",
    "        \n",
    "        \n",
    "            #save_path = saver.save(sess, checkpoint_prefix, global_step=20,write_meta_graph=False)\n",
    "            save_path = saver.save(sess, checkpoint_prefix)\n",
    "            print(\"Saved model\", model_name, save_path)\n",
    "            \n",
    "            #calculate predictions and prediction probability    \n",
    "#             feed_dict={cnn.input_x:x_dev, cnn.input_y: y_dev, cnn.dropout_keep_prob: 1.0}\n",
    "#             y_pred, y_pred_proba = sess.run([cnn.predictions, cnn.pred_proba],feed_dict)\n",
    "            #print(y_pred, y_pred_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please make sure you change the size below so that the source model is saved with that name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys 10000\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.407695, average accuracy 0.856671,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 0, average loss 0.397615, average accuracy 0.857337,\n",
      "\t\tDev epoch 0, auc 0.692604, new accuracy 0.857337, right accuracy 0.857337,\n",
      "\t\t\t\t    Time taken for 0 epochs =  88.72469019889832\n",
      "Train epoch 2, average loss 0.339955, average accuracy 0.861879,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 2, average loss 0.354773, average accuracy 0.858356,\n",
      "\t\tDev epoch 2, auc 0.772811, new accuracy 0.858356, right accuracy 0.858356,\n",
      "Train epoch 4, average loss 0.277497, average accuracy 0.883814,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 4, average loss 0.314543, average accuracy 0.868886,\n",
      "\t\tDev epoch 4, auc 0.836851, new accuracy 0.868886, right accuracy 0.868886,\n",
      "\t\t\t\t    Time taken for 4 epochs =  431.0676200389862\n",
      "Train epoch 6, average loss 0.217605, average accuracy 0.915765,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 6, average loss 0.291369, average accuracy 0.879416,\n",
      "\t\tDev epoch 6, auc 0.86581, new accuracy 0.879416, right accuracy 0.879416,\n",
      "Train epoch 8, average loss 0.172885, average accuracy 0.936298,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 8, average loss 0.28275, average accuracy 0.88553,\n",
      "\t\tDev epoch 8, auc 0.879418, new accuracy 0.88553, right accuracy 0.88553,\n",
      "\t\t\t\t    Time taken for 8 epochs =  773.3590502738953\n",
      "Train epoch 10, average loss 0.126968, average accuracy 0.958634,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 10, average loss 0.260407, average accuracy 0.894022,\n",
      "\t\tDev epoch 10, auc 0.88848, new accuracy 0.894022, right accuracy 0.894022,\n",
      "Saved model toys /home/arunima/fproject/final_project/runs/toys/size_10000/checkpoints/toys_model\n",
      "vid 10000\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.481085, average accuracy 0.808694,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 0, average loss 0.468767, average accuracy 0.804348,\n",
      "\t\tDev epoch 0, auc 0.717131, new accuracy 0.804348, right accuracy 0.804348,\n",
      "\t\t\t\t    Time taken for 0 epochs =  89.90016770362854\n",
      "Train epoch 2, average loss 0.397901, average accuracy 0.819812,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 2, average loss 0.422566, average accuracy 0.827106,\n",
      "\t\tDev epoch 2, auc 0.796011, new accuracy 0.827106, right accuracy 0.827106,\n",
      "Train epoch 4, average loss 0.324281, average accuracy 0.857873,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 4, average loss 0.377617, average accuracy 0.831522,\n",
      "\t\tDev epoch 4, auc 0.832358, new accuracy 0.831522, right accuracy 0.831522,\n",
      "\t\t\t\t    Time taken for 4 epochs =  432.0004172325134\n",
      "Train epoch 6, average loss 0.258365, average accuracy 0.898137,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 6, average loss 0.374081, average accuracy 0.841033,\n",
      "\t\tDev epoch 6, auc 0.845446, new accuracy 0.841033, right accuracy 0.841033,\n",
      "Train epoch 8, average loss 0.204336, average accuracy 0.928185,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 8, average loss 0.346414, average accuracy 0.862092,\n",
      "\t\tDev epoch 8, auc 0.853468, new accuracy 0.862092, right accuracy 0.862092,\n",
      "\t\t\t\t    Time taken for 8 epochs =  773.740359544754\n",
      "Train epoch 10, average loss 0.160297, average accuracy 0.948317,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 10, average loss 0.350355, average accuracy 0.861413,\n",
      "\t\tDev epoch 10, auc 0.857013, new accuracy 0.861413, right accuracy 0.861413,\n",
      "Saved model vid /home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints/vid_model\n",
      "aut 10000\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.420475, average accuracy 0.845052,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 0, average loss 0.383425, average accuracy 0.853261,\n",
      "\t\tDev epoch 0, auc 0.778312, new accuracy 0.853261, right accuracy 0.853261,\n",
      "\t\t\t\t    Time taken for 0 epochs =  89.80394458770752\n",
      "Train epoch 2, average loss 0.329084, average accuracy 0.866386,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 2, average loss 0.335984, average accuracy 0.862432,\n",
      "\t\tDev epoch 2, auc 0.809478, new accuracy 0.862432, right accuracy 0.862432,\n",
      "Train epoch 4, average loss 0.278992, average accuracy 0.884515,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 4, average loss 0.321656, average accuracy 0.876698,\n",
      "\t\tDev epoch 4, auc 0.837971, new accuracy 0.876698, right accuracy 0.876698,\n",
      "\t\t\t\t    Time taken for 4 epochs =  432.1426250934601\n",
      "Train epoch 6, average loss 0.230969, average accuracy 0.90615,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 6, average loss 0.330933, average accuracy 0.862092,\n",
      "\t\tDev epoch 6, auc 0.856895, new accuracy 0.862092, right accuracy 0.862092,\n",
      "Train epoch 8, average loss 0.186455, average accuracy 0.931791,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 8, average loss 0.285917, average accuracy 0.888247,\n",
      "\t\tDev epoch 8, auc 0.869524, new accuracy 0.888247, right accuracy 0.888247,\n",
      "\t\t\t\t    Time taken for 8 epochs =  772.8209567070007\n",
      "Train epoch 10, average loss 0.144084, average accuracy 0.95002,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 10, average loss 0.293664, average accuracy 0.884171,\n",
      "\t\tDev epoch 10, auc 0.873596, new accuracy 0.884171, right accuracy 0.884171,\n",
      "Saved model aut /home/arunima/fproject/final_project/runs/aut/size_10000/checkpoints/aut_model\n",
      "hnk 10000\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.459978, average accuracy 0.816106,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 0, average loss 0.426852, average accuracy 0.829144,\n",
      "\t\tDev epoch 0, auc 0.759018, new accuracy 0.829144, right accuracy 0.829144,\n",
      "\t\t\t\t    Time taken for 0 epochs =  87.72756123542786\n",
      "Train epoch 2, average loss 0.367804, average accuracy 0.835938,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 2, average loss 0.367914, average accuracy 0.831861,\n",
      "\t\tDev epoch 2, auc 0.835128, new accuracy 0.831861, right accuracy 0.831861,\n",
      "Train epoch 4, average loss 0.298516, average accuracy 0.8749,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 4, average loss 0.361282, average accuracy 0.835258,\n",
      "\t\tDev epoch 4, auc 0.869292, new accuracy 0.835258, right accuracy 0.835258,\n",
      "\t\t\t\t    Time taken for 4 epochs =  424.5644464492798\n",
      "Train epoch 6, average loss 0.235001, average accuracy 0.906851,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 6, average loss 0.298647, average accuracy 0.87466,\n",
      "\t\tDev epoch 6, auc 0.890569, new accuracy 0.87466, right accuracy 0.87466,\n",
      "Train epoch 8, average loss 0.184239, average accuracy 0.934195,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 8, average loss 0.303014, average accuracy 0.872283,\n",
      "\t\tDev epoch 8, auc 0.899576, new accuracy 0.872283, right accuracy 0.872283,\n",
      "\t\t\t\t    Time taken for 8 epochs =  761.0149607658386\n",
      "Train epoch 10, average loss 0.141607, average accuracy 0.955629,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 10, average loss 0.275137, average accuracy 0.887568,\n",
      "\t\tDev epoch 10, auc 0.904862, new accuracy 0.887568, right accuracy 0.887568,\n",
      "Saved model hnk /home/arunima/fproject/final_project/runs/hnk/size_10000/checkpoints/hnk_model\n"
     ]
    }
   ],
   "source": [
    "#Create and train the cnn models for all 4 domains\n",
    "#Pass the size to save the model name with size in different folders\n",
    "\n",
    "size_train = size_initial\n",
    "for key in list_df:\n",
    "    print(key, size_train)\n",
    "    train_cnn(key, size=size_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below cell is just for testing. Will be deleted later. Function in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys\n",
      "500\n",
      "/home/reachanamikasinha/project/runs/toys/size_500/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/size_500/checkpoints/toys_model\n",
      "0.886666666667\n",
      "auc 51.30%\n",
      "0.886666666667\n",
      "accuracy 88.67%\n",
      "Saved file successfully\n"
     ]
    }
   ],
   "source": [
    "key = 'toys'\n",
    "size=500\n",
    "#size =10000\n",
    "batch_size=50#Do not change batch size for predicting.Will lead to incorrect length of y_pred_proba that gets saved.\n",
    "print(key)\n",
    "print(size)\n",
    "V = dict_vocab_len[key]\n",
    "\n",
    "\n",
    "size_folder =  \"size_\" + str(size) \n",
    "out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key, size_folder))\n",
    "#out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key))\n",
    "\n",
    "\n",
    "checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "\n",
    "#checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "print(checkpoint_dir)\n",
    "model_name = key\n",
    "\n",
    "graph_meta_file = checkpoint_dir + '/' + 'toys_model.meta'\n",
    "\n",
    "graph=tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "      #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "        new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "  \n",
    "\n",
    "        \n",
    "    \n",
    "        x_dev = dict_dev_ids[key]\n",
    "        #print(x_dev[0])\n",
    "        #print(dict_dev_ids['vid'][0])\n",
    "        y_dev = dict_dev_ypred[key]\n",
    "        \n",
    "        #print(x_dev)\n",
    "        #print(y_dev)\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "        \n",
    "        pred_proba = graph.get_operation_by_name(\"output/pred_proba\").outputs[0]\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "        \n",
    "        y_pred = []\n",
    "        y_pred_proba = []\n",
    "        total_batch_acc = 0\n",
    "        num_batches = 0\n",
    "        y_shuffled = []\n",
    "        abs_y_pred_proba = []\n",
    "        for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                feed_dict = {input_x: x, input_y: y, dropout_keep_prob: 1.0}\n",
    "                batch_pred, batch_pred_proba  = sess.run([ predictions, pred_proba],feed_dict)\n",
    "                batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                total_batch_acc += batch_accuracy\n",
    "                y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                abs_y_pred_proba = np.concatenate([abs_y_pred_proba,np.absolute(batch_pred_proba[:,1] - batch_pred_proba[:,0])])\n",
    "                \n",
    "                y_shuffled = np.concatenate([y_shuffled, y])\n",
    "\n",
    "                num_batches += 1       \n",
    "        #y_pred = np.array(y_pred_list)         \n",
    "              \n",
    "    \n",
    "        new_acc = total_batch_acc/(num_batches)\n",
    "        print(new_acc)\n",
    "        \n",
    "        #Calculate auc\n",
    "#         false_pos_rate, true_pos_rate, _ = roc_curve(y_dev, y_pred_proba[:,1])\n",
    "        false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "        roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "        print(\"auc\",\"{:.02%}\".format(roc_auc))\n",
    "            \n",
    "        #Calculate Accuracy\n",
    "        acc = accuracy_score(y_shuffled, y_pred, normalize=True )\n",
    "        print(np.sum(y_shuffled==y_pred)/y_pred.shape[0])\n",
    "        print(\"accuracy\",\"{:.02%}\".format(acc))\n",
    "        \n",
    "        #Save absolute_y_pred_proba\n",
    "        \n",
    "        np.savez_compressed('test_file',pred_prob=abs_y_pred_proba)\n",
    "        print(\"Saved file successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[ 0.68332374  0.7064476   0.73289293  0.67621136  0.82931405]\n"
     ]
    }
   ],
   "source": [
    "#Testing loading a saved np file \n",
    "\n",
    "loaded = np.load('test_file.npz')\n",
    "print(np.array_equal(abs_y_pred_proba, loaded[\"pred_prob\"]))\n",
    "print(loaded[\"pred_prob\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_accuracy(src_key, size, tar_key):\n",
    "    \n",
    "    batch_size=50\n",
    "    print('target',tar_key)\n",
    "    print('source', src_key)\n",
    "    V = dict_vocab_len[tar_key]\n",
    "    \n",
    "    size_folder =  \"size_\" + str(size) \n",
    "    out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key, size_folder))\n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key))\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "\n",
    "    \n",
    "    print(checkpoint_dir)\n",
    "    src_model = src_key\n",
    "    #graph_meta_file = checkpoint_dir + '/' + 'hnk01_model.meta'\n",
    "    graph_meta_file = checkpoint_dir + '/' + src_model +'_model.meta'\n",
    "    graph=tf.Graph()\n",
    "\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "    \n",
    "      #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "            new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "            new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    \n",
    "            x_dev = dict_dev_ids[tar_key]      \n",
    "            y_dev = dict_dev_ypred[tar_key]\n",
    "        \n",
    "            #create graph from saved model\n",
    "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "            input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "        \n",
    "            pred_proba = graph.get_operation_by_name(\"output/pred_proba\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "        \n",
    "            y_pred = []\n",
    "            y_pred_proba = []\n",
    "            total_batch_acc = 0\n",
    "            num_batches = 0\n",
    "            y_shuffled = []\n",
    "            abs_y_pred_proba = []\n",
    "            for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                feed_dict = {input_x: x, input_y: y, dropout_keep_prob: 1.0}\n",
    "                batch_pred, batch_pred_proba  = sess.run([ predictions, pred_proba],feed_dict)\n",
    "                batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                total_batch_acc += batch_accuracy\n",
    "                y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                abs_y_pred_proba = np.concatenate([abs_y_pred_proba,np.absolute(batch_pred_proba[:,1] - batch_pred_proba[:,0])])\n",
    "                y_shuffled = np.concatenate([y_shuffled, y])\n",
    "\n",
    "                num_batches += 1       \n",
    "            #y_pred = np.array(y_pred_list)         \n",
    "              \n",
    "            new_acc = total_batch_acc/(num_batches)\n",
    "            print(new_acc)\n",
    "        \n",
    "            # Calculate auc\n",
    "            # false_pos_rate, true_pos_rate, _ = roc_curve(y_dev, y_pred_proba[:,1])\n",
    "            false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "            roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "            print(src_key, tar_key, \"AUC\",\"{:.02%}\".format(roc_auc))\n",
    "            \n",
    "            #Calculate Accuracy\n",
    "            acc = accuracy_score(y_shuffled, y_pred, normalize=True )\n",
    "            #print(np.sum(y_shuffled==y_pred)/y_pred.shape[0])\n",
    "            print('source',src_key, 'target',tar_key, \"accuracy\",\"{:.02%}\".format(acc))\n",
    "            print(\"\")\n",
    "        \n",
    "        #Save absolute_y_pred_proba\n",
    "        \n",
    "        #check if the batching process left remainders. This will result in incorrect length of y_pred_proba saved\n",
    "        if y_dev.shape[0] != abs_y_pred_proba.shape[0]:\n",
    "            print(\"Length of y_pred_proba does not match y_dev. Fix batch_size\")\n",
    "            print(\"Pred proba file not saved\")\n",
    "        else:    \n",
    "            file_name = \"src_\" + src_key + \"_tar_\" + tar_key + \"_\" + str(y_dev.shape[0])\n",
    "            np.savez_compressed('test_file',pred_prob=abs_y_pred_proba)\n",
    "            print( file_name, \"Saved file successfully\")\n",
    "    return acc, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target toys\n",
      "source toys\n",
      "/home/arunima/fproject/final_project/runs/toys/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/toys/size_10000/checkpoints/toys_model\n",
      "0.894333333333\n",
      "toys toys AUC 88.87%\n",
      "source toys target toys accuracy 89.43%\n",
      "\n",
      "src_toys_tar_toys_3000 Saved file successfully\n",
      "target vid\n",
      "source toys\n",
      "/home/arunima/fproject/final_project/runs/toys/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/toys/size_10000/checkpoints/toys_model\n",
      "0.796666666667\n",
      "toys vid AUC 57.06%\n",
      "source toys target vid accuracy 79.67%\n",
      "\n",
      "src_toys_tar_vid_3000 Saved file successfully\n",
      "target aut\n",
      "source toys\n",
      "/home/arunima/fproject/final_project/runs/toys/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/toys/size_10000/checkpoints/toys_model\n",
      "0.838333333333\n",
      "toys aut AUC 50.63%\n",
      "source toys target aut accuracy 83.83%\n",
      "\n",
      "src_toys_tar_aut_3000 Saved file successfully\n",
      "target hnk\n",
      "source toys\n",
      "/home/arunima/fproject/final_project/runs/toys/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/toys/size_10000/checkpoints/toys_model\n",
      "0.808333333333\n",
      "toys hnk AUC 50.30%\n",
      "source toys target hnk accuracy 80.83%\n",
      "\n",
      "src_toys_tar_hnk_3000 Saved file successfully\n",
      "target toys\n",
      "source vid\n",
      "/home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints/vid_model\n",
      "0.843666666667\n",
      "vid toys AUC 58.99%\n",
      "source vid target toys accuracy 84.37%\n",
      "\n",
      "src_vid_tar_toys_3000 Saved file successfully\n",
      "target vid\n",
      "source vid\n",
      "/home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints/vid_model\n",
      "0.862\n",
      "vid vid AUC 85.86%\n",
      "source vid target vid accuracy 86.20%\n",
      "\n",
      "src_vid_tar_vid_3000 Saved file successfully\n",
      "target aut\n",
      "source vid\n",
      "/home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints/vid_model\n",
      "0.837\n",
      "vid aut AUC 51.99%\n",
      "source vid target aut accuracy 83.70%\n",
      "\n",
      "src_vid_tar_aut_3000 Saved file successfully\n",
      "target hnk\n",
      "source vid\n",
      "/home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints/vid_model\n",
      "0.806333333333\n",
      "vid hnk AUC 48.72%\n",
      "source vid target hnk accuracy 80.63%\n",
      "\n",
      "src_vid_tar_hnk_3000 Saved file successfully\n",
      "target toys\n",
      "source aut\n",
      "/home/arunima/fproject/final_project/runs/aut/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/aut/size_10000/checkpoints/aut_model\n",
      "0.849666666667\n",
      "aut toys AUC 50.69%\n",
      "source aut target toys accuracy 84.97%\n",
      "\n",
      "src_aut_tar_toys_3000 Saved file successfully\n",
      "target vid\n",
      "source aut\n",
      "/home/arunima/fproject/final_project/runs/aut/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/aut/size_10000/checkpoints/aut_model\n",
      "0.796\n",
      "aut vid AUC 53.26%\n",
      "source aut target vid accuracy 79.60%\n",
      "\n",
      "src_aut_tar_vid_3000 Saved file successfully\n",
      "target aut\n",
      "source aut\n",
      "/home/arunima/fproject/final_project/runs/aut/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/aut/size_10000/checkpoints/aut_model\n",
      "0.883\n",
      "aut aut AUC 87.12%\n",
      "source aut target aut accuracy 88.30%\n",
      "\n",
      "src_aut_tar_aut_3000 Saved file successfully\n",
      "target hnk\n",
      "source aut\n",
      "/home/arunima/fproject/final_project/runs/aut/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/aut/size_10000/checkpoints/aut_model\n",
      "0.825333333333\n",
      "aut hnk AUC 49.14%\n",
      "source aut target hnk accuracy 82.53%\n",
      "\n",
      "src_aut_tar_hnk_3000 Saved file successfully\n",
      "target toys\n",
      "source hnk\n",
      "/home/arunima/fproject/final_project/runs/hnk/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/hnk/size_10000/checkpoints/hnk_model\n",
      "0.828666666667\n",
      "hnk toys AUC 51.17%\n",
      "source hnk target toys accuracy 82.87%\n",
      "\n",
      "src_hnk_tar_toys_3000 Saved file successfully\n",
      "target vid\n",
      "source hnk\n",
      "/home/arunima/fproject/final_project/runs/hnk/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/hnk/size_10000/checkpoints/hnk_model\n",
      "0.779\n",
      "hnk vid AUC 50.72%\n",
      "source hnk target vid accuracy 77.90%\n",
      "\n",
      "src_hnk_tar_vid_3000 Saved file successfully\n",
      "target aut\n",
      "source hnk\n",
      "/home/arunima/fproject/final_project/runs/hnk/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/hnk/size_10000/checkpoints/hnk_model\n",
      "0.816333333333\n",
      "hnk aut AUC 50.69%\n",
      "source hnk target aut accuracy 81.63%\n",
      "\n",
      "src_hnk_tar_aut_3000 Saved file successfully\n",
      "target hnk\n",
      "source hnk\n",
      "/home/arunima/fproject/final_project/runs/hnk/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/hnk/size_10000/checkpoints/hnk_model\n",
      "0.887666666667\n",
      "hnk hnk AUC 90.49%\n",
      "source hnk target hnk accuracy 88.77%\n",
      "\n",
      "src_hnk_tar_hnk_3000 Saved file successfully\n",
      "          toys       vid       aut       hnk\n",
      "toys  0.894333  0.843667  0.849667  0.828667\n",
      "vid   0.796667     0.862     0.796     0.779\n",
      "aut   0.838333     0.837     0.883  0.816333\n",
      "hnk   0.808333  0.806333  0.825333  0.887667\n",
      "          toys       vid       aut       hnk\n",
      "toys   0.88873  0.589859  0.506863  0.511696\n",
      "vid    0.57063   0.85863  0.532598  0.507199\n",
      "aut   0.506265  0.519886  0.871163  0.506893\n",
      "hnk   0.503023  0.487205  0.491399  0.904905\n"
     ]
    }
   ],
   "source": [
    "#calculate transfer accuracy for all domains, and save results in a dataframe\n",
    "list_df = ['toys','vid','aut','hnk'] \n",
    "#size = size_train\n",
    "size = size_initial\n",
    "transfer_results = pd.DataFrame(index=list_df,columns=list_df) #Dataframe to store accuracy on transfer. Col = Model, row = dataframe\n",
    "transfer_results_auc = pd.DataFrame(index=list_df,columns=list_df) #Dataframe to store accuracy on transfer. Col = Model, row = dataframe\n",
    "\n",
    "for s_key in list_df:\n",
    "    for t_key in list_df:\n",
    "        acc, roc_auc = predict_accuracy(s_key,size, t_key)\n",
    "        transfer_results[s_key][t_key] = acc\n",
    "        transfer_results_auc[s_key][t_key] = roc_auc\n",
    "\n",
    "print(transfer_results)\n",
    "print(transfer_results_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_transfer_vect = {} #Dictionary to store two domain vocab_vectorizer\n",
    "dict_transfer_train_ids = {} #Dictionary to store review ids of train set based on on two domains vocab_vectorizer\n",
    "dict_transfer_dev_ids = {} #Dictionary to store review ids of train set based on on two domains vocab_vectorizer\n",
    "for s_key in list_df:\n",
    "    dict_transfer_vect[s_key] = {}\n",
    "    dict_transfer_train_ids[s_key] = {}\n",
    "    dict_transfer_dev_ids[s_key] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note : size of src and tgt needs to be less than the original size read into dict_train_df with create_sized data.\n",
    "\n",
    "def process_transfer_data(src_key,tgt_key, size_train = 10000):\n",
    "      \n",
    "    #Create combined dataframe of reviewText from both domains\n",
    "    tmp_src_df = dict_train_df[src_key][:size_train] #picking the right sized subset from dict_train_df, dict_train_y\n",
    "    tmp_tgt_df = dict_train_df[tgt_key][:size_train]\n",
    "    tmp_src_df_dev = dict_dev_df[src_key][:np.int(size_train*0.3)] #picking the right sized subset from dict_train_df, dict_train_y\n",
    "    tmp_tgt_df_dev = dict_dev_df[tgt_key][:np.int(size_train*0.3)]\n",
    "    #print(tmp_src_df.shape,tmp_tgt_df.shape,tmp_src_df_dev.shape,tmp_tgt_df_dev.shape)\n",
    "    temp_two_df_reviews = pd.concat([tmp_src_df.reviewText,tmp_tgt_df.reviewText])\n",
    "    #print('combined df shape for',src_key,tgt_key,temp_two_df_reviews.shape)\n",
    "                \n",
    "    #create countVectorizer on combined dataframe of reviewText from both domains\n",
    "    dict_transfer_vect[src_key][tgt_key] = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=min_frequency)\n",
    "    dict_transfer_vect[src_key][tgt_key] = dict_transfer_vect[src_key][tgt_key].fit(temp_two_df_reviews)\n",
    "    print(\"Number words in training corpus for keys\",src_key,tgt_key,len(dict_transfer_vect[src_key][tgt_key].vocabulary_))\n",
    "                \n",
    "    #create id vectors of reviews for each df, train and dev set, using combined countVectorizer\n",
    "    #create id vectors of reviews for each df, train and dev set, using combined countVectorizer\n",
    "    dict_transfer_train_ids[src_key][tgt_key] = dict_transfer_vect[src_key][tgt_key].transform(tmp_src_df.reviewText)\n",
    "    dict_transfer_train_ids[tgt_key][src_key] = dict_transfer_vect[src_key][tgt_key].transform(tmp_tgt_df.reviewText)\n",
    "    dict_transfer_dev_ids[src_key][tgt_key] = dict_transfer_vect[src_key][tgt_key].transform(tmp_src_df_dev.reviewText)\n",
    "    dict_transfer_dev_ids[tgt_key][src_key] = dict_transfer_vect[src_key][tgt_key].transform(tmp_tgt_df_dev.reviewText)\n",
    "    # x_train = np.array(list(x_train))\n",
    "    dict_transfer_train_ids[src_key][tgt_key] = np.array(list(dict_transfer_train_ids[src_key][tgt_key]))\n",
    "    dict_transfer_train_ids[tgt_key][src_key] = np.array(list(dict_transfer_train_ids[tgt_key][src_key]))\n",
    "    dict_transfer_dev_ids[src_key][tgt_key] = np.array(list(dict_transfer_dev_ids[src_key][tgt_key]))\n",
    "    dict_transfer_dev_ids[tgt_key][src_key] = np.array(list(dict_transfer_dev_ids[tgt_key][src_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source key vid target key aut\n",
      "Number words in training corpus for keys vid aut 25821\n",
      "vid train set shape (10000, 150)\n",
      "aut train set shape (10000, 150)\n",
      "vid dev set shape (3000, 150)\n",
      "aut dev set shape (3000, 150)\n"
     ]
    }
   ],
   "source": [
    "#Convert the train and dev data to ids using the combined source and target domain vocab_vectorizer\n",
    "size_initial = size_initial\n",
    "list_src = ['vid']\n",
    "list_tgt = ['aut']\n",
    "for s_key in list_src:\n",
    "    #print(s_key)\n",
    "    for t_key in list_tgt:\n",
    "        print('source key',s_key, 'target key',t_key)\n",
    "        process_transfer_data(s_key,t_key, size_train = size_initial)\n",
    "        print(s_key,'train set shape',dict_transfer_train_ids[s_key][t_key].shape)\n",
    "        print(t_key,'train set shape',dict_transfer_train_ids[t_key][s_key].shape)\n",
    "        print(s_key,'dev set shape',dict_transfer_dev_ids[s_key][t_key].shape)\n",
    "        print(t_key,'dev set shape',dict_transfer_dev_ids[t_key][s_key].shape)\n",
    "#         print(dict_train_df[s_key]['reviewText'].iloc[1])\n",
    "#         print(dict_transfer_train_ids[s_key][t_key][1])\n",
    "#         print(dict_train_df[t_key]['reviewText'].iloc[1])\n",
    "#         print(dict_transfer_train_ids[t_key][s_key][1])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_transfer_cnn(skey,tkey,size=10000):\n",
    "     \n",
    "    x_train =  dict_transfer_train_ids[skey][tkey][:size]\n",
    "    y_train = dict_train_y[skey][:size]\n",
    "    x_dev = dict_transfer_dev_ids[skey][tkey][:np.int(size*0.3)] #Note : 0.3 is hard coded as the relative size of dev vs train.\n",
    "    y_dev = dict_dev_ypred[skey][:np.int(size*0.3)]\n",
    "    x_dev_tgt = dict_transfer_dev_ids[tkey][skey][:np.int(size*0.3)]\n",
    "    y_dev_tgt = dict_dev_ypred[tkey][:np.int(size*0.3)]\n",
    "    V = len(dict_transfer_vect[skey][tkey].vocabulary_)\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "        \n",
    "            cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('completed cnn creation')\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            size_folder =  \"size_\" + str(size) \n",
    "            out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", skey, tkey, size_folder))\n",
    "            #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key))\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            model_name = ''.join([skey, tkey])\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, model_name  + \"_model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            # Write vocabulary\n",
    "            ## vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "            \n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                \n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=True), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                   # _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    \n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                    \n",
    "#                     print('y_dev.shape',y_dev.shape)\n",
    "#                     print('y_shuffled.shape',y_shuffled.shape)\n",
    "                    \n",
    "                    if np.array_equal(y_shuffled,y_dev):\n",
    "                        print(\"Yes\")\n",
    "                    #right_acc = total_batch_acc/(num_batches)\n",
    "                    #avg_auc = total_auc/(num_batches*batch_size)\n",
    "                    \n",
    "                    #Calculate Accuracy\n",
    "                    #new_acc = accuracy_score(y_shuffled, y_pred, normalize=True )                   \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g}, auc {:g},\".format(e, avg_loss, avg_acc, roc_auc))\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                    \n",
    "                    \n",
    "            #Estimate accuracy on target dev set\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            num_batches = 0\n",
    "            total_auc = 0\n",
    "            y_pred = []\n",
    "            y_pred_proba = []\n",
    "            y_shuffled = []\n",
    "            total_batch_acc = 0\n",
    "            for ii, (x, y) in enumerate(batch_generator(x_dev_tgt, y_dev_tgt, batch_size, Trainable=False), 1):\n",
    "\n",
    "                feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                total_loss += loss*len(x)\n",
    "                total_acc += accuracy*len(x)\n",
    "\n",
    "                batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                total_batch_acc += batch_accuracy\n",
    "                y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                y_shuffled = np.concatenate([y_shuffled, y])\n",
    "\n",
    "                num_batches += 1\n",
    "                    \n",
    "            avg_loss = total_loss/(num_batches*batch_size)\n",
    "            avg_acc = total_acc/(num_batches*batch_size)\n",
    "\n",
    "#             print('y_dev.shape',y_dev.shape)\n",
    "#             print('y_shuffled.shape',y_shuffled.shape)\n",
    "\n",
    "            if np.array_equal(y_shuffled,y_dev):\n",
    "                print(\"Yes\")\n",
    "                #right_acc = total_batch_acc/(num_batches)\n",
    "\n",
    "            #Calculate Accuracy, AUC\n",
    "            #new_acc = accuracy_score(y_shuffled, y_pred, normalize=True ) \n",
    "            false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "            roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "            #print(\"\\t\\t\",tkey,\"Dev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "            print(\"\\t\\t\",tkey,\"Dev epoch {}, average loss {:g}, average accuracy {:g}, auc {:g},\".format(e, avg_loss, avg_acc, roc_auc))\n",
    "\n",
    "        # Save model weights for future use.       \n",
    "            #save_path = saver.save(sess, checkpoint_prefix, global_step=20,write_meta_graph=False)\n",
    "            save_path = saver.save(sess, checkpoint_prefix)\n",
    "            print(\"Saved model\", model_name, save_path)\n",
    "            \n",
    "            #calculate predictions and prediction probability    \n",
    "#             feed_dict={cnn.input_x:x_dev, cnn.input_y: y_dev, cnn.dropout_keep_prob: 1.0}\n",
    "#             y_pred, y_pred_proba = sess.run([cnn.predictions, cnn.pred_proba],feed_dict)\n",
    "            #print(y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source key vid target key aut\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.478197, average accuracy 0.809896,\n",
      "\t\tDev epoch 0, average loss 0.471252, average accuracy 0.804348, auc 0.705543,\n",
      "\t\t\t\t    Time taken for 0 epochs =  87.77323269844055\n",
      "Train epoch 2, average loss 0.394225, average accuracy 0.824619,\n",
      "\t\tDev epoch 2, average loss 0.409488, average accuracy 0.817935, auc 0.803251,\n",
      "Train epoch 4, average loss 0.325213, average accuracy 0.861278,\n",
      "\t\tDev epoch 4, average loss 0.385697, average accuracy 0.824728, auc 0.834558,\n",
      "\t\t\t\t    Time taken for 4 epochs =  427.7412483692169\n",
      "Train epoch 6, average loss 0.257961, average accuracy 0.900841,\n",
      "\t\tDev epoch 6, average loss 0.387685, average accuracy 0.829823, auc 0.856426,\n",
      "Train epoch 8, average loss 0.200503, average accuracy 0.929187,\n",
      "\t\tDev epoch 8, average loss 0.411617, average accuracy 0.831182, auc 0.866389,\n",
      "\t\t\t\t    Time taken for 8 epochs =  768.1429822444916\n",
      "Train epoch 10, average loss 0.155163, average accuracy 0.951923,\n",
      "\t\tDev epoch 10, average loss 0.345871, average accuracy 0.852242, auc 0.872416,\n",
      "\t\t aut Dev epoch 10, average loss 0.326487, average accuracy 0.871603, auc 0.81466,\n",
      "Saved model vidaut /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n"
     ]
    }
   ],
   "source": [
    "s_key = 'vid'\n",
    "t_key = 'aut'\n",
    "print('source key',s_key, 'target key',t_key)\n",
    "train_transfer_cnn(s_key,t_key,size_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Updated continue_train for adding samples from target domain to continue to train on source domain.\n",
    "def continue_transfer_train(skey,size,tkey,tgt_train_df,tgt_train_y): \n",
    "#Note size is size of source domain train set for picking the right sized model parameters\n",
    "    \n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key))\n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"testruns\", src_key))\n",
    "    \n",
    "    size_folder =  \"size_\" + str(size) \n",
    "    out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", skey, tkey, size_folder))\n",
    "    #saved model being picked is the one that was trained on source domain, but with the vocabulary of both domains combined\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    print(checkpoint_dir)\n",
    "    src_model = ''.join([skey, tkey])\n",
    "    #graph_meta_file = checkpoint_dir + '/' + 'hnk01_model.meta'\n",
    "#     graph_meta_file = checkpoint_dir + '/' + src_model +'01_model.meta'\n",
    "    graph=tf.Graph()\n",
    "    \n",
    "    x_train = tgt_train_df\n",
    "    y_train = tgt_train_y\n",
    "    x_dev = dict_transfer_dev_ids[tkey][skey][:np.int(size*0.3)]\n",
    "    y_dev = dict_dev_ypred[tkey][:np.int(size*0.3)]\n",
    "    V = len(dict_transfer_vect[skey][tkey].vocabulary_)\n",
    "       \n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:           \n",
    "            cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    " \n",
    "            saver = tf.train.Saver()\n",
    "    \n",
    "          #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "#             new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "#             new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "            \n",
    "            \n",
    "            #initializing weights from a previous session \n",
    "            initialising_model = src_model+'_model'\n",
    "            print(\" RESTORING SESSION FOR WEIGHTS INITIALIZATION\")\n",
    "            # Exclude output layer weights from variables we will restore\n",
    "            variables_to_restore = [v for v in tf.global_variables()]\n",
    "            # Replace variables scope with that of the current model\n",
    "            loader = tf.train.Saver({v.op.name.replace(src_model, initialising_model): v for v in variables_to_restore})\n",
    "            load_path = checkpoint_dir + '/' + initialising_model \n",
    "            #load_path = checkpoint_dir  \n",
    "            loader.restore(sess, load_path)\n",
    "            print(\" Model loaded from: \" + load_path) \n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "           \n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=True), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                   # _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    #avg_auc = total_auc/(batch_size*(len(x_train)//batch_size))\n",
    "                   # print(\"Train epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                                    \n",
    "                    right_acc = total_batch_acc/(num_batches)\n",
    "                    #avg_auc = total_auc/(num_batches*batch_size)\n",
    "                    \n",
    "                    #Calculate Accuracy\n",
    "                    new_acc = accuracy_score(y_shuffled, y_pred, normalize=True ) \n",
    "                     \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "                    print(\"\\t\\tDev epoch {}, auc {:g}, new accuracy {:g}, right accuracy {:g},\".format(e,  roc_auc, new_acc, right_acc))\n",
    "                    #print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate the predicted probability for positive and negative class on target train set using source model\n",
    "#source model is the one built on the combined vocabulary of both\n",
    "def predict_transfer_probability(src_key, size, tar_key):\n",
    "    #size here is full target train set size - so we can calculate uncertainty on all of it before sorting\n",
    "    \n",
    "    batch_size=50\n",
    "    print('target',tar_key,'source', src_key)\n",
    "    V = len(dict_transfer_vect[src_key][tar_key].vocabulary_)\n",
    "    \n",
    "    size_folder =  \"size_\" + str(size) \n",
    "    out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key, tar_key, size_folder))\n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key))\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))    \n",
    "    print(checkpoint_dir)\n",
    "    src_model = ''.join([src_key, tar_key])\n",
    "    #graph_meta_file = checkpoint_dir + '/' + 'hnk01_model.meta'\n",
    "    graph_meta_file = checkpoint_dir + '/' + src_model +'_model.meta'\n",
    "    graph=tf.Graph()\n",
    "    \n",
    "    x_train = dict_transfer_train_ids[tar_key][src_key][:size]\n",
    "    y_train = dict_train_y[tar_key][:size]\n",
    "\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "    \n",
    "      #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "            new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "            new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "        \n",
    "            #create graph from saved model\n",
    "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "            input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "        \n",
    "            pred_proba = graph.get_operation_by_name(\"output/pred_proba\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "        \n",
    "            y_pred = []\n",
    "            y_pred_proba = []\n",
    "            total_batch_acc = 0\n",
    "            num_batches = 0\n",
    "            y_shuffled = []\n",
    "            abs_y_pred_proba = []\n",
    "            for ii, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                feed_dict = {input_x: x, input_y: y, dropout_keep_prob: 1.0}\n",
    "                batch_pred, batch_pred_proba  = sess.run([ predictions, pred_proba],feed_dict)\n",
    "                batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                total_batch_acc += batch_accuracy\n",
    "                y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                abs_y_pred_proba = np.concatenate([abs_y_pred_proba,np.absolute(batch_pred_proba[:,1] - batch_pred_proba[:,0])])\n",
    "                y_shuffled = np.concatenate([y_shuffled, y])\n",
    "\n",
    "                num_batches += 1       \n",
    "            #y_pred = np.array(y_pred_list)         \n",
    "              \n",
    "            new_acc = total_batch_acc/(num_batches)\n",
    "            print(new_acc)\n",
    "        \n",
    "            # Calculate auc\n",
    "            # false_pos_rate, true_pos_rate, _ = roc_curve(y_dev, y_pred_proba[:,1])\n",
    "            false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "            roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "            print(src_key, tar_key, \"AUC\",\"{:.02%}\".format(roc_auc))\n",
    "            \n",
    "            #Calculate Accuracy\n",
    "            acc = accuracy_score(y_shuffled, y_pred, normalize=True )\n",
    "            #print(np.sum(y_shuffled==y_pred)/y_pred.shape[0])\n",
    "            print('source',src_key, 'target',tar_key, \"accuracy\",\"{:.02%}\".format(acc))\n",
    "            print(\"\")\n",
    "        \n",
    "        #Save absolute_y_pred_proba\n",
    "        \n",
    "        #check if the batching process left remainders. This will result in incorrect length of y_pred_proba saved\n",
    "        if y_train.shape[0] != abs_y_pred_proba.shape[0]:\n",
    "            print(\"Length of y_pred_proba does not match y_dev. Fix batch_size\")\n",
    "            print(\"Pred proba file not saved\")\n",
    "        else:\n",
    "            return abs_y_pred_proba\n",
    "#             file_name = \"src_\" + src_key + \"_tar_\" + tar_key + \"_\" + \"train\" + str(y_train.shape[0])\n",
    "#             np.savez_compressed(file_name,pred_prob=abs_y_pred_proba)\n",
    "#             print( file_name, \"Saved file successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on target sample of size: 2500\n",
      "(2500, 150) (2500,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      "# batches = 19\n",
      "Train epoch 0, average loss 0.425472, average accuracy 0.835938,\n",
      "\t\tDev epoch 0, average loss 0.3496, average accuracy 0.866848,\n",
      "\t\tDev epoch 0, auc 0.82734, new accuracy 0.866848, right accuracy 0.866848,\n",
      "\t\t\t\t    Time taken for 0 epochs =  26.90039610862732\n",
      "Train epoch 2, average loss 0.201903, average accuracy 0.919819,\n",
      "\t\tDev epoch 2, average loss 0.313703, average accuracy 0.872962,\n",
      "\t\tDev epoch 2, auc 0.849248, new accuracy 0.872962, right accuracy 0.872962,\n",
      "Train epoch 4, average loss 0.124531, average accuracy 0.964227,\n",
      "\t\tDev epoch 4, average loss 0.321829, average accuracy 0.871943,\n",
      "\t\tDev epoch 4, auc 0.853826, new accuracy 0.871943, right accuracy 0.871943,\n",
      "\t\t\t\t    Time taken for 4 epochs =  121.24110126495361\n",
      "Train epoch 6, average loss 0.0903847, average accuracy 0.981086,\n",
      "\t\tDev epoch 6, average loss 0.295588, average accuracy 0.882133,\n",
      "\t\tDev epoch 6, auc 0.858025, new accuracy 0.882133, right accuracy 0.882133,\n",
      "Train epoch 8, average loss 0.0614113, average accuracy 0.992188,\n",
      "\t\tDev epoch 8, average loss 0.304955, average accuracy 0.879755,\n",
      "\t\tDev epoch 8, auc 0.859865, new accuracy 0.879755, right accuracy 0.879755,\n",
      "\t\t\t\t    Time taken for 8 epochs =  215.4503619670868\n",
      "Train epoch 10, average loss 0.0455217, average accuracy 0.997944,\n",
      "\t\tDev epoch 10, average loss 0.314218, average accuracy 0.878736,\n",
      "\t\tDev epoch 10, auc 0.860014, new accuracy 0.878736, right accuracy 0.878736,\n",
      "Training on target sample of size: 5000\n",
      "(5000, 150) (5000,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      "# batches = 39\n",
      "Train epoch 0, average loss 0.359747, average accuracy 0.857572,\n",
      "\t\tDev epoch 0, average loss 0.367768, average accuracy 0.847826,\n",
      "\t\tDev epoch 0, auc 0.847032, new accuracy 0.847826, right accuracy 0.847826,\n",
      "\t\t\t\t    Time taken for 0 epochs =  48.27493929862976\n",
      "Train epoch 2, average loss 0.203337, average accuracy 0.920072,\n",
      "\t\tDev epoch 2, average loss 0.287002, average accuracy 0.883492,\n",
      "\t\tDev epoch 2, auc 0.868505, new accuracy 0.883492, right accuracy 0.883492,\n",
      "Train epoch 4, average loss 0.135797, average accuracy 0.953726,\n",
      "\t\tDev epoch 4, average loss 0.293394, average accuracy 0.881114,\n",
      "\t\tDev epoch 4, auc 0.875687, new accuracy 0.881114, right accuracy 0.881114,\n",
      "\t\t\t\t    Time taken for 4 epochs =  224.75298047065735\n",
      "Train epoch 6, average loss 0.0891332, average accuracy 0.981771,\n",
      "\t\tDev epoch 6, average loss 0.301601, average accuracy 0.882133,\n",
      "\t\tDev epoch 6, auc 0.877905, new accuracy 0.882133, right accuracy 0.882133,\n",
      "Train epoch 8, average loss 0.0627233, average accuracy 0.991386,\n",
      "\t\tDev epoch 8, average loss 0.29142, average accuracy 0.886209,\n",
      "\t\tDev epoch 8, auc 0.881369, new accuracy 0.886209, right accuracy 0.886209,\n",
      "\t\t\t\t    Time taken for 8 epochs =  401.31482887268066\n",
      "Train epoch 10, average loss 0.0474132, average accuracy 0.995994,\n",
      "\t\tDev epoch 10, average loss 0.276356, average accuracy 0.891644,\n",
      "\t\tDev epoch 10, auc 0.883288, new accuracy 0.891644, right accuracy 0.891644,\n",
      "Training on target sample of size: 10000\n",
      "(10000, 150) (10000,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.34623, average accuracy 0.858574,\n",
      "\t\tDev epoch 0, average loss 0.291809, average accuracy 0.883832,\n",
      "\t\tDev epoch 0, auc 0.863731, new accuracy 0.883832, right accuracy 0.883832,\n",
      "\t\t\t\t    Time taken for 0 epochs =  88.16617512702942\n",
      "Train epoch 2, average loss 0.205083, average accuracy 0.918369,\n",
      "\t\tDev epoch 2, average loss 0.276992, average accuracy 0.886889,\n",
      "\t\tDev epoch 2, auc 0.883437, new accuracy 0.886889, right accuracy 0.886889,\n",
      "Train epoch 4, average loss 0.13595, average accuracy 0.953826,\n",
      "\t\tDev epoch 4, average loss 0.26308, average accuracy 0.897079,\n",
      "\t\tDev epoch 4, auc 0.889535, new accuracy 0.897079, right accuracy 0.897079,\n",
      "\t\t\t\t    Time taken for 4 epochs =  424.26227736473083\n",
      "Train epoch 6, average loss 0.0888083, average accuracy 0.979567,\n",
      "\t\tDev epoch 6, average loss 0.271033, average accuracy 0.896739,\n",
      "\t\tDev epoch 6, auc 0.892442, new accuracy 0.896739, right accuracy 0.896739,\n",
      "Train epoch 8, average loss 0.0631397, average accuracy 0.989583,\n",
      "\t\tDev epoch 8, average loss 0.260345, average accuracy 0.892663,\n",
      "\t\tDev epoch 8, auc 0.895433, new accuracy 0.892663, right accuracy 0.892663,\n",
      "\t\t\t\t    Time taken for 8 epochs =  761.0284225940704\n",
      "Train epoch 10, average loss 0.0465923, average accuracy 0.994992,\n",
      "\t\tDev epoch 10, average loss 0.270465, average accuracy 0.898438,\n",
      "\t\tDev epoch 10, auc 0.896861, new accuracy 0.898438, right accuracy 0.898438,\n"
     ]
    }
   ],
   "source": [
    "#Continue training with adding random samples from target domain\n",
    "size_model = size_initial\n",
    "size_list = [2500,4000,7000]\n",
    "src_key = 'vid'\n",
    "tgt_key = 'aut'\n",
    "for size in size_list:\n",
    "    print('Training on target sample of size:',size)\n",
    "    tgt_train_df = dict_transfer_train_ids[tgt_key][src_key][:size]\n",
    "    tgt_train_y = dict_train_y[tgt_key][:size]\n",
    "    print(tgt_train_df.shape,tgt_train_y.shape)\n",
    "    continue_transfer_train(src_key,size_model,tgt_key,tgt_train_df,tgt_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target aut source vid\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      "0.8641\n",
      "vid aut AUC 81.68%\n",
      "source vid target aut accuracy 86.41%\n",
      "\n",
      "(10000,) (10000,) (10000,)\n",
      "max, min uncertainty absolute 0.999808251858 8.78274440765e-05\n",
      "max, min length 150 0\n",
      "max, min certainty*length 144.65676856 0.0\n",
      "For range 0.00 to 0.10, average certainty = 0.14, average length = 71.59, average certainty per length = 9.74\n",
      "For range 0.10 to 0.20, average certainty = 0.38, average length = 66.24, average certainty per length = 24.84\n",
      "For range 0.20 to 0.30, average certainty = 0.56, average length = 63.87, average certainty per length = 35.62\n",
      "For range 0.30 to 0.40, average certainty = 0.69, average length = 56.90, average certainty per length = 38.98\n",
      "For range 0.40 to 0.50, average certainty = 0.78, average length = 55.27, average certainty per length = 42.80\n",
      "For range 0.50 to 0.60, average certainty = 0.84, average length = 48.21, average certainty per length = 40.58\n",
      "For range 0.60 to 0.70, average certainty = 0.89, average length = 46.07, average certainty per length = 41.14\n",
      "For range 0.70 to 0.80, average certainty = 0.93, average length = 42.09, average certainty per length = 39.26\n",
      "For range 0.80 to 0.90, average certainty = 0.96, average length = 35.99, average certainty per length = 34.65\n",
      "For range 0.90 to 1.00, average certainty = 0.99, average length = 31.25, average certainty per length = 30.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arunima/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-ef70151e8d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m            np.average(c_div_len_sorted[i*range_l:(i+1)*range_l])))\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_train_target_abs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_target_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "#Calculating certainty for target train set review\n",
    "\n",
    "src_key = 'vid' #source domain\n",
    "tgt_key = 'aut' #target domain\n",
    "size = size_initial #This is source model train set data size to read from the right file.\n",
    "\n",
    "#calculate absolute difference of positive and negative class probability for target train set using source model built on combined vocab.\n",
    "\n",
    "u_train_target_abs = predict_transfer_probability(src_key, size, tgt_key)\n",
    "train_target_len = np.count_nonzero(dict_transfer_train_ids[tgt_key][src_key],axis =1)\n",
    "c_div_len_target = u_train_target_abs*150/train_target_len #calculates certainty per word id in the review\n",
    "# file_name = \"src_\" + src_key + \"_tar_\" + tar_key + \"_\" + \"train\" + str(size)\n",
    "# u_train_target_abs = np.load(file_name)\n",
    "print(u_train_target_abs.shape,train_target_len.shape,c_times_len_target.shape)\n",
    "print('max, min uncertainty absolute',np.max(u_train_target_abs),np.min(u_train_target_abs))\n",
    "print('max, min length',np.max(train_target_len),np.min(train_target_len))\n",
    "print('max, min certainty*length',np.max(c_times_len_target),np.min(c_times_len_target))\n",
    "\n",
    "#See if certainty is correlated with length\n",
    "span = 0.1\n",
    "sort_ids = np.argsort(u_train_target_abs)\n",
    "u_train_target_sorted = u_train_target_abs[sort_ids]\n",
    "train_target_len_sorted = train_target_len[sort_ids]\n",
    "c_div_len_sorted = c_times_len_target[sort_ids]\n",
    "range_l = int(span*len(u_train_target_sorted))\n",
    "\n",
    "for i in range(np.int(1/span)):\n",
    "    print(\"For range %0.2f to %0.2f, average certainty = %0.2f, average length = %0.2f, average certainty per length = %0.2f\"\n",
    "          %(i*span,(i+1)*span,np.average(u_train_target_sorted[i*range_l:(i+1)*range_l]),np.average(train_target_len_sorted[i*range_l:(i+1)*range_l]),\n",
    "           np.average(c_div_len_sorted[i*range_l:(i+1)*range_l])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztvXuUVNd95/v9VfVpqGrZVCOjLFSi\nhawoMCaIbtOR2mGtGSNPhCMkuUcvrIjEyfW11p2bmxmwb0/gWmPAUSIyfWWRWTNrMkqcG+dKUVqv\ntJFxBvlaZOVeYmSDuxHBhmVLQqCCRNhQxOou6Orqff+o2tW7Tu29zz6verE/a2mJ7q46Z5/X7+z9\ne3x/xBiDxWKxWDqXRLMHYLFYLJZ4sYbeYrFYOhxr6C0Wi6XDsYbeYrFYOhxr6C0Wi6XDsYbeYrFY\nOhxr6C0Wi6XDsYbeYrFYOhxr6C0Wi6XD6Wr2AADgQx/6EFu+fHmzh2GxWCxtxZEjR37CGFvi9bmW\nMPTLly/H4cOHmz0Mi8ViaSuI6B2Tz1nXjcVisXQ41tBbLBZLh2MNvcVisXQ41tBbLBZLh2MNvcVi\nsXQ4LZF1EwXjEzmM7j+JXL6AJBFKjCGbSWFkwwoMD2Srfz+bL+D6yu8BYOfe48gXigCArgRhdm6+\nEcu6mxfjwcG+mu8tvzaF77x1AfxjKSeB+9fegAMnztdt2z2eBAFzkj4vmZSDnfeuqo5z1yvHcXG6\nWPO3w+9cwHOvn0GJMSSJMPThXpz6aaFmn7LvEwGMoe5ciMfN97Pq+g/g79+8AHGIPd1J/JuPZuuO\nT3VOhweydddE9r0v/vUxTM2Uavbz+/9mtXS761cuwYET52vOJf+/yM99oBtdyaTnOGXby0rGrzsO\nfn1Vxy5+VzzXBFTPbybl4O41S7X3DidJhA8vSeOt89M1xy2Onx/X2XwBi1IOiID8dLH674vTRek9\nKBvTN46eq7k/AKA37WDHPatqjvOx8WPV+zJBQJKA4tz8d9zHKI5Ld968eGz8GJ49dLrmXtU9R3B9\n7u41S/HykXcxLQ7Whfuchhlvs6FW6DA1ODjIwqRXjk/ksP3lYygUS3V/SzlJ3L82i5eO5Gr+7iTK\nD4jM8IqojLMOJ0EAAcWS+RedBGHTbcsw9r0zvr7H4cep+371M989g6Lfg5Jsx31OU04ST9w3b6zd\n14R/77nvnkFJsv9kgvDwbcvqthv1OHWf5+PnyI5Ddn1V3x154aivcx3k3mkkTpIw+sAaDA9k8dj4\nMTxz6HSo7cnOmxe6/YZ9jrwIMt44IaIjjLFBz891gqFft/u1mtlPuyLOrIIQ5KXUaLzG2GrH4Pea\npJ0EuruSuFQoIiFZdVjk/NwHujFTYjUzcL4aTTkJXC7OhXo24qCnOwknmcClQrjVSRiuKkO/fNu+\nCEdjsVgs/uGTgkxE7imjfRoa+rYPxo5P5Jo9BIvFYqmuOPKFIi5OF8EA5PIFbBmbxEf+49801Va1\nvaEf3X+y2UOwWCwWLdPFOYy8cLRpxr7tDf3ZDvDNWyyWzqc4x/CF55tj7Nve0F+fSTV7CBaLxWJE\niTGMvNh4Y9/2hp7nHVssFks7UCwx7HrleEP32faGvlXyWRtNykmiN+00exgWiyUAskKuOGl7Qz8+\nkQNFvM1sJoXNQ30tbUifuG818iFvlrSTAEV98gxwkoTNQ33IpObPb6IJ47B0PkTA5qE+9HQnG7bP\nZDMeKg/aXgJhdP/JwIUUKSdZU/EoFscM3rgYjw+vBqDP0w9S5JQAkExS4Mq9bCaF4YFsXZk8p6c7\niemZknJc2UwKB7fdUfM7XXVx1BRLDAdOnMfkjjtr9r9lbDL2fTcbfu47pciv1Xnk9j48PrwaB06c\nx9RMY853iTFkUg6mZmaVz7g4yWkEbT+jD5p1k82k8MR9q5GtBHNFg53LF7D95WPVgIluZu/XVGdS\nDr6yqR+jD6xBNpPyvRpJOclqXGJkwwqknNqZipMkzMzqqwhl52x4IFtzPvispDftlMvyXfvgN2rQ\nuUsuX8BN2/Zh3e7XMD6Rw/BAFpuH+oy/H/ZBWdAV363vJEn6YDlJqrl27vMaJ06CGjqrbRVeOpLD\n+ETOt51IOwkQUH0e/JIvFAEG6Tl3EoSd964KtN2geFbGEtGfAbgbwHuMsV90/e1/BzAKYAlj7CdE\nRAD+CMBdAKYB/CZj7PtegwhTGauaGelm2m69CtU2uNCVWwBMR2/aweXinFIDxs8xuAW8ZMJbbsGt\nqSuznmOVzeh1qES9Rl48GomeCL9WMhGp9SuXKDV1VKJVpiQThLk5FmlpfTYjF8yTCYINfPnVQON3\nEoTRB9cAALaOTUrHn0k56FnQVSfG9ciffAcH37zg/8BiJuUksdBJxOK75sbazwqKCwceeutiKBkL\nfj+YCOAFITIJBCL6lwDeB/AXoqEnomUA/hTASgBrK4b+LgC/g7Khvx3AHzHGbvcaRBhDrxPPMlXM\nu2nbPu1Lwe3O6E07yhuSADy1qd/XhVWJNG0e6qu6j0zRHQsQnSiTl+vBSRLA4Fs8TSUOplLBrBMc\nSxK6EoSCRpVQhD+IQdxGYc+l7lqtu3kxvn/6Ut29l3ISeOK+W6v7lKk4eo1LVJxsFTYP9WHwxsWe\n7sMgrlL+TDbKNene99u7N8a3/Si1bohoOYBvuAz9iwB+D8DXAQxWDP1/B/C3jLHnKp85CeDjjLFz\nuu1HoV4Z5o3pNaN2o5sh+J0te+1/jjFfx6QzwDopXhm68+r1QtmzqR9AvVSzCapz6EcuuH/Xq0ar\nMP4gms6u+TWJQm43iJ+enxvRWBMBqa4ECsW5yO4V2X7Xr1wS2wuCH5dKUpr/HCSukSTCkw+VV0B8\n25m0A8ZgvFJ3b8/0HPSmHUx86U7vDwbE1NAHCsYS0b0Acoyxo1QbYc4COCP8/G7ld3WGnogeBfAo\nAPT1mftmZQwPZKs3Nr9Rto5NGt/0IxtWSFcFqrf/2XxBOkMQ/ed+UPkP+c3EYwaAdzrpyIYVdS4V\nUVrWFPds2T0G3UOXJJKe//GJHL7w/FHPh0R2PlTjeeK+1dKXwiXDBzhTib/suGeVkSuqxBj2uK69\neG6A+hePu5fAw7cvw+PDq6X3nRdn84W6FSBj5RJ7vytA0/0TUD3Hz4aUJVbBr7n4LIvw5zoIJcaw\n/eVjuH/t/HbT3V3VZ9XvNXjyoTXSlWRprl72/P3Ls9UYVDPxPaMnojSAAwDuZIxdIqJTmJ/R7wPw\nBGPs/6t879sA/gNj7Ihu+2Fn9BwTN47K+Mtmi6qsFtWsLmiTAtOZlclqQaWBzpfGpisf3SqDz45M\nDCN3IwDmD5TMv6y6FlnX33WNSXTwGeu+N85pZ/YEYFHKkc4EZfGZZIKk2vvcKIsNc0zH+Y+XLkuP\nLUHAW0/4cxPIXD9uMimnmiEVV7aQ7t72kxHW053E5eKc9PzI3D69aQcbb13qa6VyavdGqb1QxfKC\nrPJNic11Q0SrAXwb5WArANwA4CyA2wDsQhNcNxzTwKypb9XkBtMZsij3w4/Dy9+nexCdBNW8AHTj\n84pb8GOu6WYFuf/UbzDMcaWe6lZXJn83xd04RRXojIIkEd584q7qz16uMGB+ZaaLJ+zZ1B+J21K2\nX1VcxC9+n0fTlwvfjt/r5rdBzSnFM6i6hnH66WOTKWaMHWOMXccYW84YW46ye+ajjLF/BLAXwG9Q\nmSEAl7yMfJSoXCDuk18oloyWgWLKIUFeCMG3Nbr/ZN1NEuV+gLKuz/hEDut2v1aTmiiiSyNzz/Ld\n4xO3ndAUfRSKJXzh+aPYOjaJdHcX9mzqV978QNnA+5kFulcJhWJJeU6SRNqHM0lknCYnno/hgWys\njS7cs0cTzaae7i4MD2S1BTk793qX1ovX2eS6FEus5rzI0nD9wFCeSVPl/wu6EtgyNombt38TyyX3\nte6e5s8MT5fmbkU/FIolHDhxvuYZDIJqv62gx+Xpoyei5wB8HMCHiOhdADsYY19VfPybKGfc/Bjl\nGf9vRTROI/wEa1R+YJlrQwxAmm7L5G8i7jiDbHWwfuUSrd8c8HcOxPG5l/Bey1hZ/CBosMyEEmPS\nAjeTcWZSDvLTM0b7Ea9XVnE8qhTaBV0J4+Ce20CObFjh2XYwXyhi3e7XtMecLxS1PuGgM3LxvPBt\nh5nZXy7O4ZGhvppZtCompbqvVC4RWezBK1snly/UPIO6IknV+VXF+lpBj8tzRs8Ye5gxtpQx5jDG\nbnAb+crM/ieVfzPG2G8zxm5mjK1mjIX3x/jATxGK+y3LH4BcvlBtGCAWTQFl36xqW6q3doJIOftW\n4Z7h89nKgRPnpasGUfp0ZMMKXzMSvkrw8tPq4DNhWQGXH1JOUlkIlc2kcP/abM2xMZgVbOULxZpG\n5DrE6yg7npSTxI57VuGJ+1bXFNIt6Erg7jVLjY//4duX1fw8PJDF6INrkHbUjyTBzP2lW0XKVp4m\nuO/voNvhFIolPPf6GeU2xNWV6jqoDKjs+XlkqE97bcQX7/hETrtSUZ1f1XPb7EAs0AESCCLDA1mj\nIhrZTaJzvXDf5NTMbN22nARpo/dBMmf4Z9yf26rwzfKsAv69w+9cqDPcsrx2fh68ZCRMfOB8RgTM\nZ53otikrjlKdR904ubGPys2yfuWS6r/dx+PO4b8s5OrnC0W8dCSH+9dmtQ2zxawbN1zWYtrQBaki\n6AqTB5rdpfuy58VrpeokCNcs7NI+i6bZV7rroEL2/AzeuFgZ3+Bj4RM+3dh0x67KGmo2HWXoAWiF\nvghQ3iSqi8d/P7r/pDTD5JqFXdVtiWl0MsQXhx+4S0n3WIjbfnx4tTTDhh+H+2FRvUCA+f6XXoae\nz4DEG11XceyVheBnnLyq1m++vowDJ87X/Kx6cFUTg+deP6MsqDOpD4jihaXzCZu4QUzqUnRuukzK\nwc57V3neB14vaH4cqvqJdbtf85XhptOH4tubnpn1vNcTRC2RMumHjjP0XjeyKs9e9T1+s6leBBen\ni7hp277qTChIjrgOPz5Vtx9VdiPKfqd7aHXCTCKy4w7qs5SNfXwih4SmgE00oGFSAE2vj6724f3L\ns9LMIdlxq9JhTZFlsOjOr8k1MZmV6nLwr8zO1X1WdowJReqp+D1Z/cTIC0cBmg/am66WxydymJas\nyjmm94x7Ba0jbDFnVLS9qJkbnT9P54dfv3JJnb9XfAB0sySG8tLdxCD6jcD78YWqYgheqHzrC7oS\nxlo2SUksIiqfpW45LTNsYWIFpjEV3XUszjH0dHcZHffOvcdDGflHhvp8nV/dNfHK6JJtR5eJJn72\nmoX1c0qdkQfKrsrPPz9Zd/8X55g0M0sXm+Av1Ki0dArFEraMTWrPk8zebB2bxGPjx6Sfj5OOm9Hr\n/Hnrdr8mXW7v3HscV1yKjwTg/rXzM5sgVYxugkTg/awAgspgq/z67pmZDlUsIgqfpepllySSGjb3\nPSArbBv73hnpS8w0prJ+5RKtL/5SoVgjw6wiSAk+hwG+tZAA9YrJK6NLth2VO8193wbpncBQrvo1\nRfeshHmh6tCdJ9l9y1CuLh68cXFDZ/YdZ+gB9dJTdSPIHjaGWn+tn0CjSBC9GhE/KYsXp4u+/Zac\nAyfOGx9Tb9pBurtcuSpzpwSNRahQXbc5xrRGSLf/wRsX44t/fUybjaM7Drcv300jcqeDSujK8EpG\nUKG6PxnKKYpRBsq94BlksklemBeqF2JdibhPXV1PlM+HCR1p6FX4zfN2f9Yk0CiiU2IUA4c6sTG/\nKwk+JtVMQxXY8lO1yhg8RaZy+QKWb9tndIxeeMVPgmIywTubL0jPmW726GflplNC1RF1frZXMoIK\nr/uzUUbeq87EL36vi9+6kqB9NILScT56HX59t7pcWmnTD1dzB3dzC9FnB9TfHDJfn2nVrAxZ5avb\nZzjywlGMvHhUuY1Myqnuu6c7iWKJIV8oVr/vNRqTY/RC1WBl6sqs0p/s5W82jX0wlH3F7riOKh6i\nciep2HHPqnLqqw/87sME06pO93kFUFff0Gh60462zmR0/0lfbUFTThIbb10a+Jj4DF9M1XXT6GrZ\nq8rQu8u3vdBl0MiCWptuW1YzS8wXijXGTWdcdMGk4YEsDm67A2/v3og5n6mD4sxBtn9ZYIuTcpLY\nee8qHNx2B57a1I9piZvDtGgJMJeEcOM+171pB6hIzMqK20yK3/zMqGQSGkSQBv2ffMifSujwQBaj\nD6wxNkROknzvwwSToiTVed33xrlIZ+5+O2+lK9IQulXJjntWIanY7rqbF9c8x1z3JswxlRjD2PfO\nKP+uewnEwVVl6IF5o2lyK3m1qxMN8MFtdyhnFLteKeuPeBkXE+OT8dmwnOf8jk/kfKccirNGXR4/\nz2M3OadBl6ziuU53d2l1e0x0h1QzKtMVU366iPvXzuvOJIlqgvd+GB7IYuJLd2LPpn7PSchsBB29\nVGPwypBSndeou0KNPuivzSZvS6nSZ7o+U+6x/OSDtS/UTMrBnk39ePZzH6tOZgDgmUOnjV2luvtF\nl7HmFeOJmqvKRy9i4q+fmqnXktblxepy7ccncp775Kl9qgKn9SuX4P3L6jxgGSXGyq4Zn/aBNyDn\neAlL8Tx2r9iFnyWr6lx7+ZNVfxfHpconN1U/zKQdvHQkV131lRjDS0dy0myK8YlcXVvBjbcuVUpa\ne/m8VbEXsSrcXbSkw32en1IoYDbCryzed37kmxnkK3BxVaLKNuL3rN/AMY87hdUOagRXraE3uUBc\ntY/nGLvlFfw04+BaMLp9iv5sbpz5zDWXL2j1aHTBI93MwklQTfEJIA/0ZTTbFz+rSzvUFQ3JAsSq\nwJpXcFb1d8K8IJUuDdfLwKScJBirrxaWZanICqIuThdrzlEuX8CWscnqys/LaLj3Mz6Rq+sNkC8U\ny4VFUKdHul9AfCz8PLsbpqScBKYlLRozKQdTV2ZDpy+6613CSk8T6hVJRdz78TN6Qvle59s0aagj\nYn30DcK9VFXBsy62v3xMaujc4ku67bhjBOKy302xxOoeHNVtRCj7KYMw+uAajD6wxrPoRnUPp51E\nzWdVS1JVAFHl992597jSkHr5k1XCbjytjeN2vYk1E+7t8+3x86PqYOWeqY3uP2lsAC9OF43dIO7Y\ni+xlXpxjypgIP++ytMNCsYTtL7+BZw6drlmxTBfn4HZz8ziOrCAKKJ+vPZv6PZMgeECVv2hVRp7L\nG5vAz4gqEcBPMaL7uBmAl47kqhOHJx9aY5zo4SQp0owpE67aGT1gli55fSbleUOI4kuqLjP8DS5b\nPqrkj025PpMKtBQUl8leS3yVYXM34fab867y++raOOpm4/xYVOJVJufJa/v8byYpn3Et0cX9mIiY\nuVdNU1f0mi6q5uo8HmOqQ8SrQTMV3XlVPjsPqPLvyCAAE1+6M9Dz4o7h+KmFyWZSuDh1pW41I66s\nTN1NvWkHO+4xc6lFyVVt6EVULof1K5d49skUH7qd964y0ncRHzyVhosMlbaJH3+makyysfGH2TSX\n3W/Ou19DqHthiqi05E2XzF7bN9XxiUOj370f3T544ZJImPEwBqkwm9cYLk4XtTNeHlDVyXgwlCdk\nOjeiDj6zN53F82dt6sqs1GUF+KxcB2JtFK7jqnXduFG5HA6cOK81DlwjnOdqm2QvuN0VMiPvJKku\nzSzlJJXaJiMbVtQtLznui+yWdxDh/t6aXPsXyznBsgd1uhKw5vjVDvfjq/RTJOR3HCaIOeSj+0/i\n/rVZT5eXnx4JnEzK0WZzuK9dHKl6qr2rxmVSo6LrFAbMa0bpyOULgbN8vLqRiYgTKt2YRIVNsUZG\n99lm0LEzer+qcbpMjqc29StnAm4/IKCeDT42fkwrY+yWSwD8aXDz74ukJQE0t7yDyK5Xjtf5e4sl\nhn1vnMMT962uc01dnC5Kj9tr3GKFsEm2g6qyVtcVzB1MDJr+CNR34MrlC3jpSM5IRAyANOvmG0fP\n1RkR7vMGoMz+Ea/dY+PHtJo7QVFdj+6u2swwt+vPqx9EibGGyiK4922K6SenrsxW70GvvsbN7DRl\n0krwzwDcDeA9xtgvVn43CuAeADMA3gTwW4yxfOVv2wF8FkAJwL9jjO2PaexKggg06VwObv+b6kbV\naYOYPJAlxup6r5oaJlXQz++SU/WQXpwuVgNlbuPkPm4vt4cs20H38Ot03HUl76bpj16oOnCZavqo\nzsfjw6u1ExKvOAMfV1D8lvknMO+7Vz1TlxX3G6dZRj4ueFGkzsiHkf+ICpMZ/Z8D+C8A/kL43bcA\nbGeMzRLRHwLYDuB3iegjAD4NYBWA6wH8P0T0C4yxcHlSPgki0OTlc+UPq1eeuMqAPvu69wOZrBQ3\nBdGv9ut3DbqM1OWomwqqqVT9ZEZA7OBlsh0x6BZEpEu1H5VxMvXR6lYeqvGo4gzcVz11ZTaw0eQt\nEU1jO7KGLu7z6TWr7TQjz/Ey8l5NdhqBp6FnjP0dES13/e5V4cdDAB6o/PtTAP6KMXYFwNtE9GMA\ntwH4TiSjNSSIQJOpy8HrweYPodgmb1HKMZJbLTFWs1z301BB9RBlUg6uzNY3slYZz0zKkfokeZWw\nLkddFFQbefEodu49jkuFonFhmWz8uvlhkOtsYpjdRjms3zXIChPQ13qECai6Z5gmAUqV24MHUU2C\nzu1k5N3Pk0lrRDfNdteIROGj/58AjFX+nUXZ8HPerfyuYeg6EXk9lF4uB74Nrxs6ly/UuGn8SKQG\ncQ/oZpzc32u6Sth57yppN6B8oVh9gb10JCedkYtw8TPAX2GZm9IcUx6/V4aP6m+6VZPMKKteogR9\n7QQnqARwkApRTgJAqjtZlWGWVcqa+JZNEAXughjzlJPA4p4FkWcnhSGtOHdeK3oeam5mNykZoQw9\nEX0RwCyAZ/mvJB+TXnsiehTAowDQ19cXZhhV/HYiCkIUDUj8ElYjx+QFJn4WkMcjeADy/rVZ7Hvj\nnK/ZjWjYgkovu/Fyt8n+ppOyVRXryNxKvLuTyXkNKgHMxzQ8kMVN2/YZG9G0k8Af3HerrzhJFASd\nsS90kji47Q7tMfLcfdEA60h6tCn0QtyH2IDHRJZ5j0JCopkENvRE9BmUg7SfYKxqWd8FsEz42A0A\nzsq+zxh7GsDTADA4OBjJqs5vJ6IgyFw8cc9EvFYiujHs3Hvct89fF48oFEs4cOI80t3+lrFAbWEZ\nYF42rkrJM8mscR+71+xa51YSg5eLUg4Gb1zsOXZA35yjf9erRpo0qm2ITWAy6bKL8FKhWFMRq5J5\naORkRQfvPmXSuBxQFzeKWWtTV2YjazbCZYcBs3tXJoPR7L6xgQw9EX0SwO8C+FeMsWnhT3sB/CUR\nfQXlYOwtAL4bepSGmFRluk+66Ev3awg5YZpReyHqf6hulpENK5QZGvlCUelC8UIXeA2i1S2+sLwy\nS0RUD9T4RE6bWSNbyXi1vlMZm0zKqcko4dkW4rGo0M0CTTRpVNvgAdXhgay8ibZEL4mPudGiWjr4\nfTGyYUWdZo9MLkD3nL9dyVoLW23upsQYtoxNYufe47h7zVJ8YGGX8kUi3j9B4zNR41kwRUTPoRxM\nXUFE7xLRZ1HOwvkAgG8R0SQR/TEAMMaOA3gewA8A/A8Av93IjBuv5gkyXZVnDp2u01kxaY4hFs9M\nz8z6LorR4dZVAaDVVx8eyCLtmNW+6TTh3U0lVFWKBH0jcrH5ioi7sGd4IIuUwbizwvUTx6fTw1Hh\ndY+oCq2I1Fk8XvAiOtXKRKdJIyI2shG1YQBFrwGJXhJvaq2S9A2DkyBPaW8ZPBcdQL3/R/KON2mS\n4lfO25R8oSxI57Va0PWgCNqXIQyeTxlj7GHG2FLGmMMYu4Ex9lXG2M8zxpYxxvor//0vwud/nzF2\nM2NsBWPsb+Idfi1e1ZAmy1WTi+B+YVycLgJUnvUFfXxE4/7Upn6cEoS2TG6W7i7zzlmyGZHsJTg1\nI5dEZoCy8caeTf3IpLul33MXaY1P5DDr4Ud1KxqK41M9bLrZqtc9oqpsVjW3Np0ZDw9ktU1jdNuR\nCZC589X9ztD9FA8Bhjr9BNy9Zqm2Y9a6mxfXNVnhq6Ndr9Q38Ja9BE2qnn0eXuTwMYeJz0RJR1XG\neqVImp5ctxCUu7/r9Ey9IFSxxNCzoAuTO+408iH6cRmZ3Cwq0TEZshmRakao4uJ0sfLAsmoRTYKg\nFHVzj5fvU7aPBJUfVPG8rNv9mg+lQXn1JqC/R3S67KYiZjp0sRTZdsT7z43bbxxnrIgX8d28/Zva\nF0SxxHDgxHn0dKvdGqd+WpDGd7yE7NzX5v61We3z4+d5SDkJLHSSkTZQ8XIFNloOoaMMPaDPMDEV\nQ+IpeKJvTdSKV8Evrq6pRRC/nMnN4udBl2UfBZlhuM9lOVNBbYxNlR0ZQ9XXGmR8dbr+QJ2xl0kp\n6HyppiJmOmQ+aEBeFGaSFVNirDpGlShfAvpaBC9EDX+TVUAUXdTcpLuTdfUlovwEfwlsHZv0FOFz\nk0k56FlQDmbLisKCIroCw943UdBxhl6HyTUU1SD9ZiWIyoqAemXhFRCW/ezOXXffLKYpi71px1dO\nem+6HIQMm6HBg2omqp2y2Y6yUIvmr6ssj7tYYtj1ynHPF6xXNo5pQZ0OmR6MqhOU6f3n5Wqcg7yq\n1RSu4T88kDXaDvPYn06pNeUkpPLIsnRK8bhlL2je99XrHP5MyM6JysgD8/GoKO6bKLiqDL3Jco7P\nElTZGSrchle1spDNHN3dhtw/89x13VLVfUMtSjmYmpmt6xy1455V0vHrsjr4dsO4BnoqjVFkqyQR\n1WxH9SITN6F6TE1WcSbuMT/1CCpMt+Fn5uv12RJjcBIUuAMU376pIdR9Tm3kkyCfmfhn8wXlC/rA\nifN44r7VnvpUYXLtdXhlfzWaq8rQmyzn+AXx+qy45Es5CRRmy9kMPF2QUJ7FzlQMLW84EGSlwG/c\ng9vuqFmm8m5LKjExP/m7Xr7rsOQLRWXusVu1UzXGBV2J6rkTZ/ImeOnwmPpSo86JVm3PjytOVwnM\nMTHyKmPIt6/S3jEhQYBuCEFWjAki5Xh4w5PrM+UOV4D/dn9h4NlN7me0WRBrdnga5YKpw4cPx74f\nL7+nWL6sKvUHav3tfmRinSQt9QkfAAAgAElEQVRpA5xe7JHIJTtJQk93l1RTJggyt5LJEjgMhHqf\nvHtMUVZx8usH1K6A/vlyscYYOQnC6INrlPIIfOyPDPXh8eHVvsch6yfL9wnUV/Y6SarJi3cfS9hz\npNu+33tdJOUkm1qcxY/BpGYjLuLqLEVERxhjg56f6wRDLxonsTpQZvgeGz+GZ18/7TkbTDnJqrvE\nnXUjbtMrE8FNUH8pwSyYrDL8JjNRlSGL+w7xUviLoyCtpzuJOaafSTpJwugD84ZeNQ4CajJ0APVM\nXfw9FKuSTMrB5I47az67KOWAqOyGkt2Lbr18FSbSAHz7Gck+g+J3BRY1vWkH+eliU4XVwiRkqDA1\n9G3vunEbJ9EQujMneCWl+4aTGTPRXaLD780ftPFCNV/fA5mY2OF3LtTMylXVeSqdlzgxyUAw9VeL\nL2cveQoTvZRiqVZQTSePIH5OlcHjvg6qk8uvH3fFyTLA+HnzO9P+wIKuqvFWwX36YownrMuDMYSK\nE4QlytTJoASVyY6Ctm8l6OXzdjcF1nWJcsM11nklpsxXbVRIIpDNpBo6qygUS3ju9TNG1XmNKuJI\nEmnb77kxzTl+4r7VeHx4NQ5uuwNPberH1BV5wZcfeA73ut2vaa+beO5UAULZdTDBq2DuudfPGG/r\nUqGIiS/dWa02VlGcY6HcjDJmW8B7ECUJoUiyN+0YVQU3S3qi7Q29yYnjn/F7krnGOq/E3Do2ieUu\no//w7cu02xBxkoT1K5f4fjmI4wmCajbmPh9xFHG4x0yV8fiJJ5j0I80KncC47zsKUSsnSdg6Nunp\nOhLPneo+M50V96adGrkHXcBx3e7XfM22dVIPccNY8Hu42cju41+7vQ+TO+7E27s3YuJLd2Jyx53Y\ns6lfWxncrL6xbW/oTU4c/4zqs71pp+6ml7lXxIINrjXz+PBqbB7qqzPeBKBbuOC9aQebfmlZjQiX\nX9zf6ulOGmnsqF4s7vNh8vBnUg42Cw3KdS8tsZk5UHtOvXSFZI243aXzHHfBkaqtYhBmSsxzBeZ2\nP6nuM5MXvJMkbLx1aY3cgw4/sQsnQZiema2e04/2LWq44eWSz42GqDwDD8rPX9dT8zNDOYXSff8O\nD2SrqcR1Y4BZ/4I4aHsf/ciGFcomykDtQ2iSK24qPSz62x4fXm2UdeGnjN+ETLq7WtzFA9HvX56t\ny5qQFY/IfONe5f4A0LOgq+ZYdSqBoltGJXks81nKfNwvHcnViHqJXLOwq2YbjVoeE+QNJlT3mew6\nyILnUUkIJwj44EIHlwrFal0F91Xn8gWcNXiRxEFT9snC7ffH703V/U51/6rqdRgaq1gp0vaGfngg\nq02bWugkqjnn61cuwUJnPhfbXZXoV3rYr0GJ2gCdzReMc+cHb1xspO8ysmGFtgkEdxd45XuLrhTd\nsct+r/JJq4yfW3AsiO6L30bZukwhXU2C7DqInw1bmCYyx1Cjv+R2ZTXTY95u/WNVY5Xdvzqpa9O+\nylHT9oYeUBdyEFAzg3FnJuQLRezcexxA/ZvWRFLAr78tDuEpbpCTRHj49mV4fFge3PSr76Ibq/i5\n9SuX1KX1yVYLuuYb4osD8P9ClLmgPj826Uvj5f3Ls3V1DiYtBFXCd+tXLsH0zGw1vrNlbBK7XjmO\nHfesqnlByK5BlASNT8VNOxl5HTIbINM0ShAwNTMbuDdEWNreRw/IfcumMwYukSrztd2/Nqv0qwYR\nJnLrsYeFYf4YS4zhmUOn8dj4sZrPuDXcxePUZXN4+esLxRJ2vXK8nK4q/J6Aui5PQPkaqeIJbn+9\nn1iKygW1yKceeXGOoae7q0ae+JGhPul9xVsIitLJQK2Y2jOHTtetEC5OFzHy4lGMT+Sq12XL2GSs\nxURe8alWxq9LvRm+/1y+gOXb9qF/16u1NsRlfOZYvRpsI3XpO8LQyzTE/cwYZCfc3b0IqG8I4vdN\n7NZjjwMx1U6m4S4aVJ07RTynKi5OF6V597LjHB7I4pqF6gWkeA1UeuM77lkl1YqXXQeVfryOS4Ui\nDm67A29XegE8Pry6bn9PbeqvxiiC+NKLJYade4/XvCD84CSpJhjem3aUD7E7PuUny6Y3Hby3QhRk\nMylttbSMZq4SeKcwvsIzTQZo1EqrI1w3Mj+zqBBogkwrXWbEkkRVXyqgXnbJxhTmoppWJoqfUc3Y\nuSYP1313I6pwci14P0ZJdZxe14P7/3kl6EIngfx0fYWzyQs2iJtMNuvVCVIFvZ5h0j5nSwzPHjqN\nTNrBolS52rOstTRXcy3dFdzuuIHXnTTxpXJlbiP1YUT4vdBOFOeY7/PVqJWWSSvBPyOi94joH4Tf\nLSaibxHRjyr/7638nojoPxPRj4noDSL6aJyDB+Sz1pEXj+KSzxmdqVZ6ibH5/VTe4CZj2v7ysVDt\nzeYqvl8vRFeTlyFSTTrcLibVDFtVIKJqouE1QxTrFvKFIvLTRTwy1FfttKVC5p4Kkif+k/ev1Lm4\ndK6vZrhDuLvu4nS5FzADMF2sNfJi1azI8EC2umJRpaoC5ZcEv4ebYeQ5cTVSiRM/56uRuvQmrps/\nB/BJ1++2Afg2Y+wWAN+u/AwAv4pyQ/BbADwK4L9FM0w1qs5IfgJxquChF8U5Vg3meo2pUCyBsfr2\ne06CtAUWHJ1Sn4hYwBXUEO1741zNz9yNIxqHBV0J3L1mqdSYTs/M1r0AR/ef1M4iVXULzxw6jYEv\nv6qsTla9VIFyeqefHqZXZudqtvHY+DGt62v9yiWB3Btxu0RMfL86e8RXxM0UIut0kkSR697oMOkZ\n+3cALrh+/SkAX6v8+2sAhoXf/wUrcwhAhoiWRjVYGVH4uBZKGlSbzghly3DVmC4VinU+39EH12D0\ngTU1PldZ0NJkptDTnazJcQ9a/XhxuihdqYh9SvOFYlUn321ML07XB7h118krpnKxIkYlGmA+y/7C\n80e1DUN4tSJfDZlWJXtJR1R1kxTHs3moT2nQGzFH5q4P1YpT5z7aMjbp6WaLIrjXrlWyUTDHWENz\n6oP66H+OMXYOABhj54jousrvswBE4Y13K787h5gImrIoptJxwwQgsE/TZEzXV3LLVemPHJMuTDKm\nXUJdJgVQKtyFILoGDz0L6nuEuotJdPn2B7fdYRwHKBRLNemcJvIO7nOuqhFwo9u2KhDLj2d8IhdI\n0lekpzuJ6cpKMAiyFD6+AgoDrz/R9Qf2Isq2fe1Io91+UWfdyF7S0qtJRI8S0WEiOnz+fPBsFNms\n1UmSVhogSfW68LLlrolPU/Z7ky71OsT9zvl4GHhOujiL49vas6nfSC6Bk6uIeXF0GTomxVBe58SP\nr9LkjOgeJNOHTDX717nRuAgab9wdhky6O/T0331fR1F1my8UsWVsMrCR53pHVyvN6Bkb1ND/E3fJ\nVP7/XuX37wIQVb5uAHBWtgHG2NOMsUHG2OCSJcHzy2WplaMPrMHog2uk/tmUk/ScBcoCcDvuWVXn\nS3eSJG3N5x5TppJBsnVsUrmcVuH3za/SkBkeyGL0wTWQeKmUbBmbrOYHq8ZxfSal/FuCqDoO9znp\nTTtY0DV/Tg6/cwHJMGIkArw/rYqRDSs8b/yUk8TDty+Tur50RmpRyoksiMkztsIivpRaoXDqajPx\nmZRjlBIcJ0EN/V4An6n8+zMAvi78/jcq2TdDAC5xF0+ciDNgnqHh9s+KJ1mVvZIgwvJt+6pqhe7g\nnuhL5y8UXWs+Lpd7ZXauzs9sIua1bvdrWH6taqzy1QSgDsYdfucCJL2XtfCCsvUrlyhn5KpYQImx\nmmPl5+SRoT7khayRXL6AZw+djq5/p2Yz3C02h/rlpyhCVyiW8I2j53D/2qyxLznlJEEUrC2ejASR\n8vq7WXfzYuV9TYBnMVpE71iLCwJw95pYw5Rm4/DqMEVEzwH4OIAPAfgnADsAjAN4HkAfgNMAHmSM\nXSAiAvBfUM7SmQbwW4wxz9ZRjWolyAnSms6rC5IKle9Ztj2/48pmUsr4gbs93/hETiv+ZrIvUUDN\nnduuy7kWjzXIOLjEhR99FNPzK7bic5etA+bNMpJEePKhNaHOsQzTY+aiaaouU2LsQHaclughAL98\n82J8//Ql6T0XxazetMOUSdbNw4yxpYwxhzF2A2Psq4yxnzLGPsEYu6Xy/wuVzzLG2G8zxm5mjK02\nMfJRost5FnG7EUwyMYIueVXfc/vAuaH08/LRLe3dOfte6Y0m+xJXKQBqXFHDA1llPMHdlMOvkT+4\n7Q70ph1f3/Mjlja6/2S5mlFi/EwrHHkWRdRBNtNj5oFxL/EtnYxuUHrTjlGNh4pMygnco6FV4VXU\np35aMGr6EzcdURkL6AW6vES+lmukdjmLDPKxxydyNZkIvbx6URG0ErMfgvp1VUHB9y/PVg0woH9R\ncf0WXRNwbsCCCKGZNOXg45CJo41P5Hy3gru+UvRjIj0dhd9abOgR9azeFF3WkngNVDK6QUgmCIwF\nL25yEoSd95bjXM1s3h01PEYU5z3nh44x9Dv31hd4mPZoNEn1ck843EZk/colGPvumZoZ4MXpIpIJ\nUi7/xTd7EL+ubsS8HBuAVk4YKBv5x4dXY/DGxUrpCF4E5SWEJnONuJtyqMbxyzcvxqmfFupcQ0FK\n4devXFL3QtK5QXQvZC9SThLrVy6pyjd0JeA7FhInfq6BHwgAYyyUpAPvJTA+kUOC1NXa7YbXS6vR\n6ZUdYeh1BSAmb06TmbRo/GSzWpVvtDTH8EGN1nmcb3YeDAXksst8Js+LrMRm1O4caV5roHohcdcO\nINdh54xsWKF8CI6f/Rkmd9wp3bZfvnH0nHGjcwbgny8XjQ0NVXqFch2e9SuX1KyGWsnIuzVvADMJ\nbi9STrIivRsyVbNSnLf95WPGRp4AaZOddiJqJVsvPIOxjSBsMFZXbKMKyonGaFrovKMiSYQ3n7jL\nc3+676sClUC8uh6qmWzaSeAP7rtVuuJRHaPX6kfUxVehc5WJXZsAaItyom5ewbOYTNxEWdeKo1V1\nWXjAXhY8FzuT5StZYTpEvX3dC9sPSSJ8YGF9wZ3XGNqdoMkdbkyDsR1h6HWVjns29ddVnbpnM06C\nAKrXi3ZzqpLFYlpZ6QWPvh9+54JyRSAjSgPnJAmbfmkZDpw4X2MQvNozes0GNw/1KbtamcRETMcd\ntvpURMxUGp/IeRoyJ0G4ZmGX7/hBs+D3jfiS8pvpxY/Z5MVg0XPKpwyzDFND3xGum4zCNdLTnayb\nrUpF0OYYMikHPQu6tCsDThgfZ5IIc4zVzFpVmimqccjiAUEpVmRv+ZZEpU3ZOXWnWapG8Myh09Lt\nAv7b9klhwOCNi7HvjXPSbfWmHVwuztX1Z9W9zDPp+VZvJsnzxTlmfByt0DpPdi38Vsr6OWaLGl7b\n0DKiZu3AZcWNKkv30wmOcakAL/mCoGJhfExiYZefB42AakMMd+Vvb9rBZklHJBPcZ0mltCnK3/IC\nNb/bHd1/Ulpl7JfiHKtuy7RJiS6tMEHlTCVeKBflQjflJPGI0CyEi56FSUkMC78WrVApGzVeDVla\nAQY0NMWyI2b0BUX0S/Z71Wycl+ubBBTdn/EjPGaqe+/1XZU4mugu8TMuN9xnygOUvWkHV4SmJZmU\nE6jijwdtD79zIbTbxSQA7BYzU/HBhcGzbtyknQQWOElp0xQ3ftyAfDVo6lP3QuxzK9tXs3zhUez7\nSrHkS6q8GTTyJdsRht4PqowDMUNFzD4Z3X8SW8cmq+mDogFxKwKK20wAdTeauDLg2/ZzO1+cKjfG\n0BkP97jCBsy4d8i9XM8XioEMNc9vf+mIud6PbluAvguU+/OqKuUoH7piieEP7lsVeSesOcbw1KZ+\njO4/6ek+yaQcXJmd81wtygyqWC0cNjsnCGGNPG/G0uo0MsWylVc3xvhRluRVsbJKPDGv3avfqhtR\n015m5MWm2e6m0qZMF+eMxsIZHshqOwk1GgKw/NqU7wpgGU5CL1wmQ6eg6fehSzsJpftJ1ZBGhFdx\n8/x+ExjK+dle9w0vQhJ1nbz2kSSqupXuX5utTnAWdCVa6h7qFILcv2HoCEPvR1kSgFG5vq4wSIQb\nbXGGJZtLMMw3zY5CKta0jFrmw24WDMDBNy9E4hLghTZ+kCmdcs0Rv3EXhnLWjwqdG8j9oo/cQVJ5\nFHgs5dTujXhKaL4io1SJHY1sWIGXjuRqWjpeboHZsR+J7XYgyP0bho5w3Zj41d14levrNGp4Zsb1\nmRSmrswaG22+zajcBCbb4efAxIXTTjnKeYXrQtaU3R1f0TV+EQOUujPBtWV0DHz51eoEgDfr0AXg\neeZXNbe9UAwUFC6WGLa43I38v5u3f1N6jbkZVU1wghLVPXXNwi5cLpaU8bh2Q3X/xkVHGHqvh1v2\nuUylZZ+Yoij60FUvAt7AGvBf5MRfIlGVoJu6G7hx0e2TAAx9uBcH33R3jWxNZFo27rTTXL6Az49N\nYtcrx7WBUfd2HhnqM4o/5PIF9HQnldWh4iqPN+s4/M4FbebX5I47q+MJm8aYyxewtRI852mxKqPL\nUD4PURd+RTVxuDhdhJOgampuK6SrhqHREghtXzClk571LJRKEnq6y1V57qo/oD4QFfbm2jzUhwMn\nzkfyMPmVOjUpjEk5SXy0b5EvY8/PW1QPXtpJoFCJRejYPNQXqJbAfd4eGz9WV6wWpxHh5fu6GoW4\nAqApJ4kFXQmlW6k7SZix8sWx05Iyxa2OqS9dWihVYiCq7TolFpO4/blhHoFbruup+j5l+LkQQbrU\nmEgzF4olnPppoaZZi1c+cpRGPuUk0d2VNNrWS0feDVQw5g64yyqS4zR1PEdfFRSOIn6jolAs1Ynz\niVgj3xia0WGq7V03Jv1KdZ+Tzay4MeBFTRyVpklv2kG6W11V29OdxPSMPtVtoZMwTgmTaWSYuK9E\n/7Qqp5znpvtpVu7HPKheCkki3L82a5yyGcZXKwbcm2Ha8oUierrnDb3ov98as1RvfroYTWWyJRDZ\nTKrhRh7ogBm9Sb9S3edUyF4MKsW5jbcuVVbVOkmCk0x4umvC5P0+Nn5M2v7QnX4pNmZJKKZ2svMU\ntFm5ew+6mX+JsUhy603wCrgD9WMPW8nrRvTrX5ktX/uyVG+82SXXZ1LKbDRL/DS6KTinI330HFGA\nSSZrmnKSIDCpkZWpy5m0BXQHfOOQUt1TKZrh+1DNzvhK42y+gEUpB1Mzs1qtF7FQxr064L8zjS84\nCQJRvTtAN6P3E7gLo12echLVlEHZJrh8syj0NnVlNrLK2WYhXt9OavLRbsiko4PSEPVKItoK4H9G\n+Xk5BuC3ACwF8FcAFgP4PoBfZ4zN6LYTVr1S169UhAdfLxWK0iwN8XOyxt+qcnV3f1ZOHPK1t1zX\ng3cvXo7MjysTWXO/OJMJiq5xtwQTNUw3625ejO+euhhL79N1Ny/Gs5/7WM3vgiiWpp0ECrNzkerm\nBKUZsspeInJXM1EFZGMPxhJRFsC/AzDIGPtFAEkAnwbwhwCeYozdAuAigM8G3YcpugIokWKJoWdB\nV1VU7MCJ89LZdk+3vJhB5f5R/V7nGuDB0Gwmpaw8XNBVe3nW3bzY09fvFxORtTiMPHdQ8MCy3+rL\n75++hE2/tKwmWB6V0+P7py/Vub2CpMNNF1vDyHMxPJO2klGRIGD0gTXV65IxaMV5NdHovrFhffRd\nAFJE1AUgDeAcgDsAvFj5+9cADIfchxGmD6J4k+vymWXIqid5Xr27GbnO35rNpPDmE3fhVMXAyqpX\nCcCDgzfg1O6N1f+e/dzHIn9Iw4ishYGhfIz56RlsGZv0HRzkBUtis/KobGqhIt4mXtNm+VajgNcc\neMVnomSOAV/862OYujILAOhZ0IW00/YhwUhpC1EzxliOiP5PAKcBFAC8CuAIgDxjbLbysXcBSNcm\nRPQogEcBoK+vL+gwqpjmH4tNrnXqjjdt24dM2gFjqMmzz6QcLHQSdUUbYlomoG/2PXWltnH38EAW\nLxw+XZO/zlDWqR+8cXHN6iJMb1M3bvllILpiLhMYwrWiO5sv+G6c4Qf3NW1XfvL+lRqffKOqn8vX\ndr7dpqWWRhZNBTb0RNQL4FMAbgKQB/ACgF+VfFR6VzHGngbwNFD20QcdB4cbQ13rOW7YuHHQVQkC\ntamX/LP5QhEpJ1nuGeraj2mz73yh3H/18DsXtAVUsubmUU3GZAGh8YlcdQZmSm/awY57VjWtpZ6s\nKXyU8Nm9rO5ABQGeDcJNUxyjqFHgWT2W1qJdRM3+NYC3GWPnGWNFAC8D+GUAmYorBwBuAHA25Bh9\n4b6p3b5gv80+ZBSKJW0zcpMlWaFYwrOHTnsaRve2wuY/p5wk9mzqr6sR4C8/93H1dCe1glKi4FWY\nhixB4KJbjcB0FtybdvDIUJ/nG7lRRt5iAcIZ+tMAhogoTUQE4BMAfgDgAIAHKp/5DICvhxuiOTID\nzntkNioYdX0mZbwkM3mI3dvyM7OUwVcJ7mCjUmgr3Y3RB9colQ8LxRJ2vVKW5OXVt60QeMukHG0V\nMKc37VQrgYMgyvvu2dSPiS/dWQ7y+8w24d+PqhLb0vq0RTCWMfY6ykHX76OcWplA2RXzuwA+T0Q/\nBnAtgK9GME4jTKtko/CNOYl66VSuMR3VzJZQu7wbn8hF4l+VFVTpzh0vmFKZy4vTxZptTc34c/9E\nDQG4e83SapHXkw+tka5KuJQ1lyn2WxSVIODh25dV4xpfeP4olm/bF8h9lRPOMw8uWzqbtgjGAgBj\nbAeAHa5fvwXgtjDbDYqX9DAQzA8to+yxcBldQQcc8FdgJOORob66LlZR4fb/m5w7XaB2597jGB7I\nYtcrxxueO72gK4GZ2XkhNIZyc/J9b5yrGnI+Ru7qEWMLQPla+R33HEONZEOYlzABNY1IGnkGEwQs\n7DKX4LBEQ1sEY1sRWeaNu32fLEMjqgerWGJV48n/W67pU6rDSRIGb1xc/TkOsStxRuF17vhnVBWV\n3IDGoaFClf9UZkg08iIXp4t17SFVNLtJNpeu4P9uJDcvKRfhWRqLSlIlDjoqsVXXQQhQG0uG8qwm\nCtwz3qC+X/7S4MRhiDJpp5pbPbr/JO5fm1Weu2bCACzSFFR5NQgx8YU2Wh+8lfjRe1MN7wtrgWfj\nmijpqBk9UN8cm/e+9MoPj6r4kyr7BeZdN+4Vg5MggODpKhCNu9/89kzKweSOO7Urivcvz1Zn4Ll8\nAS8dySmNu4nrqH/Xq8bj88vF6SKyAXP8TV6S61cuqZMsDqsNk/ahSGq5+mgbH32roRMUa2QR0Bf/\n+hjm2HwuPa8C5RlAIxtW4PA7F/Dc62e0fl1xlrl+5RJjCV+gvO91u1/TfsYt/yDL2+eYuI7iTnWc\nujIbSD9FN1sfn8jh/3j5DalB5udjZMMKOB558TLa3chzHaSUfWHFQiNXkR3juhEbLjOUZ4BRq0aa\nMjVT0qZ5AuWqV52Rd/vH/S7zrszORTr7DTv7iOJGyxeKKAUI9Kp8oeMTOYy8cFRrxHiG0qbbwldv\ntxtDH+7FU5v6rTBZDMiq0uOkY2b0cXbm4SSJ8PDtyzB44+JAGTW5fKHc8IMgFbtyK0mKM+uol3kq\nWWBZL9aRDStCSy9ENR8Msp1nDp3GM4dO11UDj+4/aTQZ4Lo6V1vDjoNvXmibHsLtRlQxQeP9NXZ3\n8dEIf5fYHOPgtjuwecj/LI+3kpPhVpLkQlTLA0jkeiEz8iknieXXprDF1cRky9hk22uxA/PNsh8b\nL8ca/NwzZ/MFqficxRKEqZkSRl48Wle4GBcdM6PXNeCIEq59snPvcfzzZfX+gqRsioJru1453pDj\n4RMLrs/vJw4QBFMfu99GJKYwAM8eOo3BGxf7ume44qOfdocWiw4xHTtuOsbQq2wCN2Q6pcogeM1w\nxQ5FpnsVBdcale7GAJyqNE3xCt5GgZMgzJaY8pyIDRniOhcMlX6xPm6HEmPY/vIxLLRSu5YIaVTm\nTccYepWGPA+CNlpVkc/6Egp/vAyuANmsnOZG3HS6wKfYJJvHCeI6F0GOtVCsD7LriGtVYukcGpV5\n0zGGXpdn3kwtbL+JP80Y68CXX8XGW5eCfLyU4kBskh33qoY/YHGd72wmFeuLM0zPXEvr0KjMm45Z\nh0YtkduME8ODhI3m4nQRzxw6Hbvh4A3DVfD4x9axydhXNetXLolVVnnqyiwyPtsjmpIg4Ndu70Oy\n0akblkhZ0JVoWOV5xxh6Ln8QVsaXsyjt+O5jGpZ2CfIFdVNfs7DLaMXQiIkqz57ila9Rky8Ule7E\nsMwxYPDGxfjAgo5ZkF+VXJmda9jkjlgL+BAHBwfZ4cOHI9nWTRGlIhLiaavXm3ZwpVhq60rDMCJw\nYfzWCQKWLor2miSJ0N1FKLTZ9bjacvo7lSQR3nzirsDfJ6IjjLFBr891zIyeE1VwI5N2YvGxMgb8\nwX23KrXd24FFIRqLhAlOzrGyyyXKc1dirO2MPBCPSqil8TQqWN9xhj4qvytj+pdGUBcR7xcbl/+2\nETSzeOoZl/AYJ0mEzUN9DXe3WSxhiMrV7EXHGXruq5c98ARg3c2LjV4ElwrFipiVvDPRw7cvC/xC\nKRRLYAx13/fb4SgqEoS2N5JzjOHx4dWY+NKd2BNxhyZCOfWzWdfH0rk8fPuyhuwnlKEnogwRvUhE\nJ4joh0T0MSJaTETfIqIfVf7fG9VgTRkeyFYfeFFf/alN/Xj2cx/D/Wuznm/S6zOpciPxB9fU9EDt\nTTsYfWANHh9eXaN97/fFfKlQrNPOH31gje9jjYI5Vq4WTXd3Yc+m/rZ0K4mrrygzGbKZFN7evRGT\nO+7E6ANrGjYDs3Q+m4f68PhwPMkAbsLO6P8IwP9gjK0EsAbADwFsA/BtxtgtAL5d+bkpuHtwbh2b\nRP+uVzH2Pb08sKgsNzyQxc57V1UNcrp7PtOBb//t3Rt9Rye5YZq6MlvVlNn1ynGkmlR5ycfQjm4l\nd29dIJolsXu7wwNZZXip75YAAB1tSURBVP9Zi8UvYge5uAmcdUNEHwRwFMCHmbARIjoJ4OOMsXNE\ntBTA3zLGtFUBUWbduPFbfJOtaL5w+YJFKQdTM7M1+iximT6nf9erxr7rsK0Le7qTcJIJXCoUtdIO\nToJwzcIu34G7ZmXVBIFQlptwz4weGz8WWbqqqHo5PpHDyItHI5HubXRvWEtrIcqWB6URWTcfBnAe\nwP9FRBNE9KdE1APg5xhj5wCg8v/rQuwjFOMTOXzh+aO+jPzIhhV46Uiuqt6YLxTrHmqxPR1XmPQT\noAzzcPemHRz/8icxueNOvL17o9aojj64BjvuWeV7H0HHl0k5mPNh5BPkHZfYPNSnjYWku5PSmdHj\nw6uxeaivRg426GKJr3S4LEMURj6bSVkjf5XTyCr4MIa+C8BHAfw3xtgAgCn4cNMQ0aNEdJiIDp8/\nH33vRD6T9zO7PJsvGOurnM0XapqdNAr37FzlokgSeTbEjpp8oejLeM0xoKe7S3kMhPLyVlcINzVT\nqhphN4M3LsaCrvmXRJgsSv5yD5tyu3moD6cqUtRB+wlbLH4JU1r3LoB3GWOvV35+EWVD/09EtFRw\n3bwn+zJj7GkATwNl102IcUgJIoh1vQ99kuszqYY0O3HDDR6fXapeZOLvMyGbhsRJvlDE5qE+qZuF\nq0we3HYHtmp6thaKJXzh+aPVn7lBjlqxNJcvoKc7iamZYNc8QeVOYcu37bOCZ5aGEnhGzxj7RwBn\niIj73z8B4AcA9gL4TOV3nwHw9VAjDIjfmRcPwJoUXPHPNrK5L6fEmNFKQmxSvvNevfum2bHFse+d\nUf6Nn2Ov61JiDJ8fm6xpmhKHIQ1q5IHyCoZfM2vkLY0kbIrH7wB4lojeANAP4A8A7AbwK0T0IwC/\nUvm54ZhkjiSJqqmNXPNkema27nNOgtDTPe8CWNBVPm064xPUePZ0J7X57FnDlQSfDQPlbBHVNrOZ\nFL7yUH9TM0l0Pm9ehWtSCNd+9a2Wq51GdZgKZegZY5OMsUHG2K2MsWHG2EXG2E8ZY59gjN1S+X9T\nmk6aTJjE1n0AsP3lY3U+8EzKwabbltUoO/Lq1vUrl9QZn5STxJ5N/WXjGaDAZnqmVK0BkG3bz0pC\n/JysDR7f3vBAFtcsbE2BrKmZWYxP5KqFcJkQ8gsWS6vBJ2Nx05pPdwSYKAeKs37VLDlfKOK51+vz\n7nnD6CfuW13XSFsMgPIm4qapdAmiGh+u+H/uizZd9LuLiA6/c6F6LEki3L92Plibb1HtlGKJYcvY\nJA6/cwGPD893nto6NtmyWStJIgx9uBeH3rpoXTQWLbbDVEhMlCcvTRerSyfdZ1UPK/+OLBeWB0v9\nGHlxX17/NyGXL2Dd7teqRT8vHcnVbOelIzkM3rgYwwPZWJQ6/eAkCEWNID4P1j4+vBqH37nQskY+\nk3JwqVDEqZ8WrJG3eGI7TIVgfCKHqSv1vnY3cwC2v/wGZkPkRW9/uawnzWfGssbeJluPKwsjly9g\n69gkFjqJOpVGnjI4PJDFyIYVDe1V60Zn5Dl/+fppPD68Gs+9rg7eNhue3dTMl6alfVi/cklD9tNx\nomY8I8U0nbBQnDMyMurv1xZPyfz8OrhP30+hkV8YoJTi5UtH7gNv5dxufpmCvhCDxEys2IElTg6c\niL6GSEbHGfqde483fFaayxewfNs+bPHZAo9n+3DXSTNIEFXdV1y7p9UJqmMz+sAaXwqdmZSDR4b6\naoTnLJYoaZSPvqMM/fhErmULg2Rwo7pu92tNW+qXWDnY+ZH/+DcNS/UKw/hEDkMf9i+ImnISdS41\nL/KFIsa+dwYjG1ZUs7OssbdEifXRB2DXK8ebPQRjspmUb8G1OJkuzmHkxXJ1aStX0m5/+Q1cDqBl\nUCjOBeokxbN+tlQqc8V6CoslLG7V1bjoqBl9u7RX4/nrzZBQ0FEsMYzuP4md965qWSneQnGuqRk3\nUzOlplcSWzqHRmlRdcSMnqcythpcDRMQ8umpHMDdotFuaSZn84XqzcfH7M7pv9qZY1Zi2NJetL2h\nbyX3hxsxsHn4nQvlfqcxWodkgpCAWaqiCgZUc+/dgdmbtu0LN8AOwhp5SxQ88iffwbOf+1js+2l7\n102ruT84YtAuyiYYKhIEPHzbMmy6LXwPSlF/HZjX3G+mcbPuEksncvDNCw1Jgmj7GX0zFCS9EFsR\njk/kYjfyQNmdMPbdM5Elfov1AUFWTATgl29ejL9/60Ikq5gQixSLpaXhRYtx0vYz+mbln+vgRrLR\nsYPiHNMqQYqz4kzKwZ5N/Ti1e6Py3ZDLF3zXBnAYgB+c+5n1cVgsHjRistr2M3pZ6X7KSeKjfYtw\n8M2mCGcCmDeSrYK7zy1/CcU5xovTRfSmnbbJhrJYmoGJpHpY2t7QixkiXEFy+bWpphr5uOhNO9h4\n61J84+g5X3nuXKly1yvHG/7ysUbeYtFzuQExxrY39ADqeqPevP2bTRyNOQmS+543D/Vh8MbFUvnj\n8YkcXjriL3jz8O3LMPa9M5E0tbZYLNESpJDPLx1h6N34yfXOZlI4uO0O3LR9X6ypj2560w523LOq\npiw/k3Kw895V1ZeWLEDjN8uoN+3gwInzLWHkk0RIEAvVpNtisfinIww99zfz2a+fYpaz+QIeGz/W\nUCMPlBt9uFciKr95ykng/rU34MCJ8741cS5OFyNxn6ScJBZ0JUJJI5QYQwu8byyWlsKP0F5QQht6\nIkoCOAwgxxi7m4huAvBXABYD+D6AX2eMzYTdjwp3wZRvcTBCQ9If3Vxf0brh1acqNw5QXto1Y4wc\nscJ35IWjoQqyLBZLLRtvXRr7PqJIr/z3AH4o/PyHAJ5ijN0C4CKAz0awDyVhC6aimslvFuRsMylH\nm86ecpJYv3IJtr98rPpialXbSShX+PLVR6v2lrVY2pVGaNKHMvREdAOAjQD+tPIzAbgDwIuVj3wN\nwHCYfXjRCgVTmVTZD85dRzvvXaX9/BP3rcaBE+dbsqLXjbtOoVV7y1os7UojbFjYGf0eAP8B5a58\nAHAtgDxjjPfxexeAtOSLiB4losNEdPj8+eBvtGYXTDkJwtTMLHL5Ahjm5QN0ubHcXdMOuGVUm32+\nLZZOoxHPVGBDT0R3A3iPMXZE/LXko1KnBGPsacbYIGNscMmS4H0TRzasQMppjkY4odyezp3RUiiW\nwJi6dV27GHmgrP8uat6Y9OK1WCzmNKJvbJgZ/ToA9xLRKZSDr3egPMPPEBF35N4A4GyoEXrAe51m\nUvFHrt0wlBt2yLhUKKKnu/392YXiHEZeOIrHxo/56sXbCLKZFPZs6m/2MCyWUHzj6LnY9xHY0DPG\ntjPGbmCMLQfwaQCvMcYeAXAAwAOVj30GwNdDj9KD4YEseha0llG9PpPCpRYyimEozjE89/qZlosp\nnM0XWrIPgcXih0ZMnuIQNftdAJ8noh+j7LP/agz7qCPOgMap3RuVvUJ7047UdZSfnmmIhkWjaMWG\nI9dnUm3lBrNYmkUkhp4x9reMsbsr/36LMXYbY+znGWMPMsauRLEPL+IMaPTvehUXp+oPI+UksfHW\npSjN1btvpmZKuFQoKv30nUzaaYwoaitkXFksYWlEwVTbyxRz4gxo5AvFOl98uUk0wzOHTmNGUe45\nx9CyvVfjpLsr2ZCbt/XWGBaLf3bco0/HjoLWcmwHJIjQV1imZ0pGhkYVrG0F4up72koBW4ul1WlE\ng/COmNE3o51gJ8wmGcpCY3EQ13YtFot/OsLQW19tcOIKsrZi8NZiaUUa0TO2Iwy9n0BsymmM//hq\nJ0mEBsVkLZa2phEpwh3xKMqqY7njoDftVEXGspkUnrhvNXbcs0oZJLUOh2goMYauZEfcXhZLrLSD\n1k1LwKtjuXpkNpPCU5XG1zvuWVUtppq6MotdrxzH1rFJ9CzoqkkD7E07HVll2cwXVyM651gs7U4j\ntG46IusGqO8dO7r/JA6/cwEvHclVA7ViNki+UKyZ9TMGbB2bjC8VpUl00KFYLB2JWzgwDjrG0Msa\nkDx76LTW0PG/1XRgspbRYrE0EJte6QNZiqW12ebw1U02k8K6mxc3dSwWy9XEI3/yndj30TGG3qZY\nhoO/FKeuzOLv37rQ1LFYLFcTB9+M/3nrGENvG2JEQ75QbHijdIvFEi8dY+iXX2sNvcViscjoGEN/\n6K2LzR7CVYX141ss7UPHGHpbct84Uk4Cp35qYyIWS7vQMYbeamg1jkJxzjb8sFjaiI4w9OMTORtA\ntFgsFgWBDT0RLSOiA0T0QyI6TkT/vvL7xUT0LSL6UeX/vdENV86uV47HvQuLxWJpW8LM6GcBfIEx\n9i8ADAH4bSL6CIBtAL7NGLsFwLcrP8dKTWWrxWKxWGoIbOgZY+cYY9+v/PtnAH4IIAvgUwC+VvnY\n1wAMhx2kxWKxWIITiY+eiJYDGADwOoCfY4ydA8ovAwDXRbEPHZmU1Ze3WCwWFaENPRFdA+AlAFsY\nY//s43uPEtFhIjp8/vz5UGPYee8qhOnBbXPCLRZLJxPK0BORg7KRf5Yx9nLl1/9EREsrf18K4D3Z\ndxljTzPGBhljg0uWLAkzDAwPZPGVh/p9G3sCsGdTP5793MdseqbFYulYwmTdEICvAvghY+wrwp/2\nAvhM5d+fAfD14MMzhxt7d6cpFSkniac29VclQq+m9MyUkzA+TyoyKSf0NmQkE9QZOb8WSwsR5pla\nB+DXAdxBRJOV/+4CsBvArxDRjwD8SuXnhsA7Tcl6wiYTVNdSUNSBzmpE0Uyle51k441UykmAUO7R\naoKTIDxx363VjlwQvmu6qkk5Sey8d1VNV6/etIOUq0lsT3eyGj+p7sP195Sry9eTD67BVzb118Vd\nUk6iel35trKZFDYP9cV+znu6k577sAtCSytDrAWmsoODg+zw4cORbnN8IlftNnV9JoWRDSu0Av/u\nxiVA2aCJL4THxo/VNTPhDamylX0AqNnv9MysZ/pnd5IwU/J3Hdxjk42fjy1JhBJj1THqzsNN2/Yp\ndfwJZZXQ9SuX4MCJ88bn1g9+rxv/zs69x2s6iHmRJMIcY9V9ANDu172PBAFzDHXnVDZ+AMprw7fj\nBd/P6P6TgauSxfvAfQ2XX5vC3795oXrtF3SVV31+zqklGGkngR/83q8G+i4RHWGMDXp+rlMNfRBM\njIxfQ6QznMC8wT78zgU89/oZlBhDkghDH+7FD879TPqS6E072HHPqtBjk/HY+DE8c+h03e83D/Xh\n8eHVRi/EoITdtte5DrLNqFC9AEZeOIqiwtLLXsyqYyQAT23ql15/r/Mq+7ts+823FJ3Lqd0bA33P\nGvoWYd3u15QzMK8Ztuq77tlo1AbrsfFjNS+dh29fhseHV2vHlM2kcHDbHaH2G3bbqu9nUg56FnTF\nsgIJg9e9ITvmIPeE7jtPPrTGeJWQzaRCaRzZl4WauA19x/SMBaKZ0UY9hvUrl9Q0KAfMZ5Sqh4or\ndebyBWx/+RgA776Tfs7N48Orq4bdjaqTl2mHL904wm57ZMMK6cx15731q59WQHdcqr/JjhHQ3xOq\nbZUY85zJu8cU1Njzez6M66lTSTvxR/Y6JsGBLz9z+QIY5m/48Ylc4O2t2/0abtq2D+t2v2a0HdkY\nXjqSw/1rs9WgpSwQrNqWSYCvUCxhdP9J3+PaMjaJ/l2v+j4/qk5eJh2+vK5RmG0D88F41bkOck3j\nRHdcqr+5j1EWhHffE7r9FIol40A+fzGrsq2cBMFJ1m+LKvv5wvNHrZGX0N0Vffaam46Z0cuag/Mb\n3u9szu2zNJ05q8Zw4MR5326N0f0njZe5XjNe2biActtA0xUBRzVr5j5nv+MQr1GYbXOGB7LSYwl6\nTeNkZMMKqY/eSZL2mMVjvGnbPulnxHtCtQrglBhDyklqZ/b8OvD98pm5O9DP/3Y2X0Am7eD9y7PV\n42tmz4hWdhtdakDAu2MMvcrY5fIFrNv9mi93TtCXRljXQ9DveM14ddvy+zIUH3S/LjKv8xNm215E\nORGICr5fMZtHFmhXBXJ1kwHxnuDb+sLzR6XGNpNysPPeVcq/J6ichbN1bBKj+09iZMMK7cSF72/d\n7tciERxMEPDBhU6oDKBWNfJAY/pdd4yhv17hOyTM+7rdsziVv9jrpaEyPqoxBLmQuuMRb1qTGa9q\nWxy/LyLVrNmLTNqRPvhuoxSH4Y3yJaxCFp/xSkP1Ol7ZSmTkhaMAAUVFSq7snuD7kK0gpmZmAQBP\nPrSmbubvJAlgqBpZPyuhqM4tY8DkjjuNs6raCb8r1qB0jI9e5juULdf4LE7nL9YZZp3vXzaGoBdS\nta1Hhvp8+/t1flWgMTOK8Ykc3r88W/d7LzeF+P0g/nX+PZOZr5/9un//2PixuvvpmUOnQ8eMZCuR\n4hxTGnndPTE8kMU1C+vndsUSw65Xjlf3JRak9XR31b0YTOJCQHT31aJK8ZzX9kxjDa3E/Wvjmdi4\n6ZgZvWzZr5rFns0XtEt5L5+maskfpeshjm3teuV43Yy6UTOK0f0npfniPd1dRoHpIP51r/xwr2NX\n7ffwOxdqMqly+UJdIZ2MIK4iP7NiAjxjQXmFK+XidLF6b3Cf/ciGFdg6Nhl4XOtXLpHWZPjlZ1dm\nMT6RM4o17NnUjy2KMbciY987g8EbF8du7Ds6j16Xl322MtNyQwDe3r2xugxXvSz459qNZqWg6gp9\nvM5j0FqEMDUMuu/zAGQQ/N43umNwY1Jv4Hd7gDzNN+p9ecFz/gEoDXmSCG8+cRcGvvyqr9hAmOsZ\nBWFqUEzz6DvGdSND50rxSuUbHsji4LY7lBo4jXB3RIXoZuArlrd3b8TBbXc0LBAZJnVSN3vUuURU\n3+Mz36A+5jBGwe994+V246ScJNavXOLp3jLdHlA+t1NXZutSJk1XgVHGP3jOv9dnAGDHPat8aQ81\n08gD0Z4nFR1t6HV51ab+9Cj97n6IKuc76vqCoIQ5j17GUeUzzkjE7QAABKPjV+1X5Qv2Mi5B7hvx\nHlbtI5NycP/aLF46kvO8zrJnQte4J18oolhiVQnwbCaF+9dmMbr/pOe9GfVkqFAsYededX/o3rRT\n1SRqvp/CnEZMGjvadeOFqRuj0e6OKPVk4pQs8EvQ82iqxeJ2ifTvelWZkmdyPlXXgRtV2e/FLJuo\nxd901xII7mIxOb+A/thl53J8IoeRF48qA8dRk3ISmC0xpXZQK5Ig4CsP9Qe+L65KCQS/mKbyxZXy\npyLKnO9GpBWaEvQ8uot0ZMhmRbpCFJPzqQuID9642Pckga86gt5LQa6lyXX2yrPnFIqlqgaS+/eq\nc3nNgq4af3km5eDuNUurL0BVym0QCsW5SLbTSBalHJt1c7USpXGOMre/mfCXhGqWLXOJRFE/oHo5\nmby0oq7G9bqWYa7z8EBWmWEjonoR5PIFjE/ktLLZbt0h7mbxw4KuBK7Mtp9BVxHVS86Lq8rQt4Lo\nmQlRGucoZAXiIOi18JN2OrJhhdZ1ENfLTpex5Z79+jkPXtdS9zeT/Xi9GAF9hsqWsUlsGZusNojR\nrUpN3UVurszOIZkglNrIPdMKXDWGvhW1TlREaZzjlBUISthr4cflpmpIQkAsLzsTA8ZXEn7Pg8m1\nNNGjV+3HK+9d5aN3o5ul8mNX6S+ZsCBJmLaG3hexGXoi+iSAPwKQBPCnjLGGtRSU0YpaJyqiNs6N\njjF4EfW10M1WVX56hnhe8CYGjKEcKL50uVjXq9jrPOiupepvpuf7wInzyjFnXfGJoEVJ6e5y5lWY\nGNF0G/riVTSqmjcWQ09ESQD/FeWese8C+B4R7WWM/SCO/ZnQSkFJE1rNOAflV77yt/jRe1PVn2+5\nrifSa+E1W1W5I3Q9gsNgegw6ga6o70nT8+1Vd8AZHsgGNvRTMyU8Nn7MyE10NfDw7csasp+48uhv\nA/BjxthbjLEZAH8F4FMx7cuIsFrnFv+4jTwA/Oi9KSQT8llMkGuhm60Cja+DiOJ+ivqeNL33/Twj\nutx7L557/Yyvwi2RlJMMte9WgTDfnrMRxOW6yQI4I/z8LoDbY9qXEa0alOxk3EaeMztXr38e9Fo0\nU/pYhpceixdx3JOm976fZ2Tnvau0/W6dJCmD4CXGpNdl+bUpHHzzQs1nEwAWpR3kp4vaZutOgrSK\nnnGi0rq/5boefOvzH2/waOTEZehlU7aac0FEjwJ4FAD6+vpiGsY8rRiUvJrhbeXCXguTDKVGusHc\nOf88S8VETyVJFEvTctN7388z4j5O0dhxTX1VXj73S8uui58sJJVG/9l8AYtSDmZmSzX+/J7uJJxk\nAvlCEQkC+DsqAUD0+vMUTn7NetMOLhdL1Tx9Wc+AR/7kOzUvqXU3L8azn/uYdNzNIJbKWCL6GICd\njLENlZ+3AwBj7AnZ5zu5OfjVzHJF9yMgeDNkN1FWEceJiZJmq405LI+NH5Nm8TTSZdHpNLsy9nsA\nbiGimwDkAHwawK/FtC9Li3LLdT1S980t1/VEto92Wam5x7ko5YAINS6JVhtzWLgx59W0SSI8fPsy\na+SbQGxaN0R0F4A9KKdX/hlj7PdVn7Uz+s5FlnXTKn5Li6XdafaMHoyxbwL4Zlzbt7QH1qhbLM2n\no2WKLRaLxWINvcVisXQ81tBbLBZLh2MNvcVisXQ41tBbLBZLh9MSrQSJ6DyAdyLY1IcA/CSC7bQL\n9ng7m6vpeK+mYwWiO94bGWNLvD7UEoY+KojosElOaadgj7ezuZqO92o6VqDxx2tdNxaLxdLhWENv\nsVgsHU6nGfqnmz2ABmOPt7O5mo73ajpWoMHH21E+eovFYrHU02kzeovFYrG4aEtDT0SfJKKTRPRj\nItom+fsCIhqr/P11Ilre+FFGh8Hxfp6IfkBEbxDRt4noxmaMMwq8jlX43ANExIiorTM1TI6XiB6q\nXN/jRPSXjR5jlBjcy31EdICIJir3813NGGcUENGfEdF7RPQPir8TEf3nyrl4g4g+GttgGGNt9R/K\nssdvAvgwgG4ARwF8xPWZ/xXAH1f+/WkAY80ed8zHux5AuvLvf9uux2tyrJXPfQDA3wE4BGCw2eOO\n+dreAmACQG/l5+uaPe6Yj/dpAP+28u+PADjV7HGHON5/CeCjAP5B8fe7APwNyh35hgC8HtdY2nFG\nb9J4/FMAvlb594sAPkFE8o7UrY/n8TLGDjDGpis/HgJwQ4PHGBWmTeV/D8B/AnC5kYOLAZPj/RyA\n/8oYuwgAjLH3GjzGKDE5Xgbgg5V/LwJwtoHjixTG2N8BuKD5yKcA/AUrcwhAhoiWxjGWdjT0ssbj\n7tY81c8wxmYBXAJwbUNGFz0mxyvyWZRnCe2I57ES0QCAZYyxbzRyYDFhcm1/AcAvENFBIjpERJ9s\n2Oiix+R4dwLYTETvotzP4ncaM7Sm4PfZDkxsjUdixLPxuOFn2gXjYyGizQAGAfyrWEcUH9pjJaIE\ngKcA/GajBhQzJte2C2X3zcdRXqn9v0T0i4yxfMxjiwOT430YwJ8zxp6s9J7+vyvHOyf5brvTMDvV\njjP6dwEsE36+AfXLu+pniKgL5SWgbgnVypgcL4joXwP4IoB7GWNXGjS2qPE61g8A+EUAf0tEp1D2\na+5t44Cs6b38dcZYkTH2NoCTKBv+dsTkeD8L4HkAYIx9B8BClHVhOhGjZzsK2tHQVxuPE1E3ysHW\nva7P7AXwmcq/HwDwGqtEP9oQz+OtuDP+O8pGvp19uNpjZYxdYox9iDG2nDG2HOV4xL2MsXZtOGxy\nL4+jHGwHEX0IZVfOWw0dZXSYHO9pAJ8AACL6Fygb+vMNHWXj2AvgNyrZN0MALjHGzsWxo7Zz3TDG\nZonofwOwH/ONx48T0ZcBHGaM7QXwVZSXfD9GeSb/6eaNOByGxzsK4BoAL1RizqcZY/c2bdABMTzW\njsHwePcDuJOIfgCgBGCEMfbT5o06OIbH+wUAf0JEW1F2Y/xmu07SiOg5lF1uH6rEHHYAcACAMfbH\nKMcg7gLwYwDTAH4rtrG06Tm0WCwWiyHt6LqxWCwWiw+sobdYLJYOxxp6i8Vi6XCsobdYLJYOxxp6\ni8Vi6XCsobdYLJYOxxp6i8Vi6XCsobdYLJYO5/8HDtU5PTsbv+4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe23f8f8518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "import pylab\n",
    "\n",
    "matplotlib.pyplot.scatter(u_train_target_abs,train_target_len)\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target labels pre sort [0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      " Target labels post sort [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      " Certainty sorted \n",
      " First 20 [  8.78274441e-05   6.38395548e-04   6.69091940e-04   7.13229179e-04\n",
      "   7.18593597e-04   9.52094793e-04   1.34050846e-03   1.76942348e-03\n",
      "   1.81540847e-03   2.14239955e-03   2.14433670e-03   3.23754549e-03\n",
      "   3.28880548e-03   3.52361798e-03   3.61365080e-03   3.80355120e-03\n",
      "   4.05833125e-03   4.53045964e-03   5.11655211e-03   5.16593456e-03] \n",
      " Last 20 [ 0.99876553  0.99879622  0.99879766  0.9988389   0.99885935  0.99888998\n",
      "  0.99896646  0.99910325  0.99918407  0.99921024  0.99922711  0.99925858\n",
      "  0.99942958  0.99944371  0.99945509  0.99950451  0.99954647  0.99958009\n",
      "  0.99960864  0.99980825]\n",
      "\n",
      "Training on least certain first\n",
      "Training on target sample of size: 2500 with average certainty 0.310\n",
      "(2500, 150) (2500,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      "# batches = 19\n",
      "Train epoch 0, average loss 1.00138, average accuracy 0.627467,\n",
      "\t\tDev epoch 0, average loss 0.351296, average accuracy 0.870584,\n",
      "\t\tDev epoch 0, auc 0.803681, new accuracy 0.870584, right accuracy 0.870584,\n",
      "\t\t\t\t    Time taken for 0 epochs =  26.852464199066162\n",
      "Train epoch 2, average loss 0.426208, average accuracy 0.824836,\n",
      "\t\tDev epoch 2, average loss 0.334561, average accuracy 0.881793,\n",
      "\t\tDev epoch 2, auc 0.842256, new accuracy 0.881793, right accuracy 0.881793,\n",
      "Train epoch 4, average loss 0.324353, average accuracy 0.886102,\n",
      "\t\tDev epoch 4, average loss 0.305715, average accuracy 0.877038,\n",
      "\t\tDev epoch 4, auc 0.845716, new accuracy 0.877038, right accuracy 0.877038,\n",
      "\t\t\t\t    Time taken for 4 epochs =  121.65613269805908\n",
      "Train epoch 6, average loss 0.222742, average accuracy 0.958059,\n",
      "\t\tDev epoch 6, average loss 0.316931, average accuracy 0.877038,\n",
      "\t\tDev epoch 6, auc 0.853262, new accuracy 0.877038, right accuracy 0.877038,\n",
      "Train epoch 8, average loss 0.170239, average accuracy 0.97903,\n",
      "\t\tDev epoch 8, average loss 0.313619, average accuracy 0.87534,\n",
      "\t\tDev epoch 8, auc 0.857021, new accuracy 0.87534, right accuracy 0.87534,\n",
      "\t\t\t\t    Time taken for 8 epochs =  216.3336079120636\n",
      "Train epoch 10, average loss 0.133288, average accuracy 0.98972,\n",
      "\t\tDev epoch 10, average loss 0.291206, average accuracy 0.882473,\n",
      "\t\tDev epoch 10, auc 0.860115, new accuracy 0.882473, right accuracy 0.882473,\n",
      "Training on target sample of size: 5000 with average certainty 0.507\n",
      "(5000, 150) (5000,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      "# batches = 39\n",
      "Train epoch 0, average loss 0.616826, average accuracy 0.744191,\n",
      "\t\tDev epoch 0, average loss 0.313134, average accuracy 0.879416,\n",
      "\t\tDev epoch 0, auc 0.847758, new accuracy 0.879416, right accuracy 0.879416,\n",
      "\t\t\t\t    Time taken for 0 epochs =  48.532644748687744\n",
      "Train epoch 2, average loss 0.340328, average accuracy 0.861178,\n",
      "\t\tDev epoch 2, average loss 0.292869, average accuracy 0.879416,\n",
      "\t\tDev epoch 2, auc 0.874232, new accuracy 0.879416, right accuracy 0.879416,\n",
      "Train epoch 4, average loss 0.227231, average accuracy 0.923077,\n",
      "\t\tDev epoch 4, average loss 0.27894, average accuracy 0.887568,\n",
      "\t\tDev epoch 4, auc 0.884384, new accuracy 0.887568, right accuracy 0.887568,\n",
      "\t\t\t\t    Time taken for 4 epochs =  225.89411854743958\n",
      "Train epoch 6, average loss 0.166463, average accuracy 0.956731,\n",
      "\t\tDev epoch 6, average loss 0.276741, average accuracy 0.88587,\n",
      "\t\tDev epoch 6, auc 0.889137, new accuracy 0.88587, right accuracy 0.88587,\n",
      "Train epoch 8, average loss 0.107085, average accuracy 0.98758,\n",
      "\t\tDev epoch 8, average loss 0.26033, average accuracy 0.893342,\n",
      "\t\tDev epoch 8, auc 0.891926, new accuracy 0.893342, right accuracy 0.893342,\n",
      "\t\t\t\t    Time taken for 8 epochs =  402.6373052597046\n",
      "Train epoch 10, average loss 0.0803411, average accuracy 0.99399,\n",
      "\t\tDev epoch 10, average loss 0.261697, average accuracy 0.893342,\n",
      "\t\tDev epoch 10, auc 0.89281, new accuracy 0.893342, right accuracy 0.893342,\n",
      "Training on target sample of size: 10000 with average certainty 0.715\n",
      "(10000, 150) (10000,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.346775, average accuracy 0.859776,\n",
      "\t\tDev epoch 0, average loss 0.29585, average accuracy 0.876698,\n",
      "\t\tDev epoch 0, auc 0.868351, new accuracy 0.876698, right accuracy 0.876698,\n",
      "\t\t\t\t    Time taken for 0 epochs =  88.5027244091034\n",
      "Train epoch 2, average loss 0.200388, average accuracy 0.923478,\n",
      "\t\tDev epoch 2, average loss 0.265379, average accuracy 0.893003,\n",
      "\t\tDev epoch 2, auc 0.889173, new accuracy 0.893003, right accuracy 0.893003,\n",
      "Train epoch 4, average loss 0.133814, average accuracy 0.956731,\n",
      "\t\tDev epoch 4, average loss 0.261764, average accuracy 0.89606,\n",
      "\t\tDev epoch 4, auc 0.892836, new accuracy 0.89606, right accuracy 0.89606,\n",
      "\t\t\t\t    Time taken for 4 epochs =  425.8291997909546\n",
      "Train epoch 6, average loss 0.0882789, average accuracy 0.979868,\n",
      "\t\tDev epoch 6, average loss 0.283985, average accuracy 0.889266,\n",
      "\t\tDev epoch 6, auc 0.896253, new accuracy 0.889266, right accuracy 0.889266,\n",
      "Train epoch 8, average loss 0.0621907, average accuracy 0.989483,\n",
      "\t\tDev epoch 8, average loss 0.259406, average accuracy 0.897079,\n",
      "\t\tDev epoch 8, auc 0.897134, new accuracy 0.897079, right accuracy 0.897079,\n",
      "\t\t\t\t    Time taken for 8 epochs =  762.8266808986664\n",
      "Train epoch 10, average loss 0.0447703, average accuracy 0.994391,\n",
      "\t\tDev epoch 10, average loss 0.275602, average accuracy 0.897418,\n",
      "\t\tDev epoch 10, auc 0.898323, new accuracy 0.897418, right accuracy 0.897418,\n"
     ]
    }
   ],
   "source": [
    "#Active transfer learning : Continue training with adding selected samples from target domain\n",
    "#In this cell, samples where we have the least absolute difference in predicted probability of positive and negative class are added first.\n",
    "\n",
    "\n",
    "size_model = size_initial\n",
    "src_key = 'vid'\n",
    "tgt_key = 'aut'\n",
    "\n",
    "#Create a sorted version of the certainty, and correspondingly sorted target train set ids, and labels.\n",
    "sort_ids = np.argsort(u_train_target_abs)\n",
    "certainty_sorted = u_train_target_abs[sort_ids]\n",
    "#print(sort_ids)\n",
    "df_target_ids_pre = dict_transfer_train_ids[tgt_key][src_key]\n",
    "df_target_labels_pre = dict_train_y[tgt_key]\n",
    "print('Target labels pre sort',df_target_labels_pre[-20:])\n",
    "print(type(df_target_labels_pre))\n",
    "#df_target_ids_pre = df_target_ids_pre.iloc([sort_ids])\n",
    "df_target_ids = df_target_ids_pre[sort_ids]\n",
    "df_target_labels = df_target_labels_pre[sort_ids]\n",
    "print('\\n Target labels post sort',df_target_labels[-20:])\n",
    "print('\\n Certainty sorted','\\n First 20',certainty_sorted[:20],'\\n Last 20',certainty_sorted[-20:])\n",
    "\n",
    "\n",
    "print('\\nTraining on least certain first')\n",
    "size_list = size_list\n",
    "for size in size_list:\n",
    "    avg_certainty = np.average(certainty_sorted[:size])\n",
    "    print('Training on target sample of size:',size,'with average certainty %0.3f'%avg_certainty)\n",
    "    tgt_train_df = df_target_ids[:size]\n",
    "    tgt_train_y = df_target_labels[:size]\n",
    "    avg_certainty = np.average(certainty_sorted[:size])\n",
    "    print(tgt_train_df.shape,tgt_train_y.shape)\n",
    "    continue_transfer_train(src_key,size_model,tgt_key,tgt_train_df,tgt_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target labels pre sort [0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      " Target labels post sort [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1]\n",
      "\n",
      " Certainty sorted \n",
      " First 20 [  8.78274441e-05   6.69091940e-04   7.18593597e-04   7.13229179e-04\n",
      "   6.38395548e-04   1.81540847e-03   9.52094793e-04   2.14433670e-03\n",
      "   3.80355120e-03   4.05833125e-03   3.23754549e-03   4.53045964e-03\n",
      "   1.34050846e-03   3.28880548e-03   5.40760159e-03   3.61365080e-03\n",
      "   1.76942348e-03   8.53490829e-03   5.43239713e-03   1.02912784e-02] \n",
      " Last 20 [ 0.89449245  0.90011358  0.90871054  0.90871054  0.90871054  0.92040753\n",
      "  0.92671835  0.9346655   0.93972015  0.96450335  0.96594661  0.96748078\n",
      "  0.56709552  0.57681489  0.7729829   0.77322745  0.3428182   0.3428182\n",
      "  0.3428182   0.3428182 ]\n",
      "\n",
      "Training on least certain first\n",
      "Training on target sample of size: 2500 with average certainty per word id 0.648237200728 with average certainty 0.417\n",
      "(2500, 150) (2500,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      "# batches = 19\n",
      "Train epoch 0, average loss 1.09323, average accuracy 0.652138,\n",
      "\t\tDev epoch 0, average loss 0.362097, average accuracy 0.86481,\n",
      "\t\tDev epoch 0, auc 0.771536, new accuracy 0.86481, right accuracy 0.86481,\n",
      "\t\t\t\t    Time taken for 0 epochs =  26.24660873413086\n",
      "Train epoch 2, average loss 0.410268, average accuracy 0.818257,\n",
      "\t\tDev epoch 2, average loss 0.343735, average accuracy 0.876019,\n",
      "\t\tDev epoch 2, auc 0.824304, new accuracy 0.876019, right accuracy 0.876019,\n",
      "Train epoch 4, average loss 0.3092, average accuracy 0.896382,\n",
      "\t\tDev epoch 4, average loss 0.335846, average accuracy 0.875679,\n",
      "\t\tDev epoch 4, auc 0.81979, new accuracy 0.875679, right accuracy 0.875679,\n",
      "\t\t\t\t    Time taken for 4 epochs =  120.20981884002686\n",
      "Train epoch 6, average loss 0.20412, average accuracy 0.960526,\n",
      "\t\tDev epoch 6, average loss 0.319696, average accuracy 0.880774,\n",
      "\t\tDev epoch 6, auc 0.833477, new accuracy 0.880774, right accuracy 0.880774,\n",
      "Train epoch 8, average loss 0.159562, average accuracy 0.981497,\n",
      "\t\tDev epoch 8, average loss 0.315903, average accuracy 0.874321,\n",
      "\t\tDev epoch 8, auc 0.828301, new accuracy 0.874321, right accuracy 0.874321,\n",
      "\t\t\t\t    Time taken for 8 epochs =  214.3352508544922\n",
      "Train epoch 10, average loss 0.118212, average accuracy 0.994655,\n",
      "\t\tDev epoch 10, average loss 0.312922, average accuracy 0.878057,\n",
      "\t\tDev epoch 10, auc 0.835956, new accuracy 0.878057, right accuracy 0.878057,\n",
      "Training on target sample of size: 5000 with average certainty per word id 1.32991609807 with average certainty 0.562\n",
      "(5000, 150) (5000,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_10000/checkpoints/vidaut_model\n",
      "# batches = 39\n",
      "Train epoch 0, average loss 0.519117, average accuracy 0.771234,\n",
      "\t\tDev epoch 0, average loss 0.319942, average accuracy 0.879755,\n",
      "\t\tDev epoch 0, auc 0.837941, new accuracy 0.879755, right accuracy 0.879755,\n",
      "\t\t\t\t    Time taken for 0 epochs =  48.60777950286865\n",
      "Train epoch 2, average loss 0.296149, average accuracy 0.877204,\n",
      "\t\tDev epoch 2, average loss 0.284196, average accuracy 0.887908,\n",
      "\t\tDev epoch 2, auc 0.869374, new accuracy 0.887908, right accuracy 0.887908,\n",
      "Train epoch 4, average loss 0.193139, average accuracy 0.940705,\n",
      "\t\tDev epoch 4, average loss 0.277115, average accuracy 0.888247,\n",
      "\t\tDev epoch 4, auc 0.874525, new accuracy 0.888247, right accuracy 0.888247,\n",
      "\t\t\t\t    Time taken for 4 epochs =  224.75921988487244\n",
      "Train epoch 6, average loss 0.135241, average accuracy 0.967949,\n",
      "\t\tDev epoch 6, average loss 0.272246, average accuracy 0.892323,\n",
      "\t\tDev epoch 6, auc 0.878414, new accuracy 0.892323, right accuracy 0.892323,\n",
      "Train epoch 8, average loss 0.0956601, average accuracy 0.988582,\n",
      "\t\tDev epoch 8, average loss 0.335771, average accuracy 0.879416,\n",
      "\t\tDev epoch 8, auc 0.876319, new accuracy 0.879416, right accuracy 0.879416,\n",
      "\t\t\t\t    Time taken for 8 epochs =  400.5755879878998\n",
      "Train epoch 10, average loss 0.0651496, average accuracy 0.996194,\n",
      "\t\tDev epoch 10, average loss 0.280507, average accuracy 0.890625,\n",
      "\t\tDev epoch 10, auc 0.87817, new accuracy 0.890625, right accuracy 0.890625,\n"
     ]
    }
   ],
   "source": [
    "#In this cell, we add samples with the lowest certainty per word id first.\n",
    "size_model = size_initial\n",
    "src_key = 'vid'\n",
    "tgt_key = 'aut'\n",
    "\n",
    "#Create a sorted version of the certainty, and correspondingly sorted target train set ids, and labels.\n",
    "sort_ids = np.argsort(c_div_len_target)\n",
    "c_div_len_sorted = c_div_len_target[sort_ids]\n",
    "certainty_sorted = u_train_target_abs[sort_ids]\n",
    "#print(sort_ids)\n",
    "df_target_ids_pre = dict_transfer_train_ids[tgt_key][src_key]\n",
    "df_target_labels_pre = dict_train_y[tgt_key]\n",
    "print('Target labels pre sort',df_target_labels_pre[-20:])\n",
    "print(type(df_target_labels_pre))\n",
    "#df_target_ids_pre = df_target_ids_pre.iloc([sort_ids])\n",
    "df_target_ids = df_target_ids_pre[sort_ids]\n",
    "df_target_labels = df_target_labels_pre[sort_ids]\n",
    "print('\\n Target labels post sort',df_target_labels[-20:])\n",
    "print('\\n Certainty sorted','\\n First 20',certainty_sorted[:20],'\\n Last 20',certainty_sorted[-20:])\n",
    "\n",
    "\n",
    "print('\\nTraining on least certain first')\n",
    "size_list = [2500,5000]\n",
    "for size in size_list:\n",
    "    avg_certainty = np.average(certainty_sorted[:size])\n",
    "    avg_c_div_len = np.average(c_div_len_sorted[:size])\n",
    "    print('Training on target sample of size:',size,'with average certainty per word id',avg_c_div_len,'with average certainty %0.3f'%avg_certainty)\n",
    "    tgt_train_df = df_target_ids[:size]\n",
    "    tgt_train_y = df_target_labels[:size]\n",
    "    avg_certainty = np.average(certainty_sorted[:size])\n",
    "    print(tgt_train_df.shape,tgt_train_y.shape)\n",
    "    continue_transfer_train(src_key,size_model,tgt_key,tgt_train_df,tgt_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on most certain first\n",
      "Training on target sample of size: 5000 with average certainty 0.998\n",
      "(5000, 150) (5000,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_40000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_40000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_40000/checkpoints/vidaut_model\n",
      "# batches = 39\n",
      "Train epoch 0, average loss 0.0311065, average accuracy 0.995793,\n",
      "\t\tDev epoch 0, average loss 0.292827, average accuracy 0.8771,\n",
      "\t\tDev epoch 0, auc 0.885195, new accuracy 0.8771, right accuracy 0.8771,\n",
      "\t\t\t\t    Time taken for 0 epochs =  72.58204889297485\n",
      "Train epoch 2, average loss 0.00856757, average accuracy 0.997196,\n",
      "\t\tDev epoch 2, average loss 0.277037, average accuracy 0.885753,\n",
      "\t\tDev epoch 2, auc 0.883378, new accuracy 0.885753, right accuracy 0.885753,\n",
      "Train epoch 4, average loss 0.0031396, average accuracy 0.999599,\n",
      "\t\tDev epoch 4, average loss 0.281147, average accuracy 0.884073,\n",
      "\t\tDev epoch 4, auc 0.884967, new accuracy 0.884073, right accuracy 0.884073,\n",
      "\t\t\t\t    Time taken for 4 epochs =  296.8984706401825\n",
      "Train epoch 6, average loss 0.0022796, average accuracy 0.999599,\n",
      "\t\tDev epoch 6, average loss 0.29636, average accuracy 0.891381,\n",
      "\t\tDev epoch 6, auc 0.885226, new accuracy 0.891381, right accuracy 0.891381,\n",
      "Train epoch 8, average loss 0.00197001, average accuracy 0.999599,\n",
      "\t\tDev epoch 8, average loss 0.278567, average accuracy 0.890205,\n",
      "\t\tDev epoch 8, auc 0.886847, new accuracy 0.890205, right accuracy 0.890205,\n",
      "\t\t\t\t    Time taken for 8 epochs =  521.1753370761871\n",
      "Train epoch 10, average loss 0.00118972, average accuracy 1,\n",
      "\t\tDev epoch 10, average loss 0.282585, average accuracy 0.891801,\n",
      "\t\tDev epoch 10, auc 0.887149, new accuracy 0.891801, right accuracy 0.891801,\n",
      "Training on target sample of size: 10000 with average certainty 0.994\n",
      "(10000, 150) (10000,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_40000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_40000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_40000/checkpoints/vidaut_model\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.0533156, average accuracy 0.991286,\n",
      "\t\tDev epoch 0, average loss 0.275337, average accuracy 0.888357,\n",
      "\t\tDev epoch 0, auc 0.884534, new accuracy 0.888357, right accuracy 0.888357,\n",
      "\t\t\t\t    Time taken for 0 epochs =  112.30772113800049\n",
      "Train epoch 2, average loss 0.0154097, average accuracy 0.994992,\n",
      "\t\tDev epoch 2, average loss 0.361128, average accuracy 0.837534,\n",
      "\t\tDev epoch 2, auc 0.888342, new accuracy 0.837534, right accuracy 0.837534,\n",
      "Train epoch 4, average loss 0.00826394, average accuracy 0.997796,\n",
      "\t\tDev epoch 4, average loss 0.275314, average accuracy 0.887013,\n",
      "\t\tDev epoch 4, auc 0.890223, new accuracy 0.887013, right accuracy 0.887013,\n",
      "\t\t\t\t    Time taken for 4 epochs =  495.4780731201172\n",
      "Train epoch 6, average loss 0.00481273, average accuracy 0.999199,\n",
      "\t\tDev epoch 6, average loss 0.273092, average accuracy 0.892473,\n",
      "\t\tDev epoch 6, auc 0.891597, new accuracy 0.892473, right accuracy 0.892473,\n",
      "Train epoch 8, average loss 0.00340826, average accuracy 0.9997,\n",
      "\t\tDev epoch 8, average loss 0.27578, average accuracy 0.893901,\n",
      "\t\tDev epoch 8, auc 0.892777, new accuracy 0.893901, right accuracy 0.893901,\n",
      "\t\t\t\t    Time taken for 8 epochs =  878.8334648609161\n",
      "Train epoch 10, average loss 0.00273814, average accuracy 0.9999,\n",
      "\t\tDev epoch 10, average loss 0.278236, average accuracy 0.888189,\n",
      "\t\tDev epoch 10, auc 0.893472, new accuracy 0.888189, right accuracy 0.888189,\n",
      "Training on target sample of size: 20000 with average certainty 0.974\n",
      "(20000, 150) (20000,)\n",
      "/home/arunima/fproject/final_project/runs/vid/aut/size_40000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/aut/size_40000/checkpoints/vidaut_model\n",
      " Model loaded from: /home/arunima/fproject/final_project/runs/vid/aut/size_40000/checkpoints/vidaut_model\n",
      "# batches = 156\n",
      "Train epoch 0, average loss 0.122257, average accuracy 0.972055,\n",
      "\t\tDev epoch 0, average loss 0.258149, average accuracy 0.894405,\n",
      "\t\tDev epoch 0, auc 0.897095, new accuracy 0.894405, right accuracy 0.894405,\n",
      "\t\t\t\t    Time taken for 0 epochs =  191.77233839035034\n",
      "Train epoch 2, average loss 0.0466922, average accuracy 0.984125,\n",
      "\t\tDev epoch 2, average loss 0.252826, average accuracy 0.896757,\n",
      "\t\tDev epoch 2, auc 0.904139, new accuracy 0.896757, right accuracy 0.896757,\n",
      "Train epoch 4, average loss 0.0232002, average accuracy 0.994241,\n",
      "\t\tDev epoch 4, average loss 0.268115, average accuracy 0.900118,\n",
      "\t\tDev epoch 4, auc 0.906356, new accuracy 0.900118, right accuracy 0.900118,\n",
      "\t\t\t\t    Time taken for 4 epochs =  892.1500623226166\n",
      "Train epoch 6, average loss 0.0148054, average accuracy 0.997496,\n",
      "\t\tDev epoch 6, average loss 0.257715, average accuracy 0.897765,\n",
      "\t\tDev epoch 6, auc 0.907013, new accuracy 0.897765, right accuracy 0.897765,\n",
      "Train epoch 8, average loss 0.00980449, average accuracy 0.998948,\n",
      "\t\tDev epoch 8, average loss 0.274512, average accuracy 0.89953,\n",
      "\t\tDev epoch 8, auc 0.906063, new accuracy 0.89953, right accuracy 0.89953,\n",
      "\t\t\t\t    Time taken for 8 epochs =  1592.8588778972626\n",
      "Train epoch 10, average loss 0.00776135, average accuracy 0.999349,\n",
      "\t\tDev epoch 10, average loss 0.288098, average accuracy 0.899362,\n",
      "\t\tDev epoch 10, auc 0.907787, new accuracy 0.899362, right accuracy 0.899362,\n"
     ]
    }
   ],
   "source": [
    "#In this cell, samples where we have the most absolute difference in predicted probability of positive and negative class are added first.\n",
    "size_model = size_initial\n",
    "print('Training on most certain first')\n",
    "size_list = size_list\n",
    "for size in size_list:\n",
    "    avg_certainty = np.average(certainty_sorted[-size:])\n",
    "    print('Training on target sample of size:',size,'with average certainty %0.3f'%avg_certainty)\n",
    "    tgt_train_df = df_target_ids[-size:]\n",
    "    tgt_train_y = df_target_labels[-size:]\n",
    "    print(tgt_train_df.shape,tgt_train_y.shape)\n",
    "    continue_transfer_train(src_key,size_model,tgt_key,tgt_train_df,tgt_train_y)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code that can likely be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.410732, average accuracy 0.84986,\n",
      "\t\tDev epoch 0, average loss 0.384446, average accuracy 0.857337,\n",
      "\t\tDev epoch 0, auc 0.761469, new accuracy 0.857337, right accuracy 0.857337,\n",
      "\t\t\t\t    Time taken for 0 epochs =  85.97571444511414\n",
      "Train epoch 3, average loss 0.278746, average accuracy 0.882512,\n",
      "Train epoch 6, average loss 0.190353, average accuracy 0.92508,\n",
      "\t\tDev epoch 6, average loss 0.267068, average accuracy 0.892323,\n",
      "\t\tDev epoch 6, auc 0.884168, new accuracy 0.892323, right accuracy 0.892323,\n",
      "Train epoch 9, average loss 0.120366, average accuracy 0.963241,\n",
      "Train epoch 12, average loss 0.0750643, average accuracy 0.983874,\n",
      "\t\tDev epoch 12, average loss 0.250142, average accuracy 0.90591,\n",
      "\t\tDev epoch 12, auc 0.898789, new accuracy 0.90591, right accuracy 0.90591,\n",
      "\t\t\t\t    Time taken for 12 epochs =  1057.8321163654327\n",
      "Train epoch 15, average loss 0.0461413, average accuracy 0.995292,\n",
      "Saved model toys /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n"
     ]
    }
   ],
   "source": [
    "#Testing only on toys\n",
    "list_df = ['toys',10000]\n",
    "for key in list_df:\n",
    "    print(key)\n",
    "    train_cnn(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target vid\n",
      "source vid\n",
      "/home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/arunima/fproject/final_project/runs/vid/size_10000/checkpoints/vid_model\n",
      "0.857333333333\n",
      "vid vid AUC 84.74%\n",
      "vid vid accuracy 85.73%\n",
      "\n",
      "src_vid_tar_vid_3000 Saved file successfully\n"
     ]
    }
   ],
   "source": [
    "#Ak - this seems like a repeat of the previous cell. Not sure it is needed.\n",
    "size = 10000\n",
    "for s_key in list_df:\n",
    "    for t_key in list_df:\n",
    "        predict_accuracy(s_key, size, t_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Old version of get_target-data\n",
    "def get_target_data(key,size = 5000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "    #key = list_df[0]\n",
    "    #print('Toys reviews\\n')\n",
    "    #dict_vectorizers[key] = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=min_frequency)\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "    dict_train_ids[key], dict_train_y[key],dict_dev_ids[key], dict_dev_ypred[key], dict_vocab_len[key] = process_inputs(key,vocab_processor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-224ec9642a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_target_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-0ac178ef7c6f>\u001b[0m in \u001b[0;36mget_target_data\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#dict_vectorizers[key] = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=min_frequency)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdict_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_dev_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_train_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_dev_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_df_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_toys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_toys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdict_train_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_train_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_dev_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_dev_ypred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_vocab_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_processor' is not defined"
     ]
    }
   ],
   "source": [
    "get_target_data('vid',2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial version of continue train.\n",
    "#size is needed to get the right source domain file to load\n",
    "def continue_train(src_key, size, tar_key):\n",
    "    \n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key))\n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"testruns\", src_key))\n",
    "    \n",
    "    size_folder =  \"size_\" + str(size) \n",
    "    out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key, size_folder))\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "\n",
    "    \n",
    "    print(checkpoint_dir)\n",
    "    src_model = src_key\n",
    "    #graph_meta_file = checkpoint_dir + '/' + 'hnk01_model.meta'\n",
    "#     graph_meta_file = checkpoint_dir + '/' + src_model +'01_model.meta'\n",
    "    graph=tf.Graph()\n",
    "    \n",
    "    x_train = dict_train_ids[tar_key]\n",
    "    y_train = dict_train_y[tar_key]\n",
    "    x_dev = dict_dev_ids[tar_key]\n",
    "    y_dev = dict_dev_ypred[tar_key]\n",
    "    V = dict_vocab_len[tar_key]\n",
    "    \n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:           \n",
    "            cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    " \n",
    "            saver = tf.train.Saver()\n",
    "    \n",
    "          #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "#             new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "#             new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "            \n",
    "            \n",
    "            #initializing weights from a previous session \n",
    "            initialising_model = src_model+'_model'\n",
    "            print(\" RESTORING SESSION FOR WEIGHTS INITIALIZATION\")\n",
    "            # Exclude output layer weights from variables we will restore\n",
    "            variables_to_restore = [v for v in tf.global_variables()]\n",
    "            # Replace variables scope with that of the current model\n",
    "            loader = tf.train.Saver({v.op.name.replace(src_model, initialising_model): v for v in variables_to_restore})\n",
    "            load_path = checkpoint_dir + '/' + initialising_model \n",
    "            #load_path = checkpoint_dir  \n",
    "            loader.restore(sess, load_path)\n",
    "            print(\" Model loaded from: \" + load_path) \n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "           \n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=True), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                   # _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    #avg_auc = total_auc/(batch_size*(len(x_train)//batch_size))\n",
    "                   # print(\"Train epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                    \n",
    "                    \n",
    "                    right_acc = total_batch_acc/(num_batches)\n",
    "                    #avg_auc = total_auc/(num_batches*batch_size)\n",
    "                    \n",
    "                   #Calculate Accuracy\n",
    "                    new_acc = accuracy_score(y_shuffled, y_pred, normalize=True ) \n",
    "                     \n",
    "                    \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "                    print(\"\\t\\tDev epoch {}, auc {:g}, new accuracy {:g}, right accuracy {:g},\".format(e,  roc_auc, new_acc, right_acc))\n",
    "                    #print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/reachanamikasinha/project/runs/toys/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/size_10000/checkpoints/toys_model\n",
      " Model loaded from: /home/reachanamikasinha/project/runs/toys/size_10000/checkpoints/toys_model\n",
      "# batches = 15\n",
      "Train epoch 0, average loss 0.0276176, average accuracy 0.996875,\n",
      "\t\tDev epoch 0, average loss 0.232405, average accuracy 0.910156,\n",
      "\t\tDev epoch 0, auc 0.919749, new accuracy 0.910156, right accuracy 0.910156,\n",
      "\t\t\t\t    Time taken for 0 epochs =  15.348036289215088\n",
      "Train epoch 3, average loss 0.0183418, average accuracy 1,\n",
      "Train epoch 6, average loss 0.0133835, average accuracy 1,\n",
      "\t\tDev epoch 6, average loss 0.238629, average accuracy 0.908203,\n",
      "\t\tDev epoch 6, auc 0.9181, new accuracy 0.908203, right accuracy 0.908203,\n",
      "Train epoch 9, average loss 0.0105111, average accuracy 1,\n",
      "Train epoch 12, average loss 0.00948409, average accuracy 1,\n",
      "\t\tDev epoch 12, average loss 0.229321, average accuracy 0.914062,\n",
      "\t\tDev epoch 12, auc 0.915269, new accuracy 0.914062, right accuracy 0.914062,\n",
      "\t\t\t\t    Time taken for 12 epochs =  200.676424741745\n",
      "Train epoch 15, average loss 0.00868266, average accuracy 1,\n",
      "Train epoch 18, average loss 0.00785425, average accuracy 1,\n",
      "\t\tDev epoch 18, average loss 0.249066, average accuracy 0.908203,\n",
      "\t\tDev epoch 18, auc 0.915878, new accuracy 0.908203, right accuracy 0.908203,\n",
      "Train epoch 21, average loss 0.006944, average accuracy 1,\n"
     ]
    }
   ],
   "source": [
    "# remeber to cal get_target_data('vid',2000) with the target key and size you want to add to source domain\n",
    "#then call continue_train with size of src domain file u want to load\n",
    "continue_train(\"toys\", 10000, \"vid\")\n",
    "\n",
    "#for comparison before train toys vid AUC 81.24%\n",
    "#toys vid accuracy 81.46%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train results \n",
    "\n",
    "toys(10000)\n",
    "completed cnn creation\n",
    "# batches = 78\n",
    "Train epoch 0, average loss 0.410732, average accuracy 0.84986,\n",
    "\t\tDev epoch 0, average loss 0.384446, average accuracy 0.857337,\n",
    "\t\tDev epoch 0, auc 0.761469, new accuracy 0.857337, right accuracy 0.857337,\n",
    "\t\t\t\t    Time taken for 0 epochs =  85.97571444511414\n",
    "Train epoch 3, average loss 0.278746, average accuracy 0.882512,\n",
    "Train epoch 6, average loss 0.190353, average accuracy 0.92508,\n",
    "\t\tDev epoch 6, average loss 0.267068, average accuracy 0.892323,\n",
    "\t\tDev epoch 6, auc 0.884168, new accuracy 0.892323, right accuracy 0.892323,\n",
    "Train epoch 9, average loss 0.120366, average accuracy 0.963241,\n",
    "Train epoch 12, average loss 0.0750643, average accuracy 0.983874,\n",
    "\t\tDev epoch 12, average loss 0.250142, average accuracy 0.90591,\n",
    "\t\tDev epoch 12, auc 0.898789, new accuracy 0.90591, right accuracy 0.90591,\n",
    "\t\t\t\t    Time taken for 12 epochs =  1057.8321163654327\n",
    "Train epoch 15, average loss 0.0461413, average accuracy 0.995292,\n",
    "Saved model toys /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n",
    "\n",
    "Continue train \n",
    "toys(2000)\n",
    "# batches = 15\n",
    "Train epoch 0, average loss 0.0328055, average accuracy 0.998958,\n",
    "\t\tDev epoch 0, average loss 0.253235, average accuracy 0.900391,\n",
    "\t\tDev epoch 0, auc 0.902401, new accuracy 0.900391, right accuracy 0.900391,\n",
    "\t\t\t\t    Time taken for 0 epochs =  15.97208309173584\n",
    "Train epoch 3, average loss 0.0209501, average accuracy 1,\n",
    "Train epoch 6, average loss 0.0157637, average accuracy 1,\n",
    "\t\tDev epoch 6, average loss 0.246794, average accuracy 0.910156,\n",
    "\t\tDev epoch 6, auc 0.904086, new accuracy 0.910156, right accuracy 0.910156,\n",
    "Train epoch 9, average loss 0.0133948, average accuracy 0.999479,\n",
    "Train epoch 12, average loss 0.0110164, average accuracy 1,\n",
    "\t\tDev epoch 12, average loss 0.268596, average accuracy 0.900391,\n",
    "\t\tDev epoch 12, auc 0.903477, new accuracy 0.900391, right accuracy 0.900391,\n",
    "\t\t\t\t    Time taken for 12 epochs =  203.32078433036804\n",
    "Train epoch 15, average loss 0.00984509, average accuracy 1,\n",
    "\n",
    "\n",
    "Continue train \n",
    "toys(4000)\n",
    "/home/reachanamikasinha/project/testruns/toys/checkpoints\n",
    " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
    "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n",
    " Model loaded from: /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n",
    "# batches = 31\n",
    "Train epoch 0, average loss 0.0316445, average accuracy 0.997732,\n",
    "\t\tDev epoch 0, average loss 0.238046, average accuracy 0.911458,\n",
    "\t\tDev epoch 0, auc 0.915253, new accuracy 0.911458, right accuracy 0.911458,\n",
    "\t\t\t\t    Time taken for 0 epochs =  33.795907497406006\n",
    "Train epoch 3, average loss 0.021633, average accuracy 0.999748,\n",
    "Train epoch 6, average loss 0.0158767, average accuracy 0.999496,\n",
    "\t\tDev epoch 6, average loss 0.272812, average accuracy 0.903646,\n",
    "\t\tDev epoch 6, auc 0.915281, new accuracy 0.903646, right accuracy 0.903646,\n",
    "Train epoch 9, average loss 0.0130432, average accuracy 0.999748,\n",
    "Train epoch 12, average loss 0.0109889, average accuracy 1,\n",
    "\t\tDev epoch 12, average loss 0.275246, average accuracy 0.908854,\n",
    "\t\tDev epoch 12, auc 0.91456, new accuracy 0.908854, right accuracy 0.908854,\n",
    "\t\t\t\t    Time taken for 12 epochs =  418.0527718067169\n",
    "Train epoch 15, average loss 0.0100661, average accuracy 0.999748,\n",
    "Train epoch 18, average loss 0.00892393, average accuracy 1,\n",
    "\t\tDev epoch 18, average loss 0.276631, average accuracy 0.907986,\n",
    "\t\tDev epoch 18, auc 0.915397, new accuracy 0.907986, right accuracy 0.907986,\n",
    "Train epoch 21, average loss 0.00748347, average accuracy 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of transfer learning testing\n",
    "\n",
    "Comparison\n",
    "target vid\n",
    "/home/reachanamikasinha/project/runs/toys/checkpoints\n",
    "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/checkpoints/toys01_model\n",
    "0.814630681818\n",
    "toys vid AUC 81.24%\n",
    "toys vid accuracy 81.46%\n",
    "\n",
    "Transfer accuracy source toys, target vid(source trained on 10,000 and \n",
    "auc 0.893548, new accuracy 0.898438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving output of model saved run to be able to compare dev accuracy\n",
    "\n",
    "# toys\n",
    "\n",
    "completed cnn creation\n",
    "#batches = 1562\n",
    "Train epoch 0, average loss 0.281697, average accuracy 0.886964,\n",
    "\t\tDev epoch 0, average loss 0.244123, average accuracy 0.908854,\n",
    "\t\t\t\t    Time taken for 0 epochs =  37.60847544670105\n",
    "Train epoch 3, average loss 0.142607, average accuracy 0.944032,\n",
    "Train epoch 6, average loss 0.0771984, average accuracy 0.971111,\n",
    "\t\tDev epoch 6, average loss 0.234148, average accuracy 0.925581,\n",
    "Train epoch 9, average loss 0.0413356, average accuracy 0.985665,\n",
    "Train epoch 12, average loss 0.0262069, average accuracy 0.991207,\n",
    "\t\tDev epoch 12, average loss 0.259911, average accuracy 0.931858,\n",
    "\t\t\t\t    Time taken for 12 epochs =  458.23175573349\n",
    "Train epoch 15, average loss 0.0191853, average accuracy 0.994008,\n",
    "Train epoch 18, average loss 0.0141925, average accuracy 0.995569,\n",
    "\t\tDev epoch 18, average loss 0.297451, average accuracy 0.932759,\n",
    "Train epoch 21, average loss 0.0111091, average accuracy 0.996669,\n",
    "Train epoch 24, average loss 0.00874389, average accuracy 0.997289,\n",
    "\t\tDev epoch 24, average loss 0.37409, average accuracy 0.930889,\n",
    "\t\t\t\t    Time taken for 24 epochs =  879.0003838539124\n",
    "Train epoch 27, average loss 0.00864632, average accuracy 0.997309,\n",
    "Train epoch 30, average loss 0.00784798, average accuracy 0.997459,\n",
    "\t\tDev epoch 30, average loss 0.358449, average accuracy 0.932192,\n",
    "Train epoch 33, average loss 0.00659365, average accuracy 0.997819,\n",
    "Train epoch 36, average loss 0.00578367, average accuracy 0.998349,\n",
    "\t\tDev epoch 36, average loss 0.373084, average accuracy 0.931958,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1299.6440062522888\n",
    "Train epoch 39, average loss 0.0064537, average accuracy 0.998089,\n",
    "Train epoch 42, average loss 0.00580202, average accuracy 0.998259,\n",
    "\t\tDev epoch 42, average loss 0.391404, average accuracy 0.933393,\n",
    "Train epoch 45, average loss 0.00514404, average accuracy 0.99845,\n",
    "Train epoch 48, average loss 0.00371194, average accuracy 0.99892,\n",
    "\t\tDev epoch 48, average loss 0.469131, average accuracy 0.931457,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1720.3404235839844\n",
    "Train epoch 51, average loss 0.0041625, average accuracy 0.99878,\n",
    "Train epoch 54, average loss 0.00460097, average accuracy 0.99856,\n",
    "\t\tDev epoch 54, average loss 0.412365, average accuracy 0.934195,\n",
    "Train epoch 57, average loss 0.00364978, average accuracy 0.99895,\n",
    "Saved model toys /home/ubuntu/project/runs/cnn/checkpoints/toys_model\n",
    "\n",
    "# vid\n",
    "completed cnn creation\n",
    "#batches = 1562\n",
    "Train epoch 0, average loss 0.364666, average accuracy 0.84355,\n",
    "\t\tDev epoch 0, average loss 0.319497, average accuracy 0.86071,\n",
    "\t\t\t\t    Time taken for 0 epochs =  37.799813985824585\n",
    "Train epoch 3, average loss 0.205366, average accuracy 0.917043,\n",
    "Train epoch 6, average loss 0.121604, average accuracy 0.952835,\n",
    "\t\tDev epoch 6, average loss 0.270975, average accuracy 0.899272,\n",
    "Train epoch 9, average loss 0.07385, average accuracy 0.972371,\n",
    "Train epoch 12, average loss 0.0478919, average accuracy 0.982955,\n",
    "\t\tDev epoch 12, average loss 0.35926, average accuracy 0.8959,\n",
    "\t\t\t\t    Time taken for 12 epochs =  459.7377371788025\n",
    "Train epoch 15, average loss 0.0352903, average accuracy 0.987636,\n",
    "Train epoch 18, average loss 0.0294105, average accuracy 0.989997,\n",
    "\t\tDev epoch 18, average loss 0.453399, average accuracy 0.899439,\n",
    "Train epoch 21, average loss 0.0230395, average accuracy 0.992167,\n",
    "Train epoch 24, average loss 0.021058, average accuracy 0.992928,\n",
    "\t\tDev epoch 24, average loss 0.537794, average accuracy 0.899773,\n",
    "\t\t\t\t    Time taken for 24 epochs =  881.6988339424133\n",
    "Train epoch 27, average loss 0.0175359, average accuracy 0.994248,\n",
    "Train epoch 30, average loss 0.0139743, average accuracy 0.995549,\n",
    "\t\tDev epoch 30, average loss 0.571489, average accuracy 0.899673,\n",
    "Train epoch 33, average loss 0.0134842, average accuracy 0.995739,\n",
    "Train epoch 36, average loss 0.0106022, average accuracy 0.996829,\n",
    "\t\tDev epoch 36, average loss 0.609015, average accuracy 0.900407,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1303.694475889206\n",
    "Train epoch 39, average loss 0.0102687, average accuracy 0.996589,\n",
    "Train epoch 42, average loss 0.00967124, average accuracy 0.997099,\n",
    "\t\tDev epoch 42, average loss 0.639956, average accuracy 0.900174,\n",
    "Train epoch 45, average loss 0.00792713, average accuracy 0.997689,\n",
    "Train epoch 48, average loss 0.00804649, average accuracy 0.997559,\n",
    "\t\tDev epoch 48, average loss 0.696189, average accuracy 0.899272,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1725.6383044719696\n",
    "Train epoch 51, average loss 0.00929602, average accuracy 0.997259,\n",
    "Train epoch 54, average loss 0.0067116, average accuracy 0.998039,\n",
    "\t\tDev epoch 54, average loss 0.606182, average accuracy 0.900007,\n",
    "Train epoch 57, average loss 0.00869572, average accuracy 0.997289,\n",
    "Saved model vid /home/ubuntu/project/runs/cnn/checkpoints/vid_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For 100,000 rows\n",
    "## Output of predict on source domain toys\n",
    "Target toys\n",
    "\n",
    "\n",
    "## Output of predict on source domainvid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEEPING TRACK OF RESULTS FROM DIFFERENT RUNS\n",
    "#### Number samples = 10000, Number batches = 156, Without pre-trained embeddings, no dropout\n",
    "Train epoch 0, loss 0.357085, average loss 0.440941, acc 0.84375, average acc 0.845152,\n",
    "\tDev epoch 0, loss 0.494501, average loss 0.394619, acc 0.78125, average acc 0.854959,\n",
    "\t\tTime taken for 0 epochs =  36.18872332572937\n",
    "Train epoch 2, loss 0.25934, average loss 0.335786, acc 0.875, average acc 0.862079,\n",
    "Train epoch 4, loss 0.214457, average loss 0.263726, acc 0.875, average acc 0.891827,\n",
    "Train epoch 6, loss 0.161232, average loss 0.194851, acc 0.890625, average acc 0.92528,\n",
    "Train epoch 8, loss 0.0968393, average loss 0.132971, acc 0.984375, average acc 0.958133,\n",
    "Train epoch 10, loss 0.0598382, average loss 0.0935878, acc 1, average acc 0.978766,\n",
    "\tDev epoch 10, loss 0.318434, average loss 0.279314, acc 0.875, average acc 0.891304,\n",
    "\t\tTime taken for 10 epochs =  366.54717350006104\n",
    "Train epoch 12, loss 0.0432213, average loss 0.089715, acc 1, average acc 0.979768,\n",
    "Train epoch 14, loss 0.0724975, average loss 0.299487, acc 1, average acc 0.957933,\n",
    "Train epoch 16, loss 0.0388074, average loss 0.0520482, acc 1, average acc 0.991987,\n",
    "Train epoch 18, loss 0.0239645, average loss 0.0351604, acc 1, average acc 0.997196,\n",
    "Train epoch 20, loss 0.0157139, average loss 0.0272624, acc 1, average acc 0.996595,\n",
    "\tDev epoch 20, loss 0.304895, average loss 0.283551, acc 0.921875, average acc 0.902853,\n",
    "\t\tTime taken for 20 epochs =  697.680163860321\n",
    "Train epoch 22, loss 0.0131277, average loss 0.017179, acc 1, average acc 0.999299,\n",
    "Train epoch 24, loss 0.0104588, average loss 0.0121853, acc 1, average acc 0.9999,\n",
    "Train epoch 26, loss 0.0072446, average loss 0.00969622, acc 1, average acc 0.9999,\n",
    "Train epoch 28, loss 0.00628954, average loss 0.00805781, acc 1, average acc 0.9999,\n",
    "Train epoch 30, loss 0.00585206, average loss 0.00689346, acc 1, average acc 0.9999,\n",
    "\tDev epoch 30, loss 0.37002, average loss 0.327092, acc 0.875, average acc 0.902514,\n",
    "\t\tTime taken for 30 epochs =  1029.1201057434082\n",
    "Train epoch 32, loss 0.00559655, average loss 0.00594074, acc 1, average acc 0.9999,\n",
    "Train epoch 34, loss 0.0053231, average loss 0.00518374, acc 1, average acc 1,\n",
    "Train epoch 36, loss 0.00504235, average loss 0.00461033, acc 1, average acc 1,\n",
    "Train epoch 38, loss 0.00477842, average loss 0.00416377, acc 1, average acc 1,\n",
    "Train epoch 40, loss 0.00451324, average loss 0.00380536, acc 1, average acc 1,\n",
    "\tDev epoch 40, loss 0.451459, average loss 0.382435, acc 0.859375, average acc 0.899796,\n",
    "\t\tTime taken for 40 epochs =  1360.2103555202484\n",
    "        \n",
    "        \n",
    "#### Number samples = 10000, Number batches = 156, With pre-trained embeddings(Trainable = False), dropout = 0.8\n",
    "Train epoch 0, average loss 0.819034, average acc 0.802784,\n",
    "\tDev epoch 0, average loss 0.415172, average acc 0.853601,\n",
    "\t\tTime taken for 0 epochs =  35.559093713760376\n",
    "Train epoch 2, average loss 0.403757, average acc 0.841046,\n",
    "Train epoch 4, average loss 0.340479, average acc 0.860777,\n",
    "\tDev epoch 5, average loss 0.329067, average acc 0.867188,\n",
    "Train epoch 6, average loss 0.289147, average acc 0.882312,\n",
    "Train epoch 8, average loss 0.237817, average acc 0.904948,\n",
    "Train epoch 10, average loss 0.194272, average acc 0.923978,\n",
    "\tDev epoch 10, average loss 0.330927, average acc 0.876698,\n",
    "\t\tTime taken for 10 epochs =  363.92726016044617\n",
    "Train epoch 12, average loss 0.149883, average acc 0.940405,\n",
    "Train epoch 14, average loss 0.128152, average acc 0.951322,\n",
    "\tDev epoch 15, average loss 0.349508, average acc 0.877717,\n",
    "Train epoch 16, average loss 0.101319, average acc 0.961639,\n",
    "Train epoch 18, average loss 0.079585, average acc 0.970052,\n",
    "Train epoch 20, average loss 0.0705579, average acc 0.97516,\n",
    "\tDev epoch 20, average loss 0.364253, average acc 0.878057,\n",
    "\t\tTime taken for 20 epochs =  692.3398864269257\n",
    "Train epoch 22, average loss 0.0631964, average acc 0.978466,\n",
    "Train epoch 24, average loss 0.0484077, average acc 0.984876,\n",
    "\tDev epoch 25, average loss 0.435054, average acc 0.877717,\n",
    "Train epoch 26, average loss 0.0433892, average acc 0.985377,\n",
    "Train epoch 28, average loss 0.0368327, average acc 0.988381,\n",
    "Train epoch 30, average loss 0.0308169, average acc 0.990385,\n",
    "\tDev epoch 30, average loss 0.570798, average acc 0.875679,\n",
    "\t\tTime taken for 30 epochs =  1052.273297548294\n",
    "Train epoch 32, average loss 0.0291807, average acc 0.991086,\n",
    "Train epoch 34, average loss 0.0271599, average acc 0.991987,\n",
    "\tDev epoch 35, average loss 0.661539, average acc 0.87534,\n",
    "Train epoch 36, average loss 0.029594, average acc 0.991486,\n",
    "Train epoch 38, average loss 0.0236557, average acc 0.99359,\n",
    "Train epoch 40, average loss 0.018746, average acc 0.995292,\n",
    "\tDev epoch 40, average loss 0.506544, average acc 0.878397,\n",
    "\t\tTime taken for 40 epochs =  1466.0729427337646\n",
    "        \n",
    "        \n",
    "#### Changes. Changed convolutional layer weights to xavier initialization. Added random see = 42 to train-test split. Dropped learning rate initial to 0.007\n",
    "\n",
    "Train epoch 0, average loss 0.399899, average accuracy 0.858273,\n",
    "\tDev epoch 0, average loss 0.374428, average accuracy 0.857337,\n",
    "\t\tTime taken for 0 epochs =  34.83389401435852\n",
    "Train epoch 2, average loss 0.326706, average accuracy 0.869391,\n",
    "Train epoch 4, average loss 0.269826, average accuracy 0.891126,\n",
    "\tDev epoch 5, average loss 0.303238, average accuracy 0.88519,\n",
    "Train epoch 6, average loss 0.219368, average accuracy 0.911659,\n",
    "Train epoch 8, average loss 0.171234, average accuracy 0.935296,\n",
    "Train epoch 10, average loss 0.13296, average accuracy 0.953325,\n",
    "\tDev epoch 10, average loss 0.293018, average accuracy 0.887568,\n",
    "\t\tTime taken for 10 epochs =  370.47870922088623\n",
    "Train epoch 12, average loss 0.100562, average accuracy 0.967348,\n",
    "Train epoch 14, average loss 0.0793127, average accuracy 0.977063,\n",
    "\tDev epoch 15, average loss 0.320119, average accuracy 0.886209,\n",
    "Train epoch 16, average loss 0.0582729, average accuracy 0.988482,\n",
    "Train epoch 18, average loss 0.0456755, average accuracy 0.990385,\n",
    "Train epoch 20, average loss 0.0405185, average accuracy 0.992788,\n",
    "\tDev epoch 20, average loss 0.321604, average accuracy 0.886889,\n",
    "\t\tTime taken for 20 epochs =  705.0958936214447\n",
    "Train epoch 22, average loss 0.0351258, average accuracy 0.993089,\n",
    "Train epoch 24, average loss 0.0270392, average accuracy 0.996194,\n",
    "\tDev epoch 25, average loss 0.399808, average accuracy 0.884171,\n",
    "Train epoch 26, average loss 0.0262923, average accuracy 0.995994,\n",
    "Train epoch 28, average loss 0.0242657, average accuracy 0.995994,\n",
    "Train epoch 30, average loss 0.0208821, average accuracy 0.996394,\n",
    "\tDev epoch 30, average loss 0.413923, average accuracy 0.886889,\n",
    "\t\tTime taken for 30 epochs =  1039.4113600254059\n",
    "Train epoch 32, average loss 0.017492, average accuracy 0.997696,\n",
    "Train epoch 34, average loss 0.0146527, average accuracy 0.998097,\n",
    "\tDev epoch 35, average loss 0.386267, average accuracy 0.884851,\n",
    "Train epoch 36, average loss 0.0168233, average accuracy 0.997396,\n",
    "Train epoch 38, average loss 0.0142984, average accuracy 0.997796,\n",
    "Train epoch 40, average loss 0.0110543, average accuracy 0.998998,\n",
    "\tDev epoch 40, average loss 0.478341, average accuracy 0.884171,\n",
    "\t\tTime taken for 40 epochs =  1374.086744070053\n",
    "Train epoch 42, average loss 0.012298, average accuracy 0.998397,\n",
    "Train epoch 44, average loss 0.0116889, average accuracy 0.998197,\n",
    "\tDev epoch 45, average loss 0.448394, average accuracy 0.88587,\n",
    "Train epoch 46, average loss 0.0107089, average accuracy 0.998197,\n",
    "Train epoch 48, average loss 0.00953887, average accuracy 0.998898,\n",
    "Train epoch 50, average loss 0.0097256, average accuracy 0.998898,\n",
    "\tDev epoch 50, average loss 0.424627, average accuracy 0.886209,\n",
    "\t\tTime taken for 50 epochs =  1708.131004333496\n",
    "Train epoch 52, average loss 0.00792942, average accuracy 0.999099,\n",
    "Train epoch 54, average loss 0.00777054, average accuracy 0.999099,\n",
    "\tDev epoch 55, average loss 0.434766, average accuracy 0.887228,\n",
    "Train epoch 56, average loss 0.00812112, average accuracy 0.999099,\n",
    "Train epoch 58, average loss 0.00817043, average accuracy 0.998798,\n",
    "Train epoch 60, average loss 0.00776972, average accuracy 0.998498,\n",
    "\tDev epoch 60, average loss 0.447535, average accuracy 0.886889,\n",
    "\t\tTime taken for 60 epochs =  2042.1303217411041\n",
    "Train epoch 62, average loss 0.00759579, average accuracy 0.998998,\n",
    "Train epoch 64, average loss 0.00697335, average accuracy 0.998798,\n",
    "\tDev epoch 65, average loss 0.514295, average accuracy 0.88519,\n",
    "Train epoch 66, average loss 0.00579109, average accuracy 0.999199,\n",
    "Train epoch 68, average loss 0.00583337, average accuracy 0.999499,\n",
    "\n",
    "#### Changes. set trainable = True in glove embeddings. Changed learning rate back to 0.01 initial.\n",
    "# batches = 156\n",
    "Train epoch 0, average loss 0.399899, average accuracy 0.858273,\n",
    "\tDev epoch 0, average loss 0.374428, average accuracy 0.857337,\n",
    "\t\tTime taken for 0 epochs =  34.83389401435852\n",
    "Train epoch 2, average loss 0.326706, average accuracy 0.869391,\n",
    "Train epoch 4, average loss 0.269826, average accuracy 0.891126,\n",
    "\tDev epoch 5, average loss 0.303238, average accuracy 0.88519,\n",
    "Train epoch 6, average loss 0.219368, average accuracy 0.911659,\n",
    "Train epoch 8, average loss 0.171234, average accuracy 0.935296,\n",
    "Train epoch 10, average loss 0.13296, average accuracy 0.953325,\n",
    "\tDev epoch 10, average loss 0.293018, average accuracy 0.887568,\n",
    "\t\tTime taken for 10 epochs =  370.47870922088623\n",
    "Train epoch 12, average loss 0.100562, average accuracy 0.967348,\n",
    "Train epoch 14, average loss 0.0793127, average accuracy 0.977063,\n",
    "\tDev epoch 15, average loss 0.320119, average accuracy 0.886209,\n",
    "Train epoch 16, average loss 0.0582729, average accuracy 0.988482,\n",
    "Train epoch 18, average loss 0.0456755, average accuracy 0.990385,\n",
    "Train epoch 20, average loss 0.0405185, average accuracy 0.992788,\n",
    "\tDev epoch 20, average loss 0.321604, average accuracy 0.886889,\n",
    "\t\tTime taken for 20 epochs =  705.0958936214447\n",
    "Train epoch 22, average loss 0.0351258, average accuracy 0.993089,\n",
    "Train epoch 24, average loss 0.0270392, average accuracy 0.996194,\n",
    "\tDev epoch 25, average loss 0.399808, average accuracy 0.884171,\n",
    "Train epoch 26, average loss 0.0262923, average accuracy 0.995994,\n",
    "Train epoch 28, average loss 0.0242657, average accuracy 0.995994,\n",
    "Train epoch 30, average loss 0.0208821, average accuracy 0.996394,\n",
    "\tDev epoch 30, average loss 0.413923, average accuracy 0.886889,\n",
    "\t\tTime taken for 30 epochs =  1039.4113600254059\n",
    "Train epoch 32, average loss 0.017492, average accuracy 0.997696,\n",
    "Train epoch 34, average loss 0.0146527, average accuracy 0.998097,\n",
    "\tDev epoch 35, average loss 0.386267, average accuracy 0.884851,\n",
    "Train epoch 36, average loss 0.0168233, average accuracy 0.997396,\n",
    "Train epoch 38, average loss 0.0142984, average accuracy 0.997796,\n",
    "Train epoch 40, average loss 0.0110543, average accuracy 0.998998,\n",
    "\tDev epoch 40, average loss 0.478341, average accuracy 0.884171,\n",
    "\t\tTime taken for 40 epochs =  1374.086744070053\n",
    "Train epoch 42, average loss 0.012298, average accuracy 0.998397,\n",
    "Train epoch 44, average loss 0.0116889, average accuracy 0.998197,\n",
    "\tDev epoch 45, average loss 0.448394, average accuracy 0.88587,\n",
    "Train epoch 46, average loss 0.0107089, average accuracy 0.998197,\n",
    "Train epoch 48, average loss 0.00953887, average accuracy 0.998898,\n",
    "Train epoch 50, average loss 0.0097256, average accuracy 0.998898,\n",
    "\tDev epoch 50, average loss 0.424627, average accuracy 0.886209,\n",
    "\t\tTime taken for 50 epochs =  1708.131004333496\n",
    "Train epoch 52, average loss 0.00792942, average accuracy 0.999099,\n",
    "Train epoch 54, average loss 0.00777054, average accuracy 0.999099,\n",
    "\tDev epoch 55, average loss 0.434766, average accuracy 0.887228,\n",
    "Train epoch 56, average loss 0.00812112, average accuracy 0.999099,\n",
    "Train epoch 58, average loss 0.00817043, average accuracy 0.998798,\n",
    "Train epoch 60, average loss 0.00776972, average accuracy 0.998498,\n",
    "\tDev epoch 60, average loss 0.447535, average accuracy 0.886889,\n",
    "\t\tTime taken for 60 epochs =  2042.1303217411041\n",
    "Train epoch 62, average loss 0.00759579, average accuracy 0.998998,\n",
    "Train epoch 64, average loss 0.00697335, average accuracy 0.998798,\n",
    "\tDev epoch 65, average loss 0.514295, average accuracy 0.88519,\n",
    "Train epoch 66, average loss 0.00579109, average accuracy 0.999199,\n",
    "Train epoch 68, average loss 0.00583337, average accuracy 0.999499,\n",
    "\n",
    "\n",
    "#### Changes : increased sample size to 20000. increased filter number to 256 per filter size. Both together slowed it down 4 times. Ran 150 epochs.\n",
    "\n",
    "Result - get to accuracy of about 90.5% on dev set. First saw it in about 80 epochs.\n",
    "\n",
    "number of batches = 312\n",
    "Train epoch 0, average loss 0.399409, average accuracy 0.851763,\n",
    "\tDev epoch 0, average loss 0.390935, average accuracy 0.849798,\n",
    "\t\tTime taken for 0 epochs =  126.17049622535706\n",
    "Train epoch 2, average loss 0.310082, average accuracy 0.877955,\n",
    "Train epoch 4, average loss 0.240321, average accuracy 0.905298,\n",
    "\tDev epoch 5, average loss 0.309063, average accuracy 0.879872,\n",
    "Train epoch 6, average loss 0.183422, average accuracy 0.929137,\n",
    "Train epoch 8, average loss 0.132305, average accuracy 0.950871,\n",
    "Train epoch 10, average loss 0.091186, average accuracy 0.96855,\n",
    "\tDev epoch 10, average loss 0.359275, average accuracy 0.887769,\n",
    "\t\tTime taken for 10 epochs =  1301.3605210781097\n",
    "Train epoch 12, average loss 0.0655771, average accuracy 0.978966,\n",
    "Train epoch 14, average loss 0.0487824, average accuracy 0.986579,\n",
    "\tDev epoch 15, average loss 0.31907, average accuracy 0.901546,\n",
    "Train epoch 16, average loss 0.0353836, average accuracy 0.991136,\n",
    "Train epoch 18, average loss 0.0272109, average accuracy 0.993389,\n",
    "Train epoch 20, average loss 0.0209386, average accuracy 0.995843,\n",
    "\tDev epoch 20, average loss 0.473887, average accuracy 0.888609,\n",
    "\t\tTime taken for 20 epochs =  2475.8890883922577\n",
    "Train epoch 22, average loss 0.0169513, average accuracy 0.996444,\n",
    "Train epoch 24, average loss 0.0144857, average accuracy 0.997045,\n",
    "\tDev epoch 25, average loss 0.52256, average accuracy 0.886929,\n",
    "Train epoch 26, average loss 0.0115876, average accuracy 0.998147,\n",
    "Train epoch 28, average loss 0.00961356, average accuracy 0.998347,\n",
    "Train epoch 30, average loss 0.00915859, average accuracy 0.998297,\n",
    "\tDev epoch 30, average loss 0.458756, average accuracy 0.895665,\n",
    "\t\tTime taken for 30 epochs =  3649.31303191185\n",
    "Train epoch 32, average loss 0.00898325, average accuracy 0.998498,\n",
    "Train epoch 34, average loss 0.00841926, average accuracy 0.998448,\n",
    "\tDev epoch 35, average loss 0.457308, average accuracy 0.897681,\n",
    "Train epoch 36, average loss 0.00646681, average accuracy 0.999149,\n",
    "Train epoch 38, average loss 0.00662382, average accuracy 0.998598,\n",
    "Train epoch 40, average loss 0.00595506, average accuracy 0.999149,\n",
    "\tDev epoch 40, average loss 0.378182, average accuracy 0.901546,\n",
    "\t\tTime taken for 40 epochs =  4823.336989402771\n",
    "Train epoch 42, average loss 0.00593351, average accuracy 0.999149,\n",
    "Train epoch 44, average loss 0.00464219, average accuracy 0.999249,\n",
    "\tDev epoch 45, average loss 0.431081, average accuracy 0.90121,\n",
    "Train epoch 46, average loss 0.00444085, average accuracy 0.999499,\n",
    "Train epoch 48, average loss 0.00461485, average accuracy 0.999349,\n",
    "Train epoch 50, average loss 0.00466378, average accuracy 0.999199,\n",
    "\tDev epoch 50, average loss 0.380632, average accuracy 0.90289,\n",
    "\t\tTime taken for 50 epochs =  5997.558866024017\n",
    "Train epoch 52, average loss 0.00401276, average accuracy 0.999299,\n",
    "Train epoch 54, average loss 0.00360064, average accuracy 0.999549,\n",
    "\tDev epoch 55, average loss 0.472261, average accuracy 0.900706,\n",
    "Train epoch 56, average loss 0.00390259, average accuracy 0.999449,\n",
    "Train epoch 58, average loss 0.00343323, average accuracy 0.999499,\n",
    "Train epoch 60, average loss 0.00328182, average accuracy 0.999549,\n",
    "\tDev epoch 60, average loss 0.405813, average accuracy 0.901714,\n",
    "\t\tTime taken for 60 epochs =  7171.330280542374\n",
    "Train epoch 62, average loss 0.00357674, average accuracy 0.999499,\n",
    "Train epoch 64, average loss 0.00316356, average accuracy 0.999449,\n",
    "\tDev epoch 65, average loss 0.484432, average accuracy 0.899698,\n",
    "Train epoch 66, average loss 0.00242786, average accuracy 0.99975,\n",
    "Train epoch 68, average loss 0.0029979, average accuracy 0.999399,\n",
    "Train epoch 70, average loss 0.00219736, average accuracy 0.999599,\n",
    "\tDev epoch 70, average loss 0.522188, average accuracy 0.89953,\n",
    "\t\tTime taken for 70 epochs =  8345.40755033493\n",
    "Train epoch 72, average loss 0.00263028, average accuracy 0.999599,\n",
    "Train epoch 74, average loss 0.00262097, average accuracy 0.999599,\n",
    "\tDev epoch 75, average loss 0.501381, average accuracy 0.90037,\n",
    "Train epoch 76, average loss 0.00184087, average accuracy 0.99975,\n",
    "Train epoch 78, average loss 0.00261343, average accuracy 0.999399,\n",
    "Train epoch 80, average loss 0.00210662, average accuracy 0.9997,\n",
    "\tDev epoch 80, average loss 0.437249, average accuracy 0.90457,\n",
    "\t\tTime taken for 80 epochs =  9519.476462364197\n",
    "Train epoch 82, average loss 0.00218873, average accuracy 0.999599,\n",
    "Train epoch 84, average loss 0.00204953, average accuracy 0.9997,\n",
    "\tDev epoch 85, average loss 0.440843, average accuracy 0.90457,\n",
    "Train epoch 86, average loss 0.00178851, average accuracy 0.99975,\n",
    "Train epoch 88, average loss 0.00177724, average accuracy 0.999599,\n",
    "Train epoch 90, average loss 0.0018953, average accuracy 0.999599,\n",
    "\tDev epoch 90, average loss 0.465653, average accuracy 0.905074,\n",
    "\t\tTime taken for 90 epochs =  10693.68774318695\n",
    "Train epoch 92, average loss 0.00145395, average accuracy 0.9998,\n",
    "Train epoch 94, average loss 0.00158429, average accuracy 0.999649,\n",
    "\tDev epoch 95, average loss 0.472749, average accuracy 0.90541,\n",
    "Train epoch 96, average loss 0.00238694, average accuracy 0.999499,\n",
    "Train epoch 98, average loss 0.00175795, average accuracy 0.9997,\n",
    "Train epoch 100, average loss 0.00153946, average accuracy 0.9998,\n",
    "\tDev epoch 100, average loss 0.440989, average accuracy 0.906754,\n",
    "\t\tTime taken for 100 epochs =  11867.341850280762\n",
    "Train epoch 102, average loss 0.00144569, average accuracy 0.9998,\n",
    "Train epoch 104, average loss 0.00135836, average accuracy 0.99975,\n",
    "\tDev epoch 105, average loss 0.450465, average accuracy 0.90457,\n",
    "Train epoch 106, average loss 0.00134604, average accuracy 0.99985,\n",
    "Train epoch 108, average loss 0.00198454, average accuracy 0.999549,\n",
    "Train epoch 110, average loss 0.0016289, average accuracy 0.99975,\n",
    "\tDev epoch 110, average loss 0.43462, average accuracy 0.905242,\n",
    "\t\tTime taken for 110 epochs =  13040.53886771202\n",
    "Train epoch 112, average loss 0.00116808, average accuracy 0.99975,\n",
    "Train epoch 114, average loss 0.00163431, average accuracy 0.999649,\n",
    "\tDev epoch 115, average loss 0.544521, average accuracy 0.901714,\n",
    "Train epoch 116, average loss 0.00107182, average accuracy 0.9999,\n",
    "Train epoch 118, average loss 0.00118616, average accuracy 0.99985,\n",
    "Train epoch 120, average loss 0.00116875, average accuracy 0.9998,\n",
    "\tDev epoch 120, average loss 0.456846, average accuracy 0.906082,\n",
    "\t\tTime taken for 120 epochs =  14214.848599672318\n",
    "Train epoch 122, average loss 0.00144058, average accuracy 0.999649,\n",
    "Train epoch 124, average loss 0.00139588, average accuracy 0.999649,\n",
    "\tDev epoch 125, average loss 0.456731, average accuracy 0.90709,\n",
    "Train epoch 126, average loss 0.00129419, average accuracy 0.99975,\n",
    "Train epoch 128, average loss 0.000993939, average accuracy 0.9998,\n",
    "Train epoch 130, average loss 0.00110859, average accuracy 0.99975,\n",
    "\tDev epoch 130, average loss 0.440627, average accuracy 0.905242,\n",
    "\t\tTime taken for 130 epochs =  15387.949309825897\n",
    "Train epoch 132, average loss 0.000869354, average accuracy 0.9998,\n",
    "Train epoch 134, average loss 0.0010678, average accuracy 0.99975,\n",
    "\tDev epoch 135, average loss 0.662642, average accuracy 0.895497,\n",
    "Train epoch 136, average loss 0.00121623, average accuracy 0.99985,\n",
    "Train epoch 138, average loss 0.00106557, average accuracy 0.9998,\n",
    "Train epoch 140, average loss 0.0012005, average accuracy 0.9997,\n",
    "\tDev epoch 140, average loss 0.48323, average accuracy 0.905578,\n",
    "\t\tTime taken for 140 epochs =  16560.915422201157\n",
    "Train epoch 142, average loss 0.00142349, average accuracy 0.999649,\n",
    "Train epoch 144, average loss 0.00100832, average accuracy 0.9998,\n",
    "\tDev epoch 145, average loss 0.469864, average accuracy 0.906082,\n",
    "Train epoch 146, average loss 0.000867766, average accuracy 0.99985,\n",
    "Train epoch 148, average loss 0.000895252, average accuracy 0.9998,\n",
    "Train epoch 150, average loss 0.00128643, average accuracy 0.99975,\n",
    "\tDev epoch 150, average loss 0.504558, average accuracy 0.904738,\n",
    "\t\tTime taken for 150 epochs =  17732.689709424973\n",
    "Train epoch 152, average loss 0.00106975, average accuracy 0.99975,\n",
    "Train epoch 154, average loss 0.000923771, average accuracy 0.9998,\n",
    "\tDev epoch 155, average loss 0.46343, average accuracy 0.904066,\n",
    "    \n",
    "    \n",
    "#### Home and Kitchen, 100000 reviews. 200 epochs, min-documents = 0, embedding size = 100, embeddings trainable = True.\n",
    "\n",
    "completed cnn creation\n",
    "Number batches = 1562\n",
    "Train epoch 0, average loss 0.304591, average accuracy 0.87457,\n",
    "\tDev epoch 0, average loss 0.237596, average accuracy 0.904013,\n",
    "\t\tTime taken for 0 epochs =  36.92328929901123\n",
    "Train epoch 1, average loss 0.223893, average accuracy 0.910071,\n",
    "Train epoch 2, average loss 0.183924, average accuracy 0.927187,\n",
    "Train epoch 3, average loss 0.150811, average accuracy 0.941301,\n",
    "Train epoch 4, average loss 0.123611, average accuracy 0.952775,\n",
    "Train epoch 5, average loss 0.101681, average accuracy 0.961138,\n",
    "\tDev epoch 5, average loss 0.215108, average accuracy 0.917234,\n",
    "Train epoch 6, average loss 0.0824821, average accuracy 0.9694,\n",
    "Train epoch 7, average loss 0.0682877, average accuracy 0.975272,\n",
    "Train epoch 8, average loss 0.0581098, average accuracy 0.978353,\n",
    "Train epoch 9, average loss 0.0481609, average accuracy 0.982815,\n",
    "Train epoch 10, average loss 0.0426059, average accuracy 0.984625,\n",
    "\tDev epoch 10, average loss 0.258906, average accuracy 0.918336,\n",
    "\t\tTime taken for 10 epochs =  381.81759333610535\n",
    "Train epoch 11, average loss 0.0369901, average accuracy 0.986686,\n",
    "Train epoch 12, average loss 0.0348868, average accuracy 0.986946,\n",
    "Train epoch 13, average loss 0.0291087, average accuracy 0.989937,\n",
    "Train epoch 14, average loss 0.0271577, average accuracy 0.990807,\n",
    "Train epoch 15, average loss 0.0248202, average accuracy 0.991487,\n",
    "\tDev epoch 15, average loss 0.317684, average accuracy 0.925114,\n",
    "Train epoch 16, average loss 0.0232966, average accuracy 0.992037,\n",
    "Train epoch 17, average loss 0.0218823, average accuracy 0.992578,\n",
    "Train epoch 18, average loss 0.0189234, average accuracy 0.993878,\n",
    "Train epoch 19, average loss 0.0175714, average accuracy 0.993978,\n",
    "Train epoch 20, average loss 0.0175559, average accuracy 0.994108,\n",
    "\tDev epoch 20, average loss 0.322537, average accuracy 0.924746,\n",
    "\t\tTime taken for 20 epochs =  727.1127679347992\n",
    "Train epoch 21, average loss 0.0159045, average accuracy 0.994638,\n",
    "Train epoch 22, average loss 0.0141158, average accuracy 0.995409,\n",
    "Train epoch 23, average loss 0.0130852, average accuracy 0.995809,\n",
    "Train epoch 24, average loss 0.0131137, average accuracy 0.995709,\n",
    "Train epoch 25, average loss 0.0123618, average accuracy 0.995899,\n",
    "\tDev epoch 25, average loss 0.403685, average accuracy 0.924646,\n",
    "Train epoch 26, average loss 0.0112836, average accuracy 0.996369,\n",
    "Train epoch 27, average loss 0.0111362, average accuracy 0.996449,\n",
    "Train epoch 28, average loss 0.00865663, average accuracy 0.997299,\n",
    "Train epoch 29, average loss 0.00935199, average accuracy 0.996959,\n",
    "Train epoch 30, average loss 0.00935717, average accuracy 0.996699,\n",
    "\tDev epoch 30, average loss 0.367405, average accuracy 0.924579,\n",
    "\t\tTime taken for 30 epochs =  1072.1994183063507\n",
    "Train epoch 31, average loss 0.00909225, average accuracy 0.997149,\n",
    "Train epoch 32, average loss 0.00754316, average accuracy 0.997819,\n",
    "Train epoch 33, average loss 0.0079119, average accuracy 0.997379,\n",
    "Train epoch 34, average loss 0.00705012, average accuracy 0.997719,\n",
    "Train epoch 35, average loss 0.00730037, average accuracy 0.997829,\n",
    "\tDev epoch 35, average loss 0.370859, average accuracy 0.926749,\n",
    "Train epoch 36, average loss 0.00705196, average accuracy 0.997799,\n",
    "Train epoch 37, average loss 0.00669642, average accuracy 0.998039,\n",
    "Train epoch 38, average loss 0.00627199, average accuracy 0.998009,\n",
    "Train epoch 39, average loss 0.00528379, average accuracy 0.998309,\n",
    "Train epoch 40, average loss 0.00593793, average accuracy 0.998229,\n",
    "\tDev epoch 40, average loss 0.383177, average accuracy 0.927618,\n",
    "\t\tTime taken for 40 epochs =  1416.9269080162048\n",
    "Train epoch 41, average loss 0.00519099, average accuracy 0.998289,\n",
    "Train epoch 42, average loss 0.00574083, average accuracy 0.998259,\n",
    "Train epoch 43, average loss 0.00573397, average accuracy 0.998269,\n",
    "Train epoch 44, average loss 0.00478373, average accuracy 0.99844,\n",
    "Train epoch 45, average loss 0.00476654, average accuracy 0.99864,\n",
    "\tDev epoch 45, average loss 0.401872, average accuracy 0.927651,\n",
    "Train epoch 46, average loss 0.00546603, average accuracy 0.998079,\n",
    "Train epoch 47, average loss 0.00552262, average accuracy 0.998299,\n",
    "Train epoch 48, average loss 0.0044039, average accuracy 0.9986,\n",
    "Train epoch 49, average loss 0.00444202, average accuracy 0.99858,\n",
    "Train epoch 50, average loss 0.00553655, average accuracy 0.998239,\n",
    "\tDev epoch 50, average loss 0.410369, average accuracy 0.926182,\n",
    "\t\tTime taken for 50 epochs =  1761.8077738285065\n",
    "Train epoch 51, average loss 0.00446012, average accuracy 0.99862,\n",
    "Train epoch 52, average loss 0.00399219, average accuracy 0.9988,\n",
    "Train epoch 53, average loss 0.00413133, average accuracy 0.99876,\n",
    "Train epoch 54, average loss 0.00465115, average accuracy 0.99857,\n",
    "Train epoch 55, average loss 0.00390704, average accuracy 0.99883,\n",
    "\tDev epoch 55, average loss 0.523764, average accuracy 0.924212,\n",
    "Train epoch 56, average loss 0.00380121, average accuracy 0.99894,\n",
    "Train epoch 57, average loss 0.0041114, average accuracy 0.99863,\n",
    "Train epoch 58, average loss 0.00400388, average accuracy 0.99874,\n",
    "Train epoch 59, average loss 0.00436616, average accuracy 0.99868,\n",
    "Train epoch 60, average loss 0.00410136, average accuracy 0.99879,\n",
    "\tDev epoch 60, average loss 0.465152, average accuracy 0.926115,\n",
    "\t\tTime taken for 60 epochs =  2106.424416542053\n",
    "Train epoch 61, average loss 0.00331209, average accuracy 0.99906,\n",
    "Train epoch 62, average loss 0.00366131, average accuracy 0.99892,\n",
    "Train epoch 63, average loss 0.00425752, average accuracy 0.99884,\n",
    "Train epoch 64, average loss 0.00341706, average accuracy 0.99892,\n",
    "Train epoch 65, average loss 0.00352085, average accuracy 0.99884,\n",
    "\tDev epoch 65, average loss 0.443976, average accuracy 0.919404,\n",
    "Train epoch 66, average loss 0.00392861, average accuracy 0.99874,\n",
    "Train epoch 67, average loss 0.00390108, average accuracy 0.99879,\n",
    "Train epoch 68, average loss 0.00344137, average accuracy 0.99892,\n",
    "Train epoch 69, average loss 0.00282472, average accuracy 0.99918,\n",
    "Train epoch 70, average loss 0.00305406, average accuracy 0.99906,\n",
    "\tDev epoch 70, average loss 0.497239, average accuracy 0.926783,\n",
    "\t\tTime taken for 70 epochs =  2451.336544752121\n",
    "Train epoch 71, average loss 0.00335245, average accuracy 0.99887,\n",
    "Train epoch 72, average loss 0.00321827, average accuracy 0.99912,\n",
    "Train epoch 73, average loss 0.0030572, average accuracy 0.99906,\n",
    "Train epoch 74, average loss 0.00308252, average accuracy 0.99905,\n",
    "Train epoch 75, average loss 0.00312562, average accuracy 0.99909,\n",
    "\tDev epoch 75, average loss 0.454947, average accuracy 0.927818,\n",
    "Train epoch 76, average loss 0.00308124, average accuracy 0.99912,\n",
    "Train epoch 77, average loss 0.00282977, average accuracy 0.9991,\n",
    "Train epoch 78, average loss 0.00250926, average accuracy 0.99929,\n",
    "Train epoch 79, average loss 0.00278992, average accuracy 0.99917,\n",
    "Train epoch 80, average loss 0.00226384, average accuracy 0.99932,\n",
    "\tDev epoch 80, average loss 0.524143, average accuracy 0.925314,\n",
    "\t\tTime taken for 80 epochs =  2796.155880212784\n",
    "Train epoch 81, average loss 0.00289398, average accuracy 0.99907,\n",
    "Train epoch 82, average loss 0.00267882, average accuracy 0.99915,\n",
    "Train epoch 83, average loss 0.00251234, average accuracy 0.99917,\n",
    "Train epoch 84, average loss 0.00212931, average accuracy 0.99929,\n",
    "Train epoch 85, average loss 0.00204477, average accuracy 0.99936,\n",
    "\tDev epoch 85, average loss 0.533493, average accuracy 0.926382,\n",
    "Train epoch 86, average loss 0.00236821, average accuracy 0.99926,\n",
    "Train epoch 87, average loss 0.00237569, average accuracy 0.99921,\n",
    "Train epoch 88, average loss 0.00245643, average accuracy 0.99923,\n",
    "Train epoch 89, average loss 0.00226378, average accuracy 0.99937,\n",
    "Train epoch 90, average loss 0.00233121, average accuracy 0.99925,\n",
    "\tDev epoch 90, average loss 0.58426, average accuracy 0.924112,\n",
    "\t\tTime taken for 90 epochs =  3140.9629440307617\n",
    "Train epoch 91, average loss 0.00239393, average accuracy 0.99934,\n",
    "Train epoch 92, average loss 0.00168514, average accuracy 0.99947,\n",
    "Train epoch 93, average loss 0.00218618, average accuracy 0.99934,\n",
    "Train epoch 94, average loss 0.00201177, average accuracy 0.99939,\n",
    "Train epoch 95, average loss 0.00265143, average accuracy 0.99903,\n",
    "\n",
    "\tDev epoch 95, average loss 0.522781, average accuracy 0.927284,\n",
    "Train epoch 96, average loss 0.00200737, average accuracy 0.99945,\n",
    "Train epoch 97, average loss 0.00167531, average accuracy 0.99956,\n",
    "Train epoch 98, average loss 0.00197691, average accuracy 0.99943,\n",
    "Train epoch 99, average loss 0.00189404, average accuracy 0.99943,\n",
    "Train epoch 100, average loss 0.00217102, average accuracy 0.99935,\n",
    "\tDev epoch 100, average loss 0.509596, average accuracy 0.927417,\n",
    "\t\tTime taken for 100 epochs =  3485.5287368297577\n",
    "Train epoch 101, average loss 0.00214391, average accuracy 0.99932,\n",
    "Train epoch 102, average loss 0.00166687, average accuracy 0.99949,\n",
    "Train epoch 103, average loss 0.00193214, average accuracy 0.99941,\n",
    "Train epoch 104, average loss 0.00230417, average accuracy 0.99928,\n",
    "Train epoch 105, average loss 0.00262318, average accuracy 0.99917,\n",
    "\tDev epoch 105, average loss 0.580702, average accuracy 0.924379,\n",
    "Train epoch 106, average loss 0.00217951, average accuracy 0.99936,\n",
    "Train epoch 107, average loss 0.00196965, average accuracy 0.9994,\n",
    "Train epoch 108, average loss 0.00169252, average accuracy 0.99945,\n",
    "Train epoch 109, average loss 0.00184972, average accuracy 0.99937,\n",
    "Train epoch 110, average loss 0.00203432, average accuracy 0.99938,\n",
    "\tDev epoch 110, average loss 0.580674, average accuracy 0.925214,\n",
    "\t\tTime taken for 110 epochs =  3830.0551614761353\n",
    "Train epoch 111, average loss 0.0018848, average accuracy 0.99941,\n",
    "Train epoch 112, average loss 0.001956, average accuracy 0.99934,\n",
    "Train epoch 113, average loss 0.00164951, average accuracy 0.99948,\n",
    "Train epoch 114, average loss 0.00189435, average accuracy 0.99941,\n",
    "Train epoch 115, average loss 0.00179166, average accuracy 0.99939,\n",
    "\tDev epoch 115, average loss 0.541616, average accuracy 0.926282,\n",
    "Train epoch 116, average loss 0.0018286, average accuracy 0.99944,\n",
    "Train epoch 117, average loss 0.00176589, average accuracy 0.99943,\n",
    "Train epoch 118, average loss 0.00170349, average accuracy 0.99947,\n",
    "Train epoch 119, average loss 0.00222146, average accuracy 0.99928,\n",
    "Train epoch 120, average loss 0.00133639, average accuracy 0.99959,\n",
    "\tDev epoch 120, average loss 0.587092, average accuracy 0.926482,\n",
    "\t\tTime taken for 120 epochs =  4174.599865913391\n",
    "Train epoch 121, average loss 0.00179675, average accuracy 0.99939,\n",
    "Train epoch 122, average loss 0.00170493, average accuracy 0.99948,\n",
    "Train epoch 123, average loss 0.00196213, average accuracy 0.99939,\n",
    "Train epoch 124, average loss 0.00148926, average accuracy 0.99953,\n",
    "Train epoch 125, average loss 0.00146297, average accuracy 0.99951,\n",
    "\tDev epoch 125, average loss 0.555345, average accuracy 0.926516,\n",
    "Train epoch 126, average loss 0.00200696, average accuracy 0.99936,\n",
    "Train epoch 127, average loss 0.00152734, average accuracy 0.99952,\n",
    "Train epoch 128, average loss 0.00232658, average accuracy 0.9993,\n",
    "Train epoch 129, average loss 0.00197241, average accuracy 0.99939,\n",
    "Train epoch 130, average loss 0.00210397, average accuracy 0.99924,\n",
    "\tDev epoch 130, average loss 0.670168, average accuracy 0.92291,\n",
    "\t\tTime taken for 130 epochs =  4519.105709552765\n",
    "Train epoch 131, average loss 0.00184804, average accuracy 0.99943,\n",
    "Train epoch 132, average loss 0.00178672, average accuracy 0.99941,\n",
    "Train epoch 133, average loss 0.00175419, average accuracy 0.9994,\n",
    "Train epoch 134, average loss 0.0019224, average accuracy 0.99934,\n",
    "Train epoch 135, average loss 0.00222945, average accuracy 0.99931,\n",
    "\tDev epoch 135, average loss 0.657096, average accuracy 0.923377,\n",
    "Train epoch 136, average loss 0.00175209, average accuracy 0.99934,\n",
    "Train epoch 137, average loss 0.00214912, average accuracy 0.99925,\n",
    "Train epoch 138, average loss 0.00162636, average accuracy 0.99941,\n",
    "Train epoch 139, average loss 0.00180691, average accuracy 0.99946,\n",
    "Train epoch 140, average loss 0.00118666, average accuracy 0.99959,\n",
    "\tDev epoch 140, average loss 0.575778, average accuracy 0.926015,\n",
    "\t\tTime taken for 140 epochs =  4863.487067222595\n",
    "        \n",
    "\n",
    "#### Same as above, with 60 epochs.\n",
    "\n",
    "completed cnn creation\n",
    "Number batches = 1562\n",
    "Train epoch 0, average loss 0.30547, average accuracy 0.87393,\n",
    "\tDev epoch 0, average loss 0.237115, average accuracy 0.904915,\n",
    "\t\tTime taken for 0 epochs =  36.875508308410645\n",
    "Train epoch 3, average loss 0.152626, average accuracy 0.940681,\n",
    "Train epoch 6, average loss 0.0826888, average accuracy 0.96937,\n",
    "\tDev epoch 6, average loss 0.237618, average accuracy 0.921307,\n",
    "Train epoch 9, average loss 0.0498599, average accuracy 0.982274,\n",
    "Train epoch 12, average loss 0.0340667, average accuracy 0.987906,\n",
    "\tDev epoch 12, average loss 0.290549, average accuracy 0.922075,\n",
    "\t\tTime taken for 12 epochs =  449.51883339881897\n",
    "Train epoch 15, average loss 0.0254497, average accuracy 0.991217,\n",
    "Train epoch 18, average loss 0.0187158, average accuracy 0.993648,\n",
    "\tDev epoch 18, average loss 0.306756, average accuracy 0.923945,\n",
    "Train epoch 21, average loss 0.0152883, average accuracy 0.995038,\n",
    "Train epoch 24, average loss 0.0124834, average accuracy 0.995999,\n",
    "\tDev epoch 24, average loss 0.329978, average accuracy 0.926716,\n",
    "\t\tTime taken for 24 epochs =  862.4385898113251\n",
    "Train epoch 27, average loss 0.0112456, average accuracy 0.996409,\n",
    "Train epoch 30, average loss 0.00972575, average accuracy 0.996939,\n",
    "\tDev epoch 30, average loss 0.361886, average accuracy 0.92695,\n",
    "Train epoch 33, average loss 0.00892223, average accuracy 0.997189,\n",
    "Train epoch 36, average loss 0.00712946, average accuracy 0.997789,\n",
    "\tDev epoch 36, average loss 0.381359, average accuracy 0.92685,\n",
    "\t\tTime taken for 36 epochs =  1275.2902917861938\n",
    "Train epoch 39, average loss 0.00628213, average accuracy 0.998139,\n",
    "Train epoch 42, average loss 0.00596454, average accuracy 0.998159,\n",
    "\tDev epoch 42, average loss 0.47203, average accuracy 0.924446,\n",
    "Train epoch 45, average loss 0.00500223, average accuracy 0.9985,\n",
    "Train epoch 48, average loss 0.00469857, average accuracy 0.99864,\n",
    "\tDev epoch 48, average loss 0.401148, average accuracy 0.926749,\n",
    "\t\tTime taken for 48 epochs =  1688.264419078827\n",
    "Train epoch 51, average loss 0.00402232, average accuracy 0.99886,\n",
    "Train epoch 54, average loss 0.00421826, average accuracy 0.99865,\n",
    "\tDev epoch 54, average loss 0.416046, average accuracy 0.928385,\n",
    "Train epoch 57, average loss 0.00368313, average accuracy 0.99893,\n",
    "Train epoch 60, average loss 0.00335348, average accuracy 0.99901,\n",
    "\tDev epoch 60, average loss 0.438822, average accuracy 0.927918,\n",
    "\t\tTime taken for 60 epochs =  2101.384346008301\n",
    "\n",
    "#### With the embeddings set to trainable = false.\n",
    "completed cnn creation\n",
    "# batches = 1562\n",
    "Train epoch 0, average loss 0.304837, average accuracy 0.87436,\n",
    "\t\tDev epoch 0, average loss 0.239171, average accuracy 0.905649,\n",
    "\t\t\t\t    Time taken for 0 epochs =  33.351370334625244\n",
    "Train epoch 3, average loss 0.159725, average accuracy 0.93797,\n",
    "Train epoch 6, average loss 0.0951332, average accuracy 0.963478,\n",
    "\t\tDev epoch 6, average loss 0.222475, average accuracy 0.916299,\n",
    "Train epoch 9, average loss 0.0597436, average accuracy 0.978053,\n",
    "Train epoch 12, average loss 0.0414887, average accuracy 0.985045,\n",
    "\t\tDev epoch 12, average loss 0.258621, average accuracy 0.921708,\n",
    "\t\t\t\t    Time taken for 12 epochs =  405.9028522968292\n",
    "Train epoch 15, average loss 0.03174, average accuracy 0.988536,\n",
    "Train epoch 18, average loss 0.0268379, average accuracy 0.990747,\n",
    "\t\tDev epoch 18, average loss 0.308509, average accuracy 0.922242,\n",
    "Train epoch 21, average loss 0.0237158, average accuracy 0.991737,\n",
    "Train epoch 24, average loss 0.0182942, average accuracy 0.993528,\n",
    "\t\tDev epoch 24, average loss 0.330288, average accuracy 0.922643,\n",
    "\t\t\t\t    Time taken for 24 epochs =  778.5845618247986\n",
    "Train epoch 27, average loss 0.0170312, average accuracy 0.994198,\n",
    "Train epoch 30, average loss 0.0140965, average accuracy 0.994978,\n",
    "\t\tDev epoch 30, average loss 0.389294, average accuracy 0.922476,\n",
    "Train epoch 33, average loss 0.0119405, average accuracy 0.996029,\n",
    "Train epoch 36, average loss 0.0102918, average accuracy 0.996929,\n",
    "\t\tDev epoch 36, average loss 0.390921, average accuracy 0.922943,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1150.900290966034\n",
    "Train epoch 39, average loss 0.00885309, average accuracy 0.997279,\n",
    "Train epoch 42, average loss 0.0098332, average accuracy 0.996869,\n",
    "\t\tDev epoch 42, average loss 0.408098, average accuracy 0.922476,\n",
    "Train epoch 45, average loss 0.00801163, average accuracy 0.997559,\n",
    "Train epoch 48, average loss 0.0075703, average accuracy 0.997559,\n",
    "\t\tDev epoch 48, average loss 0.405061, average accuracy 0.920339,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1523.076869726181\n",
    "Train epoch 51, average loss 0.00815841, average accuracy 0.997429,\n",
    "Train epoch 54, average loss 0.00750207, average accuracy 0.997439,\n",
    "\t\tDev epoch 54, average loss 0.411605, average accuracy 0.922109,\n",
    "Train epoch 57, average loss 0.00687906, average accuracy 0.997739,\n",
    "Train epoch 60, average loss 0.00724441, average accuracy 0.997749,\n",
    "\t\tDev epoch 60, average loss 0.513384, average accuracy 0.920039,\n",
    "\t\t\t\t    Time taken for 60 epochs =  1895.2125108242035\n",
    "                    \n",
    " #### embedding dim = 50. trainable = tur\n",
    " Writing to /home/ubuntu/W266Project/final_project/runs/cnn\n",
    "\n",
    "completed cnn creation\n",
    "# batches = 1562\n",
    "Train epoch 0, average loss 0.344391, average accuracy 0.857604,\n",
    "\t\tDev epoch 0, average loss 0.289542, average accuracy 0.875501,\n",
    "\t\t\t\t    Time taken for 0 epochs =  32.877453088760376\n",
    "Train epoch 3, average loss 0.205036, average accuracy 0.915813,\n",
    "Train epoch 6, average loss 0.143437, average accuracy 0.943792,\n",
    "\t\tDev epoch 6, average loss 0.245589, average accuracy 0.913261,\n",
    "Train epoch 9, average loss 0.10241, average accuracy 0.960037,\n",
    "Train epoch 12, average loss 0.0766644, average accuracy 0.970851,\n",
    "\t\tDev epoch 12, average loss 0.256715, average accuracy 0.918636,\n",
    "\t\t\t\t    Time taken for 12 epochs =  357.13569045066833\n",
    "Train epoch 15, average loss 0.0596088, average accuracy 0.978213,\n",
    "Train epoch 18, average loss 0.0488154, average accuracy 0.982314,\n",
    "\t\tDev epoch 18, average loss 0.298308, average accuracy 0.917835,\n",
    "Train epoch 21, average loss 0.0426194, average accuracy 0.984715,\n",
    "Train epoch 24, average loss 0.0340587, average accuracy 0.988276,\n",
    "\t\tDev epoch 24, average loss 0.324277, average accuracy 0.920339,\n",
    "\t\t\t\t    Time taken for 24 epochs =  681.0641686916351\n",
    "Train epoch 27, average loss 0.0312987, average accuracy 0.989157,\n",
    "Train epoch 30, average loss 0.0283847, average accuracy 0.990167,\n",
    "\t\tDev epoch 30, average loss 0.374785, average accuracy 0.92261,\n",
    "Train epoch 33, average loss 0.0240973, average accuracy 0.991917,\n",
    "Train epoch 36, average loss 0.0231575, average accuracy 0.992087,\n",
    "\t\tDev epoch 36, average loss 0.372793, average accuracy 0.922175,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1004.7895133495331\n",
    "Train epoch 39, average loss 0.0181793, average accuracy 0.993778,\n",
    "Train epoch 42, average loss 0.0158069, average accuracy 0.994878,\n",
    "\t\tDev epoch 42, average loss 0.394058, average accuracy 0.917535,\n",
    "Train epoch 45, average loss 0.0152652, average accuracy 0.994888,\n",
    "Train epoch 48, average loss 0.0122295, average accuracy 0.995979,\n",
    "\t\tDev epoch 48, average loss 0.415253, average accuracy 0.917601,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1328.6486732959747\n",
    "Train epoch 51, average loss 0.0120298, average accuracy 0.996009,\n",
    "Train epoch 54, average loss 0.0118266, average accuracy 0.996119,\n",
    "\t\tDev epoch 54, average loss 0.413061, average accuracy 0.922443,\n",
    "        \n",
    "#### increased embedding size to 200\n",
    "\n",
    "Train epoch 0, average loss 0.286025, average accuracy 0.882512,\n",
    "\t\tDev epoch 0, average loss 0.238953, average accuracy 0.901142,\n",
    "\t\t\t\t    Time taken for 0 epochs =  110.49707412719727\n",
    "Train epoch 3, average loss 0.115594, average accuracy 0.956656,\n",
    "Train epoch 6, average loss 0.0497285, average accuracy 0.982724,\n",
    "\t\tDev epoch 6, average loss 0.225743, average accuracy 0.929988,\n",
    "Train epoch 9, average loss 0.0273755, average accuracy 0.990967,\n",
    "Train epoch 12, average loss 0.0186573, average accuracy 0.993748,\n",
    "\t\tDev epoch 12, average loss 0.281195, average accuracy 0.931424,\n",
    "\t\t\t\t    Time taken for 12 epochs =  803.1010024547577\n",
    "Train epoch 15, average loss 0.0134854, average accuracy 0.995819,\n",
    "Train epoch 18, average loss 0.0100261, average accuracy 0.997069,\n",
    "\t\tDev epoch 18, average loss 0.315512, average accuracy 0.930188,\n",
    "Train epoch 21, average loss 0.00782281, average accuracy 0.997709,\n",
    "Train epoch 24, average loss 0.00664231, average accuracy 0.998049,\n",
    "\t\tDev epoch 24, average loss 0.325848, average accuracy 0.930288,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1495.02197265625\n",
    "Train epoch 27, average loss 0.0047185, average accuracy 0.9988,\n",
    "Train epoch 30, average loss 0.00426745, average accuracy 0.99881,\n",
    "\t\tDev epoch 30, average loss 0.402758, average accuracy 0.92892,\n",
    "Train epoch 33, average loss 0.00427015, average accuracy 0.99867,\n",
    "Train epoch 36, average loss 0.00313919, average accuracy 0.99917,\n",
    "\t\tDev epoch 36, average loss 0.364247, average accuracy 0.931858,\n",
    "\t\t\t\t    Time taken for 36 epochs =  2186.8154296875\n",
    "Train epoch 39, average loss 0.00310154, average accuracy 0.99915,\n",
    "Train epoch 42, average loss 0.00245393, average accuracy 0.99941,\n",
    "\t\tDev epoch 42, average loss 0.430721, average accuracy 0.930255,\n",
    "        \n",
    "#### decreased embedding size to 100, and reduced number of filters to 128 per filter size.\n",
    "\n",
    "# batches = 1562\n",
    "Train epoch 0, average loss 0.324282, average accuracy 0.864767,\n",
    "\t\tDev epoch 0, average loss 0.247642, average accuracy 0.896368,\n",
    "\t\t\t\t    Time taken for 0 epochs =  31.22071099281311\n",
    "Train epoch 3, average loss 0.170937, average accuracy 0.932518,\n",
    "Train epoch 6, average loss 0.111375, average accuracy 0.956436,\n",
    "\t\tDev epoch 6, average loss 0.215396, average accuracy 0.916533,\n",
    "Train epoch 9, average loss 0.0755983, average accuracy 0.971181,\n",
    "Train epoch 12, average loss 0.0536806, average accuracy 0.980444,\n",
    "\t\tDev epoch 12, average loss 0.272522, average accuracy 0.921541,\n",
    "\t\t\t\t    Time taken for 12 epochs =  300.37999510765076\n",
    "Train epoch 15, average loss 0.0438895, average accuracy 0.983705,\n",
    "Train epoch 18, average loss 0.0348257, average accuracy 0.987456,\n",
    "\t\tDev epoch 18, average loss 0.311513, average accuracy 0.922676,\n",
    "Train epoch 21, average loss 0.0279689, average accuracy 0.990087,\n",
    "Train epoch 24, average loss 0.025247, average accuracy 0.990957,\n",
    "\t\tDev epoch 24, average loss 0.341999, average accuracy 0.923811,\n",
    "\t\t\t\t    Time taken for 24 epochs =  569.6881365776062\n",
    "Train epoch 27, average loss 0.0203997, average accuracy 0.992968,\n",
    "Train epoch 30, average loss 0.0187122, average accuracy 0.993508,\n",
    "\t\tDev epoch 30, average loss 0.357159, average accuracy 0.923811,\n",
    "Train epoch 33, average loss 0.0158869, average accuracy 0.994488,\n",
    "Train epoch 36, average loss 0.0158628, average accuracy 0.994568,\n",
    "\t\tDev epoch 36, average loss 0.371735, average accuracy 0.92311,\n",
    "\t\t\t\t    Time taken for 36 epochs =  838.69158244133\n",
    "Train epoch 39, average loss 0.0133922, average accuracy 0.995659,\n",
    "Train epoch 42, average loss 0.0127672, average accuracy 0.995709,\n",
    "\t\tDev epoch 42, average loss 0.402635, average accuracy 0.92498,\n",
    "Train epoch 45, average loss 0.0119068, average accuracy 0.996049,\n",
    "Train epoch 48, average loss 0.0107565, average accuracy 0.996499,\n",
    "\t\tDev epoch 48, average loss 0.419966, average accuracy 0.92478,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1107.9437527656555\n",
    "Train epoch 51, average loss 0.0100834, average accuracy 0.996689,\n",
    "Train epoch 54, average loss 0.00994073, average accuracy 0.996889,\n",
    "\t\tDev epoch 54, average loss 0.41175, average accuracy 0.924245,\n",
    "Train epoch 57, average loss 0.00979135, average accuracy 0.996649,\n",
    "Train epoch 60, average loss 0.00821928, average accuracy 0.997469,\n",
    "\t\tDev epoch 60, average loss 0.469267, average accuracy 0.926115,\n",
    "\t\t\t\t    Time taken for 60 epochs =  1377.0541887283325\n",
    "                    \n",
    " #### increased number of filters to 512 per filter size.\n",
    " \n",
    " Train epoch 0, average loss 0.311692, average accuracy 0.870288,\n",
    "\t\tDev epoch 0, average loss 0.242969, average accuracy 0.896701,\n",
    "\t\t\t\t    Time taken for 0 epochs =  84.6077299118042\n",
    "Train epoch 3, average loss 0.142796, average accuracy 0.944482,\n",
    "Train epoch 6, average loss 0.0668666, average accuracy 0.975692,\n",
    "\t\tDev epoch 6, average loss 0.210706, average accuracy 0.924913,\n",
    "Train epoch 9, average loss 0.0342145, average accuracy 0.988336,\n",
    "Train epoch 12, average loss 0.0224968, average accuracy 0.992708,\n",
    "\t\tDev epoch 12, average loss 0.274663, average accuracy 0.926916,\n",
    "\t\t\t\t    Time taken for 12 epochs =  809.0257966518402\n",
    "Train epoch 15, average loss 0.0171242, average accuracy 0.994098,\n",
    "Train epoch 18, average loss 0.0118646, average accuracy 0.996329,\n",
    "\t\tDev epoch 18, average loss 0.302414, average accuracy 0.928218,\n",
    "Train epoch 21, average loss 0.0098251, average accuracy 0.996879,\n",
    "Train epoch 24, average loss 0.0067338, average accuracy 0.998029,\n",
    "\t\tDev epoch 24, average loss 0.331184, average accuracy 0.927985,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1532.959394454956\n",
    "Train epoch 27, average loss 0.00506558, average accuracy 0.99858,\n",
    "Train epoch 30, average loss 0.00396653, average accuracy 0.99893,\n",
    "\t\tDev epoch 30, average loss 0.343347, average accuracy 0.928352,\n",
    "Train epoch 33, average loss 0.00384556, average accuracy 0.99898,\n",
    "Train epoch 36, average loss 0.0030804, average accuracy 0.99932,\n",
    "\t\tDev epoch 36, average loss 0.373929, average accuracy 0.928552,\n",
    "\t\t\t\t    Time taken for 36 epochs =  2256.9254744052887\n",
    "Train epoch 39, average loss 0.00246744, average accuracy 0.99933,\n",
    "Train epoch 42, average loss 0.00211266, average accuracy 0.9996,\n",
    "\t\tDev epoch 42, average loss 0.361598, average accuracy 0.929053,\n",
    "Train epoch 45, average loss 0.00217611, average accuracy 0.99941,\n",
    "Train epoch 48, average loss 0.00197936, average accuracy 0.99955,\n",
    "\t\tDev epoch 48, average loss 0.391054, average accuracy 0.929754,\n",
    "\t\t\t\t    Time taken for 48 epochs =  2980.909019947052\n",
    "Train epoch 51, average loss 0.00187027, average accuracy 0.99957,\n",
    "Train epoch 54, average loss 0.00191055, average accuracy 0.99958,\n",
    "\t\tDev epoch 54, average loss 0.378678, average accuracy 0.92685,\n",
    "Train epoch 57, average loss 0.00153142, average accuracy 0.99968,\n",
    "Train epoch 60, average loss 0.00134926, average accuracy 0.99974,\n",
    "\t\tDev epoch 60, average loss 0.405928, average accuracy 0.929854,\n",
    "\t\t\t\t    Time taken for 60 epochs =  3704.6175475120544\n",
    "                    \n",
    "completed cnn creation\n",
    "#### added a filter for length 6. so filter sizes became (3,4,5,6). \n",
    "Train epoch 0, average loss 0.315134, average accuracy 0.868868,\n",
    "\t\tDev epoch 0, average loss 0.248855, average accuracy 0.89353,\n",
    "\t\t\t\t    Time taken for 0 epochs =  69.76286554336548\n",
    "Train epoch 3, average loss 0.149292, average accuracy 0.942482,\n",
    "Train epoch 6, average loss 0.072978, average accuracy 0.973211,\n",
    "\t\tDev epoch 6, average loss 0.220149, average accuracy 0.921908,\n",
    "Train epoch 9, average loss 0.0399216, average accuracy 0.985855,\n",
    "Train epoch 12, average loss 0.0274265, average accuracy 0.990367,\n",
    "\t\tDev epoch 12, average loss 0.321294, average accuracy 0.922342,\n",
    "\t\t\t\t    Time taken for 12 epochs =  666.9912831783295\n",
    "Train epoch 15, average loss 0.0199664, average accuracy 0.993398,\n",
    "Train epoch 18, average loss 0.0145588, average accuracy 0.995429,\n",
    "\t\tDev epoch 18, average loss 0.30958, average accuracy 0.921341,\n",
    "Train epoch 21, average loss 0.011635, average accuracy 0.996449,\n",
    "Train epoch 24, average loss 0.00927037, average accuracy 0.997119,\n",
    "\t\tDev epoch 24, average loss 0.352559, average accuracy 0.926449,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1264.2059795856476\n",
    "Train epoch 27, average loss 0.00758558, average accuracy 0.997789,\n",
    "Train epoch 30, average loss 0.0056179, average accuracy 0.998409,\n",
    "\t\tDev epoch 30, average loss 0.385125, average accuracy 0.926749,\n",
    "Train epoch 33, average loss 0.0049369, average accuracy 0.99856,\n",
    "Train epoch 36, average loss 0.00391196, average accuracy 0.99882,\n",
    "\t\tDev epoch 36, average loss 0.369655, average accuracy 0.926482,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1861.2735829353333\n",
    "Train epoch 39, average loss 0.00425983, average accuracy 0.99884,\n",
    "Train epoch 42, average loss 0.00378442, average accuracy 0.99898,\n",
    "\t\tDev epoch 42, average loss 0.386586, average accuracy 0.926883,\n",
    "Train epoch 45, average loss 0.00346757, average accuracy 0.99902,\n",
    "Train epoch 48, average loss 0.00338043, average accuracy 0.99902,\n",
    "\t\tDev epoch 48, average loss 0.395295, average accuracy 0.927784,\n",
    "\t\t\t\t    Time taken for 48 epochs =  2458.481989145279\n",
    "Train epoch 51, average loss 0.00368297, average accuracy 0.99901,\n",
    "Train epoch 54, average loss 0.00253086, average accuracy 0.99935,\n",
    "\t\tDev epoch 54, average loss 0.410479, average accuracy 0.92685,\n",
    "Train epoch 57, average loss 0.00307247, average accuracy 0.99915,\n",
    "Train epoch 60, average loss 0.00267205, average accuracy 0.99934,\n",
    "\t\tDev epoch 60, average loss 0.449313, average accuracy 0.926215,\n",
    "\t\t\t\t    Time taken for 60 epochs =  3055.6355838775635 \n",
    "                    \n",
    "                    \n",
    "#### With random shuffle added for batches.\n",
    "Train epoch 0, average loss 0.315134, average accuracy 0.868868,\n",
    "\t\tDev epoch 0, average loss 0.248855, average accuracy 0.89353,\n",
    "\t\t\t\t    Time taken for 0 epochs =  69.76286554336548\n",
    "Train epoch 3, average loss 0.149292, average accuracy 0.942482,\n",
    "Train epoch 6, average loss 0.072978, average accuracy 0.973211,\n",
    "\t\tDev epoch 6, average loss 0.220149, average accuracy 0.921908,\n",
    "Train epoch 9, average loss 0.0399216, average accuracy 0.985855,\n",
    "Train epoch 12, average loss 0.0274265, average accuracy 0.990367,\n",
    "\t\tDev epoch 12, average loss 0.321294, average accuracy 0.922342,\n",
    "\t\t\t\t    Time taken for 12 epochs =  666.9912831783295\n",
    "Train epoch 15, average loss 0.0199664, average accuracy 0.993398,\n",
    "Train epoch 18, average loss 0.0145588, average accuracy 0.995429,\n",
    "\t\tDev epoch 18, average loss 0.30958, average accuracy 0.921341,\n",
    "Train epoch 21, average loss 0.011635, average accuracy 0.996449,\n",
    "Train epoch 24, average loss 0.00927037, average accuracy 0.997119,\n",
    "\t\tDev epoch 24, average loss 0.352559, average accuracy 0.926449,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1264.2059795856476\n",
    "Train epoch 27, average loss 0.00758558, average accuracy 0.997789,\n",
    "Train epoch 30, average loss 0.0056179, average accuracy 0.998409,\n",
    "\t\tDev epoch 30, average loss 0.385125, average accuracy 0.926749,\n",
    "Train epoch 33, average loss 0.0049369, average accuracy 0.99856,\n",
    "Train epoch 36, average loss 0.00391196, average accuracy 0.99882,\n",
    "\t\tDev epoch 36, average loss 0.369655, average accuracy 0.926482,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1861.2735829353333\n",
    "Train epoch 39, average loss 0.00425983, average accuracy 0.99884,\n",
    "Train epoch 42, average loss 0.00378442, average accuracy 0.99898,\n",
    "\t\tDev epoch 42, average loss 0.386586, average accuracy 0.926883,\n",
    "Train epoch 45, average loss 0.00346757, average accuracy 0.99902,\n",
    "Train epoch 48, average loss 0.00338043, average accuracy 0.99902,\n",
    "\t\tDev epoch 48, average loss 0.395295, average accuracy 0.927784,\n",
    "\t\t\t\t    Time taken for 48 epochs =  2458.481989145279\n",
    "Train epoch 51, average loss 0.00368297, average accuracy 0.99901,\n",
    "Train epoch 54, average loss 0.00253086, average accuracy 0.99935,\n",
    "\t\tDev epoch 54, average loss 0.410479, average accuracy 0.92685,\n",
    "Train epoch 57, average loss 0.00307247, average accuracy 0.99915,\n",
    "Train epoch 60, average loss 0.00267205, average accuracy 0.99934,\n",
    "\t\tDev epoch 60, average loss 0.449313, average accuracy 0.926215,\n",
    "\t\t\t\t    Time taken for 60 epochs =  3055.6355838775635"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
