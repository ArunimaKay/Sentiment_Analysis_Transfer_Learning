{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arunima/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/arunima/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "import gzip\n",
    "from collections import namedtuple\n",
    "import tflearn\n",
    "# Helper libraries\n",
    "from common import vocabulary, tf_embed_viz, glove_helper\n",
    "from common import utils; reload(utils)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.50d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 50))\n"
     ]
    }
   ],
   "source": [
    "#Using pretrained GLove embeddings\n",
    "hands = glove_helper.Hands(ndim=50)  # 50, 100, 200, 300 dim are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400003, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hands.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to read the amazon review data files\n",
    "def parse(path):\n",
    "  print('start parse')\n",
    "  start_parse = time.time()\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "  end_parse = time.time()\n",
    "  print('end parse with time for parse',end_parse - start_parse)\n",
    "\n",
    "def getDF(path):\n",
    "  print('start getDF')\n",
    "  start = time.time()\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  print('end getDF')\n",
    "  end = time.time()\n",
    "  print('time taken to load data = ',end-start)\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "#df = getDF('reviews_Toys_and_Games.json.gz') #old def function corresponding to the step bt step vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 247.7467555999756\n",
      "end getDF\n",
      "time taken to load data =  247.7471568584442\n"
     ]
    }
   ],
   "source": [
    "df_hnk = getDF('reviews_Home_and_Kitchen.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_vid = getDF('reviews_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 64.39745140075684\n",
      "end getDF\n",
      "time taken to load data =  64.39804863929749\n"
     ]
    }
   ],
   "source": [
    "df_aut = getDF('reviews_Automotive.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 105.62234997749329\n",
      "end getDF\n",
      "time taken to load data =  105.62298202514648\n"
     ]
    }
   ],
   "source": [
    "df_toys = getDF('reviews_Toys_and_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy reviews train, dev and test set dataframe shape: (1351662, 9) (450554, 9) (450555, 9)\n",
      "Auto reviews train, dev and test set dataframe shape: (824260, 9) (274754, 9) (274754, 9)\n"
     ]
    }
   ],
   "source": [
    "#Create train,dev,test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_toys,devtest = train_test_split(df_toys, test_size=0.4, random_state=42)\n",
    "dev_toys,test_toys = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Toy reviews train, dev and test set dataframe shape:',train_toys.shape,dev_toys.shape,test_toys.shape)\n",
    "\n",
    "#For Video games reviews\n",
    "# train_vid,devtest = train_test_split(df_vid, test_size=0.4)\n",
    "# dev_vid,test_vid = train_test_split(devtest,test_size = 0.5)\n",
    "# print('Video games reviews train, dev and test set dataframe shape:',train_vid.shape,dev_vid.shape,test_vid.shape)\n",
    "\n",
    "#For Auto reviews\n",
    "train_aut,devtest = train_test_split(df_aut, test_size=0.4)\n",
    "dev_aut,test_aut = train_test_split(devtest,test_size = 0.5)\n",
    "print('Auto reviews train, dev and test set dataframe shape:',train_aut.shape,dev_aut.shape,test_aut.shape)\n",
    "\n",
    "#For Home and Kitchen reviews\n",
    "# train_hnk,devtest = train_test_split(df_hnk, test_size=0.4, random_state=42)\n",
    "# dev_hnk,test_hnk = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "# print('Home and Kitchen reviews train, dev and test set dataframe shape:',train_hnk.shape,dev_hnk.shape,test_hnk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             reviewerID        asin reviewerName helpful  \\\n",
      "1916352  A3I1XP89YM02QN  B009GXSBCG       Sleepy  [0, 0]   \n",
      "1814240   ADDYT3AUNIIGR  B0085Y3YV0   wetarugula  [0, 0]   \n",
      "\n",
      "                                                reviewText  overall  \\\n",
      "1916352  Son collects DC and Marvel comic collectables....      5.0   \n",
      "1814240  No complaints here. They are a bit thinner tha...      4.0   \n",
      "\n",
      "              summary  unixReviewTime  reviewTime  \n",
      "1916352  Collectables      1389225600  01 9, 2014  \n",
      "1814240        Great!      1388707200  01 3, 2014  \n",
      "             reviewerID        asin       reviewerName helpful  \\\n",
      "1736076   AMAHICM9D5E5H  B007HE310I  Deborah J Rumsley  [0, 0]   \n",
      "1208641  A3FSNQS8T99H1N  B003NVJ7WK          ralechner  [0, 0]   \n",
      "\n",
      "                                                reviewText  overall  \\\n",
      "1736076  My 2 year old grandson likes to play with his ...      4.0   \n",
      "1208641  Although there are many parts here, pots, pans...      5.0   \n",
      "\n",
      "                                                   summary  unixReviewTime  \\\n",
      "1736076                                          wolf wolf      1357344000   \n",
      "1208641  great fun for a 5 year old AND a 1 1/2 year old !      1393977600   \n",
      "\n",
      "         reviewTime  \n",
      "1736076  01 5, 2013  \n",
      "1208641  03 5, 2014  \n"
     ]
    }
   ],
   "source": [
    "#checking that we have different product ids\n",
    "print(train_toys.head(2))\n",
    "print(dev_toys.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to create a smaller sized train and dev data set. Enables testing accuracy for different sizes.\n",
    "#Also binarizes the labels. Ratings of 1,2 and to 0; Ratings of 4,5 to 1.\n",
    "\n",
    "def set_df_size(size,data_train,data_dev):\n",
    "    size_train = size\n",
    "    len_max_train = data_train[data_train.overall!=3].shape[0] #max possible length of train data set taking out the 3 ratings.\n",
    "    #print(\"Number of reviews with ratings != 3 in train set\",len_max_train)\n",
    "    temp_size_train = min(len_max_train,size_train)\n",
    "\n",
    "    len_max_dev = data_dev[data_dev.overall!=3].shape[0]\n",
    "    #print(\"Number of reviews with ratings != 3 in dev set\",len_max_dev)\n",
    "    temp_size_dev = min(len_max_dev,int(0.3*temp_size_train)) #making the dev set about 0.3 times the train set.\n",
    "\n",
    "    temp_train_data = data_train[data_train.overall != 3][:temp_size_train]\n",
    "    #print('Size of train data',temp_train_data.shape)\n",
    "    #print(temp_train_data.groupby('overall').count())\n",
    "    #print(temp_train_toys[:5])\n",
    "\n",
    "    temp_dev_data = data_dev[data_dev.overall!=3][:temp_size_dev]\n",
    "    #print('Size of dev data',temp_dev_data.shape)\n",
    "    #print(temp_dev_data.groupby('overall').count())\n",
    "    #print(temp_dev_data[:2])\n",
    "    \n",
    "    #Binarize ratings\n",
    "    temp_train_y = np.zeros(temp_size_train)\n",
    "    temp_train_y[temp_train_data.overall > 3] = 1\n",
    "    temp_dev_y = np.zeros(temp_size_dev)\n",
    "    temp_dev_y[temp_dev_data.overall>3] = 1\n",
    "    #print('binarized y shape',temp_train_y.shape,temp_dev_y.shape)\n",
    "    #print(temp_dev_y[:20],data_dev.overall[:20])\n",
    "    return temp_train_data,temp_dev_data,temp_train_y,temp_dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] #list of keys that refer to each dataframe. Adding a new dataframe would require updating this list\n",
    "dict_train_df = {} #Dict to store train input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_dev_df = {} #Dict to store dev input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_train_y = {} #Dict to store binarized train data label for each domain\n",
    "dict_dev_y = {} #Dict to store binarized dev data label for each domain\n",
    "#print(len(dict_train_df))\n",
    "\n",
    "def create_sized_data(size = 10000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "    key = list_df[0]\n",
    "    # print('Toys reviews\\n')\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "#     #print('\\n Video games reviews\\n')\n",
    "#     key = list_df[1]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_vid,dev_vid)\n",
    "    # print('\\n Auto reviews\\n')\n",
    "    key = list_df[2]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_aut,dev_aut)\n",
    "#     #print('\\n Home and Kitchen reviews\\n')\n",
    "#     key = list_df[3]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_hnk,dev_hnk)\n",
    "#print(len(dict_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9)\n"
     ]
    }
   ],
   "source": [
    "list_df = ['toys','vid','aut','hnk']\n",
    "train_data_size = 50000\n",
    "create_sized_data(train_data_size)\n",
    "print(dict_train_df['toys'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "0\n",
      "x_train_tokens_list length 10000\n",
      "Vocabulary size: 24,801\n",
      "Total words  800308\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing steps - old code. Don't need to run. Vocab_processor is much faster.\n",
    "\n",
    "#Changing to nltk punkt tokenizer as the periods are not getting removed\n",
    "print(dict_train_df['hnk'].shape[0])\n",
    "\n",
    "train_cnt = collections.Counter()\n",
    "x_train_tokens_list = []\n",
    "start = time.time()\n",
    "for i in range(dict_train_df['hnk'].shape[0]):\n",
    "    #print(dict_train_df['hnk'].iloc[i][2])\n",
    "    x_train_tokens = word_tokenize(dict_train_df['hnk'].iloc[i][4])    \n",
    "\n",
    "    #2. changing to lowercase and replacing numbers(are we losing any context by replacing all numbers in the review test?)\n",
    "    x_tokens_canonical = utils.canonicalize_words(x_train_tokens)   \n",
    "    x_train_tokens_list.append(x_tokens_canonical)\n",
    "    \n",
    "    if i%10000 == 0:\n",
    "        print(i) \n",
    "    #3. Build vocabulary\n",
    "    for items in x_tokens_canonical:\n",
    "            train_cnt[items] += 1\n",
    "            \n",
    "vocab = vocabulary.Vocabulary(train_cnt, size=None)  # size=None means unlimited\n",
    "total_words = sum(train_cnt.values())\n",
    "print(\"x_train_tokens_list length\", len(x_train_tokens_list))\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "#print(\"Vocabulary dict: \", vocab.word_to_id)\n",
    "print(\"Total words \",total_words )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_tokens_list[1]))\n",
    "print(x_train_tokens_list[1])\n",
    "print(len(x_train_tokens_list[0]))\n",
    "print(x_train_tokens_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test_tokens_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e85a12b3f03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_id_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_test_tokens_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_id_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test_tokens_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Converting all reviews to ids \n",
    "train_id_list = []\n",
    "for item in x_train_tokens_list:\n",
    "    train_id_list.append(vocab.words_to_ids(item))\n",
    "    \n",
    "test_id_list = []\n",
    "for item in x_test_tokens_list:\n",
    "    test_id_list.append(vocab.words_to_ids(item))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'item', 'was', 'well', 'made', ',', 'and', 'the', 'instructions', 'for', 'personalizing', 'it', 'by', 'the', 'persons', 'for', 'whom', 'i', 'bought', 'it', 'were', 'clear', 'and', 'uncomplicated', '.', 'so', 'i', 'hope', 'they', 'will', 'enjoy', 'putting', 'it', 'together', 'with', 'their', 'many', 'wine', 'corks', '.']\n"
     ]
    }
   ],
   "source": [
    "print((x_train_tokens_list[1]))\n",
    "print(max((train_id_list)))\n",
    "review_lengths = [len(review) for review in x_train_tokens_list]\n",
    "print(\"Shortest review:\", min(review_lengths))\n",
    "print(\"Longest review:\",max(review_lengths))\n",
    "pd.DataFrame(review_lengths).describe()\n",
    "print('90th percentile',pd.DataFrame(review_lengths).quantile(0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEICAYAAABBBrPDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4220826d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.axes()\n",
    "sns.distplot(review_lengths)\n",
    "ax.set_title(\"Distribution of the review lengths\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Based on the above, we are setting a max_length of 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vocab_processor = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=0)\n",
    "#Note : This function seems to be deprecated. Another function I ran into \n",
    "# tflearn.data_utils.VocabularyProcessor (max_document_length, min_frequency=3, vocabulary=None, tokenizer_fn=None)\n",
    "\n",
    "def process_inputs(key, vocab_processor):\n",
    "    \n",
    "    # For simplicity, we call our features x and our outputs y\n",
    "    start_vectorize = time.time()\n",
    "    x_train = dict_train_df[key].reviewText\n",
    "    y_train = dict_train_y[key]\n",
    "    x_dev = dict_dev_df[key].reviewText\n",
    "    y_dev = dict_dev_y[key]\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    # Train the vocab_processor from the training set\n",
    "    x_train = vocab_processor.fit_transform(x_train)\n",
    "    # Transform our test set with the vocabulary processor\n",
    "    x_dev = vocab_processor.transform(x_dev)\n",
    "\n",
    "    # We need these to be np.arrays instead of generators\n",
    "    x_train = np.array(list(x_train))\n",
    "    print(x_train.shape)\n",
    "    x_dev = np.array(list(x_dev))\n",
    "    y_train = np.array(y_train).astype(int)\n",
    "    y_dev = np.array(y_dev).astype(int)\n",
    "    \n",
    "#     y_train = tf.expand_dims(y_train,1)\n",
    "#     y_dev = tf.expand_dims(y_dev,1)\n",
    "    print('y train shape',y_train.shape)\n",
    "\n",
    "    V = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % V)\n",
    "    end_vectorize = time.time()\n",
    "    print('Time taken to vectorize %d size dataframe'%x_train.shape[0],end_vectorize-start_vectorize)\n",
    "\n",
    "    # Return the transformed data and the number of words\n",
    "    return x_train, y_train, x_dev, y_dev, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_df = ['toys','aut'] #redefining here to limit further analysis to these domains only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 5993\n",
      "Time taken to vectorize 10000 size dataframe 1.5661492347717285\n",
      "Number words in training corpus for toys 5993\n",
      "toys dataset id shapes (10000, 150) (3000, 150)\n",
      "sample review for domain toys It's just the model I was hoping for and more. It's challenging and has given me something more to learn in developing my model building skills. Revell as always makes good models to build! \n",
      "\n",
      "corresponding ids\n",
      " [ 111   38    1  319    6   14 1067    8    2   51  111  755    2   42  577\n",
      "   97  225   51    3  366   12 4337   15  319  535  858 5014   22  272  211\n",
      "   61  782    3  389    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0] \n",
      "\n",
      "aut\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 5559\n",
      "Time taken to vectorize 10000 size dataframe 1.4251835346221924\n",
      "Number words in training corpus for aut 5559\n",
      "aut dataset id shapes (10000, 150) (3000, 150)\n",
      "sample review for domain aut great product! very useful and compact.  easy to store in the car. recommend one for all car owners especially when traveling. \n",
      "\n",
      "corresponding ids\n",
      " [  24   30   31 1075    2 1541   57    4  354   13    1   34   78   25    8\n",
      "   47   34 2283  585   52 2161    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting reviews to ids for all domains and add padding.\n",
    "\n",
    "# Hyperparameters\n",
    "min_frequency = 5\n",
    "max_length = 150\n",
    "\n",
    "dict_vectorizers = {} #Dict to store the vocab_processor fit on each domain\n",
    "dict_train_ids = {} #Dict to store train data reviews as sparse matrix of word ids\n",
    "dict_dev_ids = {} #Dict to store dev data reviews as sparse matrix of word ids\n",
    "dict_cnn = {} #Dict to store cnn model developed on each domain. Assumes input features are developed using the corresponding count_vectorizer\n",
    "dict_dev_ypred = {} #Dict to store dev predictions\n",
    "dict_vocab_len = {} #Store vocab length of each domain\n",
    "for key in list_df:\n",
    "    \n",
    "    #Converting ratings to tokenized word id counts as a sparse matrix using count_vectorizer\n",
    "    dict_vectorizers[key] = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=min_frequency)\n",
    "    print(key)\n",
    "    dict_train_ids[key], dict_train_y[key],dict_dev_ids[key], dict_dev_ypred[key], dict_vocab_len[key] = process_inputs(key,dict_vectorizers[key])\n",
    "    \n",
    "    print(\"Number words in training corpus for\",key,(dict_vocab_len[key]))\n",
    "    print(key,'dataset id shapes',dict_train_ids[key].shape, dict_dev_ids[key].shape)\n",
    "\n",
    "    #Print a few examples for viewing\n",
    "    print('sample review for domain',key, dict_train_df[key].reviewText.iloc[3],'\\n')\n",
    "    print('corresponding ids\\n',dict_train_ids[key][3],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys (5000,)\n",
      "(array([2879, 1248,  460,  182,   86,   47,   35,   18,   18,    2,    6,\n",
      "          5,    1,    2,    4,    3,    0,    0,    2,    2]), array([   0. ,   47.9,   95.8,  143.7,  191.6,  239.5,  287.4,  335.3,\n",
      "        383.2,  431.1,  479. ,  526.9,  574.8,  622.7,  670.6,  718.5,\n",
      "        766.4,  814.3,  862.2,  910.1,  958. ]))\n",
      "Number less than 100 4192\n",
      "Number less than 150 4613\n",
      "Number less than 175 4718\n",
      "Number less than 200 4790\n",
      "aut (5000,)\n",
      "(array([3180, 1102,  362,  177,   81,   38,   24,   10,    7,    8,    1,\n",
      "          4,    0,    1,    2,    0,    0,    1,    0,    2]), array([   1.  ,   49.85,   98.7 ,  147.55,  196.4 ,  245.25,  294.1 ,\n",
      "        342.95,  391.8 ,  440.65,  489.5 ,  538.35,  587.2 ,  636.05,\n",
      "        684.9 ,  733.75,  782.6 ,  831.45,  880.3 ,  929.15,  978.  ]))\n",
      "Number less than 100 4301\n",
      "Number less than 150 4654\n",
      "Number less than 175 4759\n",
      "Number less than 200 4826\n"
     ]
    }
   ],
   "source": [
    "#code to determine max_length for vectorization.\n",
    "#Output below was generated using max_length of 1000 in vectorizer first.\n",
    "for key in list_df:\n",
    "    length = np.count_nonzero(dict_train_ids[key],axis = 1)\n",
    "    print(key,length.shape)\n",
    "    print(np.histogram(length,bins = 20))\n",
    "    print(\"Number less than 100\",np.count_nonzero(length[length <= 100]))\n",
    "    print(\"Number less than 150\",np.count_nonzero(length[length <= 150]))\n",
    "    print(\"Number less than 175\",np.count_nonzero(length[length <= 175]))\n",
    "    print(\"Number less than 200\",np.count_nonzero(length[length <= 200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "#embed_dim = 50 #use when not using pre-trained embeddings\n",
    "embed_dim = hands.shape[1]\n",
    "lstm_size = 128\n",
    "lstm_layers = 1\n",
    "attention_size = 50\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "keep_prob = 0.8\n",
    "evaluate_train = 1 # of epochs at which to print test accuracy\n",
    "evaluate_dev = 1 # of epochs at which to estimate and print dev accuracy\n",
    "time_print = 4 # of epochs at which to print time taken\n",
    "num_classes = 2\n",
    "num_epochs = 15\n",
    "batch_size=128\n",
    "\n",
    "# out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnn\"))\n",
    "# print(\"Model saving  to {}\\n\".format(out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextRNN(object):\n",
    "\n",
    "    \"\"\"\n",
    "    RNN for text classification.\n",
    "    Uses an embedding layer, followed by an RNN and softmax layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length, lstm_size, num_classes, vocab_size, learning_rate, momentum, embedding_size, \n",
    "                 gl_embed):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.seq_length = tf.placeholder(tf.int32,[None])\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            #self.W = tf.get_variable(\"W_in\",[vocab_size, embedding_size],initializer =tf.random_uniform_initializer(0,1)) #from wildML\n",
    "            self.W=tf.get_variable(name=\"embedding_\",shape=gl_embed.shape,\n",
    "                                       initializer=tf.constant_initializer(gl_embed),trainable=True)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            print('embedded_chars',self.embedded_chars.get_shape())\n",
    "            \n",
    "        with tf.name_scope(\"rnn_layer\"):\n",
    "            basic_cell = tf.contrib.rnn.BasicLSTMCell(num_units = lstm_size)\n",
    "            cell_drop = tf.contrib.rnn.DropoutWrapper(basic_cell,input_keep_prob = self.dropout_keep_prob)\n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell([cell_drop] * lstm_layers)\n",
    "            outputs,states = tf.nn.dynamic_rnn(multi_layer_cell,self.embedded_chars,dtype = tf.float32)\n",
    "            print('output from rnn',outputs.get_shape())\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.scores = tf.contrib.layers.fully_connected(outputs[:,-1],num_classes,activation_fn = None)\n",
    "            print('self.scores',self.scores.get_shape())\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            self.pred_proba = tf.nn.softmax(self.scores, name=\"pred_proba\")\n",
    "            print('self.predictions',self.predictions.get_shape())\n",
    "            \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            #self.loss = tf.losses.mean_squared_error(self.input_y, self.scores)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "#             correct_pred = tf.equal(tf.cast(tf.round(self.scores), tf.int32), self.input_y)\n",
    "#             self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "        # AUC\n",
    "#         with tf.name_scope(\"auc\"):\n",
    "#             false_pos_rate, true_pos_rate, _ = roc_curve(self.input_y, self.pred_proba[:,1])\n",
    "#             self.auc = auc(false_pos_rate, true_pos_rate)\n",
    "            \n",
    "            \n",
    "        max_grad_norm = 1.0\n",
    "        with tf.name_scope('train'):\n",
    "            #self.optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate,momentum=momentum,use_nesterov=True).minimize(self.loss)\n",
    "            #Above is simple optimizer without gradient clipping\n",
    "            \n",
    "            #Below is code with gradient clipping\n",
    "            optimizer_ = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "            gradients, variables = zip(*optimizer_.compute_gradients(self.loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n",
    "            self.train_step_ = optimizer_.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "\n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        The Attention output `Tensor`.\n",
    "\n",
    "In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    print('Shapes of attention parameters w,b,u',w_omega.get_shape(),b_omega.get_shape(),u_omega.get_shape())\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    #print('shape of vu',vu.get_shape)\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "    #print('shape of alphas',alphas.get_shape)\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "    output.set_shape((None,hidden_size)) #Set shape explicity as fully connected function gives an error otherwise in textRNNwithA\n",
    "    print('shape of attention output',output.get_shape())\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.get_shape of <tf.Tensor 'Placeholder_19:0' shape=(2, 2) dtype=int32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Placeholder_18:0' shape=(2, 2) dtype=int32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Tensordot_1:0' shape=<unknown> dtype=int32>>\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.int32,[2,2])\n",
    "b = tf.placeholder(tf.int32,[2,2])\n",
    "feed = {a:[[1,2],[3,4]],b:[[5,6],[7,8]]}\n",
    "print(b.get_shape)\n",
    "print(a.get_shape)\n",
    "c = tf.tensordot(a,b,axes = 1)\n",
    "print(c.get_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextRNNwithA(object):\n",
    "    \"\"\"\n",
    "    RNN with Attention for text classification.\n",
    "    Uses an embedding layer, followed by an RNN and softmax layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length, lstm_size, num_classes, vocab_size, learning_rate, momentum, embedding_size, \n",
    "                 gl_embed):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.seq_length = tf.placeholder(tf.int32,[None])\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            #self.W = tf.get_variable(\"W_in\",[vocab_size, embedding_size],initializer =tf.random_uniform_initializer(0,1)) #from wildML\n",
    "            self.W=tf.get_variable(name=\"embedding_\",shape=gl_embed.shape,\n",
    "                                       initializer=tf.constant_initializer(gl_embed),trainable=True)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            print('embedded_chars',self.embedded_chars.get_shape())\n",
    "            \n",
    "        with tf.name_scope(\"rnn_layer\"):\n",
    "            basic_cell = tf.contrib.rnn.BasicLSTMCell(num_units = lstm_size)\n",
    "            cell_drop = tf.contrib.rnn.DropoutWrapper(basic_cell,input_keep_prob = self.dropout_keep_prob)\n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell([cell_drop] * lstm_layers)\n",
    "            outputs,states = tf.nn.dynamic_rnn(multi_layer_cell,self.embedded_chars,dtype = tf.float32)\n",
    "            print('output from rnn',outputs.get_shape())\n",
    "            \n",
    "        with tf.name_scope(\"attention\"):\n",
    "            attention_output, alphas = attention(outputs,attention_size, return_alphas = True)\n",
    "            print('attention_output',attention_output.get_shape())\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.scores = tf.contrib.layers.fully_connected(attention_output,num_classes,activation_fn = None) #modified to take the output of attention\n",
    "            print('self.scores',self.scores.get_shape())\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            self.pred_proba = tf.nn.softmax(self.scores, name=\"pred_proba\")\n",
    "            print('self.predictions',self.predictions.get_shape())\n",
    "            \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            #self.loss = tf.losses.mean_squared_error(self.input_y, self.scores)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "#             correct_pred = tf.equal(tf.cast(tf.round(self.scores), tf.int32), self.input_y)\n",
    "#             self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "        # AUC\n",
    "#         with tf.name_scope(\"auc\"):\n",
    "#             false_pos_rate, true_pos_rate, _ = roc_curve(self.input_y, self.pred_proba[:,1])\n",
    "#             self.auc = auc(false_pos_rate, true_pos_rate)\n",
    "            \n",
    "            \n",
    "        max_grad_norm = 1.0\n",
    "        with tf.name_scope('train'):\n",
    "            #self.optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate,momentum=momentum,use_nesterov=True).minimize(self.loss)\n",
    "            #Above is simple optimizer without gradient clipping\n",
    "            \n",
    "            #Below is code with gradient clipping\n",
    "            optimizer_ = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "            gradients, variables = zip(*optimizer_.compute_gradients(self.loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n",
    "            self.train_step_ = optimizer_.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(ids, labels, batch_size=100, Trainable=False):\n",
    "            #ids is input, X_train\n",
    "            #shuffle between epochs while training only\n",
    "            \n",
    "            n_batches = len(ids)//batch_size\n",
    "            ids, labels = ids[:n_batches*batch_size], labels[:n_batches*batch_size]\n",
    "            if Trainable:\n",
    "                shuffle = np.random.permutation(np.arange(n_batches*batch_size))\n",
    "                ids, labels = ids[shuffle], labels[shuffle]\n",
    "   \n",
    "            for ii in range(0, len(ids), batch_size):\n",
    "                yield ids[ii:ii+batch_size], labels[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Actual training loop:\n",
    "\n",
    "def train_cnn(key, size=train_data_size):\n",
    "     \n",
    "    x_train = dict_train_ids[key]\n",
    "    y_train = dict_train_y[key]\n",
    "    x_dev = dict_dev_ids[key]\n",
    "    y_dev = dict_dev_ypred[key]\n",
    "    V = dict_vocab_len[key]\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "        \n",
    "            cnn = TextRNNwithA(sequence_length=x_train.shape[1], lstm_size = lstm_size, num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('completed cnn creation')\n",
    "\n",
    "#             # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "#             size_folder =  \"size_\" + str(size) \n",
    "#             out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key, size_folder))\n",
    "#             #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key))\n",
    "#             checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "#             model_name = key \n",
    "#             checkpoint_prefix = os.path.join(checkpoint_dir, model_name  + \"_model\")\n",
    "#             if not os.path.exists(checkpoint_dir):\n",
    "#                 os.makedirs(checkpoint_dir)\n",
    "#             saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            # Write vocabulary\n",
    "            ## vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "            \n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                \n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=True), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob, cnn.seq_length: np.count_nonzero(x,axis = 1)}\n",
    "                    # _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.train_step_,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    #avg_auc = total_auc/(batch_size*(len(x_train)//batch_size))\n",
    "                   # print(\"Train epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    \n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0, cnn.seq_length: np.count_nonzero(x,axis = 1)}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                    \n",
    "                    print('y_dev.shape',y_dev.shape)\n",
    "                    print('y_shuffled.shape',y_shuffled.shape)\n",
    "                    \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    f1_pos = f1_score(y_shuffled, y_pred, average = None)[1]\n",
    "                    f1_neg = f1_score(y_shuffled, y_pred, average = None)[0]\n",
    "                    f1_avg = f1_score(y_shuffled, y_pred, average = 'macro')\n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\t\\tDev epoch {}, avg loss {:g}, accuracy {:g}, auc {:g}, F1 pos {:g}, F1 neg {:g}, F1 avg {:g}\"\n",
    "                          .format(e, avg_loss, avg_acc, roc_auc, f1_pos, f1_neg, f1_avg))\n",
    "\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                                        \n",
    "        # Save model weights for future use.       \n",
    "            #save_path = saver.save(sess, checkpoint_prefix, global_step=20,write_meta_graph=False)\n",
    "#             save_path = saver.save(sess, checkpoint_prefix)\n",
    "#             print(\"Saved model\", model_name, save_path)\n",
    "            \n",
    "            #calculate predictions and prediction probability    \n",
    "#             feed_dict={cnn.input_x:x_dev, cnn.input_y: y_dev, cnn.dropout_keep_prob: 1.0}\n",
    "#             y_pred, y_pred_proba = sess.run([cnn.predictions, cnn.pred_proba],feed_dict)\n",
    "            #print(y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys 10000\n",
      "embedded_chars (?, 150, 50)\n",
      "output from rnn (?, 150, 128)\n",
      "Shapes of attention parameters w,b,u (128, 50) (50,) (50,)\n",
      "shape of attention output (?, 128)\n",
      "attention_output (?, 128)\n",
      "self.scores (?, 2)\n",
      "self.predictions (?,)\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.354425, average accuracy 0.864984,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 0, avg loss 0.227731, accuracy 0.912704, auc 0.925208, F1 pos 0.949834, F1 neg 0.664052, F1 avg 0.806943\n",
      "\t\t\t\t    Time taken for 0 epochs =  74.45206379890442\n",
      "Train epoch 1, average loss 0.163856, average accuracy 0.935897,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 1, avg loss 0.204518, accuracy 0.918818, auc 0.938052, F1 pos 0.952795, F1 neg 0.710303, F1 avg 0.831549\n",
      "Train epoch 2, average loss 0.108579, average accuracy 0.958133,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 2, avg loss 0.22698, accuracy 0.920856, auc 0.93416, F1 pos 0.954052, F1 neg 0.71481, F1 avg 0.834431\n",
      "Train epoch 3, average loss 0.0691244, average accuracy 0.974659,\n",
      "y_dev.shape (3000,)\n",
      "y_shuffled.shape (2944,)\n",
      "\t\tDev epoch 3, avg loss 0.289859, accuracy 0.920516, auc 0.92854, F1 pos 0.954492, F1 neg 0.686327, F1 avg 0.82041\n"
     ]
    }
   ],
   "source": [
    "#Create and train the rnn models for all domains in list_df\n",
    "#Pass the size to save the model name with size in different folders\n",
    "\n",
    "num_epochs = 12\n",
    "list_df = ['toys']\n",
    "size_train = train_data_size\n",
    "for key in list_df:\n",
    "    print(key, size_train)\n",
    "    train_cnn(key, size=size_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations : \n",
    "\n",
    "- Dropping sequence length in the multi_cnn_cell function immediately led to an improvement in the trainability of the RNN ie train loss did start dropping for train and dev, and finally dev accuracy improved all the way to 90%\n",
    "\n",
    "- Adding gradient clipping does make a difference and enables the LSTM to train better.\n",
    "- Increasing min_df from 2 to 5 allowed accuracy to improve a little bit more (0.5-1%) on a 10000 sample.\n",
    "\n",
    "### Output of some runs\n",
    "\n",
    "sample size = 50000. \n",
    "lstm_size = 50\n",
    "embed_size = 50\n",
    "min_df = 5\n",
    "\n",
    "Train epoch 6, average loss 0.0821299, average accuracy 0.971875,\n",
    "y_dev.shape (15000,)\n",
    "y_shuffled.shape (14976,)\n",
    "\t\tDev epoch 6, average loss 0.218732, average accuracy 0.931223, auc 0.950772, F1 pos 0.959734, F1 neg 0.76441, F1 avg 0.862072\n",
    "\n",
    "#### sample size = 50000, lstm_size = 128, embed_size = 50, min_df = 5\n",
    "        Train epoch 1, average loss 0.373705, average accuracy 0.859776,\n",
    "y_dev.shape (15000,)\n",
    "y_shuffled.shape (14976,)\n",
    "\t\tDev epoch 1, average loss 0.227598, average accuracy 0.911525, auc 0.927889, F1 pos 0.948578, F1 neg 0.683393, F1 avg 0.815985\n",
    "Train epoch 2, average loss 0.179789, average accuracy 0.93095,\n",
    "y_dev.shape (15000,)\n",
    "y_shuffled.shape (14976,)\n",
    "\t\tDev epoch 2, average loss 0.185112, average accuracy 0.925414, auc 0.953655, F1 pos 0.956379, F1 neg 0.742923, F1 avg 0.849651\n",
    "Train epoch 3, average loss 0.130883, average accuracy 0.951282,\n",
    "y_dev.shape (15000,)\n",
    "y_shuffled.shape (14976,)\n",
    "\t\tDev epoch 3, average loss 0.178948, average accuracy 0.931757, auc 0.958147, F1 pos 0.96015, F1 neg 0.762657, F1 avg 0.861403\n",
    "Train epoch 4, average loss 0.105596, average accuracy 0.96274,\n",
    "y_dev.shape (15000,)\n",
    "y_shuffled.shape (14976,)\n",
    "\t\tDev epoch 4, average loss 0.189158, average accuracy 0.930155, auc 0.953904, F1 pos 0.959277, F1 neg 0.754805, F1 avg 0.857041\n",
    "\t\t\t\t    Time taken for 4 epochs =  1568.20579123497\n",
    "Train epoch 5, average loss 0.0880419, average accuracy 0.96899,\n",
    "y_dev.shape (15000,)\n",
    "y_shuffled.shape (14976,)\n",
    "\t\tDev epoch 5, average loss 0.197142, average accuracy 0.933226, auc 0.953946, F1 pos 0.961147, F1 neg 0.762696, F1 avg 0.861921\n",
    "Train epoch 6, average loss 0.0776873, average accuracy 0.972837,\n",
    "y_dev.shape (15000,)\n",
    "y_shuffled.shape (14976,)\n",
    "\t\tDev epoch 6, average loss 0.203607, average accuracy 0.929754, auc 0.951635, F1 pos 0.958583, F1 neg 0.768893, F1 avg 0.863738\n",
    "Train epoch 7, average loss 0.0645518, average accuracy 0.977484,\n",
    "y_dev.shape (15000,)\n",
    "y_shuffled.shape (14976,)\n",
    "\t\tDev epoch 7, average loss 0.223198, average accuracy 0.931824, auc 0.947381, F1 pos 0.960128, F1 neg 0.765017, F1 avg 0.862573\n",
    "\n",
    "#### Sample size = 100000,  lstm_size = 128, embed_size = 50, min_df = 5\n",
    "        Train epoch 0, average loss 0.401195, average accuracy 0.852643,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 0, average loss 0.238714, average accuracy 0.906083, auc 0.919337, F1 pos 0.946422, F1 neg 0.619916, F1 avg 0.783169\n",
    "\t\t\t\t    Time taken for 0 epochs =  624.3892278671265\n",
    "Train epoch 1, average loss 0.183042, average accuracy 0.929728,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 1, average loss 0.166323, average accuracy 0.935029, auc 0.964439, F1 pos 0.961843, F1 neg 0.781447, F1 avg 0.871645\n",
    "Train epoch 2, average loss 0.131615, average accuracy 0.950824,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 2, average loss 0.160972, average accuracy 0.94184, auc 0.967205, F1 pos 0.966092, F1 neg 0.79578, F1 avg 0.880936\n",
    "Train epoch 3, average loss 0.111825, average accuracy 0.958187,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 3, average loss 0.160572, average accuracy 0.94194, auc 0.967076, F1 pos 0.966289, F1 neg 0.79096, F1 avg 0.878625\n",
    "Train epoch 4, average loss 0.0959312, average accuracy 0.964889,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 4, average loss 0.166863, average accuracy 0.939336, auc 0.966227, F1 pos 0.964291, F1 neg 0.798581, F1 avg 0.881436\n",
    "\t\t\t\t    Time taken for 4 epochs =  3135.383953809738\n",
    "Train epoch 5, average loss 0.0864601, average accuracy 0.96869,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 5, average loss 0.165132, average accuracy 0.938401, auc 0.966546, F1 pos 0.96383, F1 neg 0.79258, F1 avg 0.878205\n",
    "Train epoch 6, average loss 0.0805708, average accuracy 0.971081,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 6, average loss 0.179582, average accuracy 0.939236, auc 0.961596, F1 pos 0.964761, F1 neg 0.779554, F1 avg 0.872158\n",
    "Train epoch 7, average loss 0.0728268, average accuracy 0.974202,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 7, average loss 0.186312, average accuracy 0.939937, auc 0.963728, F1 pos 0.964908, F1 neg 0.791758, F1 avg 0.878333\n",
    "Train epoch 8, average loss 0.0687276, average accuracy 0.975792,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 8, average loss 0.186776, average accuracy 0.940271, auc 0.96068, F1 pos 0.965282, F1 neg 0.786388, F1 avg 0.875835\n",
    "\t\t\t\t    Time taken for 8 epochs =  5658.2309448719025\n",
    "Train epoch 9, average loss 0.0643795, average accuracy 0.977153,\n",
    "y_dev.shape (30000,)\n",
    "y_shuffled.shape (29952,)\n",
    "\t\tDev epoch 9, average loss 0.192174, average accuracy 0.938535, auc 0.958901, F1 pos 0.964034, F1 neg 0.788803, F1 avg 0.876419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is old code to build an LSTM from Anamika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "#batch_size = 50\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "embed_size = hands.shape[1]\n",
    "print(embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(gl_embed=hands.W,\n",
    "              embed_size=embed_size,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              lstm_size=lstm_size,\n",
    "              lstm_layers=lstm_layers):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #n_words = len(vocabulary_to_int)\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs_ = tf.placeholder(tf.int32,[None, None],name='inputs_')\n",
    "    with tf.name_scope('labels'):\n",
    "        labels_ = tf.placeholder(tf.int32,[None, None],name='labels_')\n",
    "    with tf.name_scope('keep_prob'):    \n",
    "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "        \n",
    "    with tf.name_scope('embedding'):\n",
    "#         embedding = tf.Variable(tf.random_normal((n_words,embed_size),-1,1),name='embedding_')\n",
    "#         embed = tf.nn.embedding_lookup(embedding,inputs_)\n",
    "        embedding=tf.get_variable(name=\"embedding_\",shape=gl_embed.shape,\n",
    "                                       initializer=tf.constant_initializer(gl_embed),trainable=False)\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "        \n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "\n",
    "        # Stack up multiple LSTM layers, for deep learning\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop]*lstm_layers)\n",
    "        \n",
    "        with tf.name_scope(\"RNN_init_state\"):\n",
    "            # Getting an initial state of all zeros\n",
    "            initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, \n",
    "                                                        activation_fn=tf.sigmoid,\n",
    "                                                        weights_initializer=\n",
    "                                                        tf.truncated_normal_initializer(stddev=0.1))   \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs_', 'labels_','initial_state', 'final_state',\n",
    "                    'keep_prob', 'cell', 'cost', 'predictions', 'optimizer',\n",
    "                    'accuracy','merged']\n",
    "    \n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    \n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = build_rnn(gl_embed=hands.W,\n",
    "              embed_size=embed_size,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              lstm_size=lstm_size,\n",
    "              lstm_layers=lstm_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('output/logs/1', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, epoch,train_writer,test_writer):\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        iteration = 1\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(model.initial_state)\n",
    "            start_time  = time.time()\n",
    "\n",
    "            for ii, (x, y) in enumerate(batch_iterator(train_ids, y_train, batch_size), 1):\n",
    "                \n",
    "                feed = {model.inputs_: x,\n",
    "                        model.labels_: y[:, None],\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state: state}\n",
    "                summary,loss, state, _ = sess.run([model.merged,model.cost, \n",
    "                                                   model.final_state, \n",
    "                                                   model.optimizer], feed_dict=feed)             \n",
    "\n",
    "                if iteration%100==0:\n",
    "                    print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Train loss: {:.3f}\".format(loss))\n",
    "                    train_writer.add_summary(summary, iteration)\n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(model.cell.zero_state(batch_size, tf.float32))\n",
    "                    \n",
    "                    for x, y in batch_iterator(dev_ids, y_dev, batch_size):\n",
    "                        feed = {model.inputs_: x,\n",
    "                                model.labels_: y[:, None],\n",
    "                                model.keep_prob: 1,\n",
    "                                model.initial_state: val_state}\n",
    "                        summary, dev_loss,batch_acc, val_state = sess.run([model.merged, model.cost,model.accuracy, \n",
    "                                                         model.final_state], feed_dict=feed)\n",
    "                        #print('batch_acc', batch_acc)\n",
    "                        val_acc.append(batch_acc)\n",
    "                        \n",
    "                    end_time  = time.time()\n",
    "\n",
    "                    test_writer.add_summary(summary,iteration)\n",
    "                    print(\"Dev loss: {:.3f}\".format(dev_loss))\n",
    "                    print(\"Dev acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "                    print(\"time = {:.3f}\".format(end_time-start_time))\n",
    "\n",
    "                iteration +=1\n",
    "        saver.save(sess, \"output/checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-fbd545e0ddd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ids' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size_options = [256]\n",
    "lstm_layers_options = [1]\n",
    "learning_rate_options = [0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm size: 256 nb layers : 1 learn rate : 0.001\n",
      "Epoch: 1/20 Iteration: 100 Train loss: 0.205\n",
      "Dev loss: 0.137\n",
      "Dev acc: 0.829\n",
      "time = 87.731\n",
      "Epoch: 2/20 Iteration: 200 Train loss: 0.143\n",
      "Dev loss: 0.138\n",
      "Dev acc: 0.829\n",
      "time = 150.346\n",
      "Epoch: 3/20 Iteration: 300 Train loss: 0.166\n",
      "Dev loss: 0.137\n",
      "Dev acc: 0.829\n",
      "time = 213.591\n",
      "Epoch: 5/20 Iteration: 400 Train loss: 0.157\n",
      "Dev loss: 0.137\n",
      "Dev acc: 0.829\n",
      "time = 54.165\n",
      "Epoch: 6/20 Iteration: 500 Train loss: 0.148\n",
      "Dev loss: 0.137\n",
      "Dev acc: 0.829\n",
      "time = 115.999\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "for lstm_size in lstm_size_options:\n",
    "    for lstm_layers in lstm_layers_options:\n",
    "        for learning_rate in learning_rate_options:\n",
    "            log_string_train = 'output/logs/2/train/lr={},rl={},ru={}'.format(learning_rate, lstm_layers, lstm_size)\n",
    "            log_string_test = 'output/logs/2/test/lr={},rl={},ru={}'.format(learning_rate, lstm_layers, lstm_size)\n",
    "            train_writer = tf.summary.FileWriter(log_string_train)\n",
    "            test_writer = tf.summary.FileWriter(log_string_test)\n",
    "            \n",
    "            print(\"lstm size: {}\".format(lstm_size),\n",
    "                    \"nb layers : {}\".format(lstm_layers),\n",
    "                    \"learn rate : {:.3f}\".format(learning_rate))\n",
    "            \n",
    "            model = build_rnn(gl_embed=hands.W,\n",
    "                      embed_size=embed_size,\n",
    "                      batch_size=batch_size,\n",
    "                      learning_rate=learning_rate,\n",
    "                      lstm_size=lstm_size,\n",
    "                      lstm_layers=lstm_layers)\n",
    "\n",
    "            train(model, epochs, train_writer,test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print(\"run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
