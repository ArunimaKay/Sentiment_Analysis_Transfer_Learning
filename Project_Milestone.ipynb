{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "(motivation for your work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "As a primary source of inspiration, we have used the PhD Thesis by Robert Remus titled Genre and Domain dependencies in Sentiment Analysis. Remus in the thesis examines the effect of genre and domain in various tasks related to sentiment analysis, one of which is sentiment polarity detection. He uses the concept of domain similarity. While various metrics for similarity are mentioned, the one we have used is JS Divergence to determine the similarity / divergence of word distributions of two domains. He also leverages JS Divergence to select a subset of instances from the source domain for creating a model for domain adaptation to the target domain. In his thesis, he uses instance selection(IS) to pick a subset from the source domain that is similar to the target domain. He shows that IS from source, and from source + target (9:1 ratio) is more effective than source only.\n",
    "\n",
    "A more recent paper that looks at the problem of domain adaptation for sentiment learning is Domain Adaptation for Large Scale Sentiment Classification : A Deep Learning Approach, but Glorot, Bordes, Bengio. The paper uses an unsupervised deep learning approach (specifically de-ionized auto-encoders) to extract a meaningful feature representation of each review in an unsupervised fashion. They then train a sentiment classifier (SVM) on the features extracted with this auto-encoder. They find that sentiment classifiers trained with this approach outperform state-of-the-art methods in domain adaptation for sentiment classification on a benchmark of Amazon reviews composed of 4 types of Amazon products (the benchmark was created by Blitzer in 2007). While their work shows a substantial reduction in loss in transfer from source to target domain, there is still a loss. \n",
    "\n",
    "We would like to focus on ways to get to the same accuracy as on the target domain model only, by enhancing the source domain with a few examples from the target domain. We want to a) detemine how many examples are needed from the target domain when they are selected randomly b) whether we can reduce the number of examples needed by applying heuristics to pick the examples from the target domain that are least represented in the source domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods \n",
    "We want to divide the project into 3 parts, the 3rd is a stretch goal. depending on time and challenge of the tasks.\n",
    " \n",
    "Part 1 : How well do different machine learning models transfer across domains ? We will evaluate 2-3 machine learning techniques (Naïve Bayes, SVM, and Neural Networks) and determine which transfers the best. We will start with pretrained GLOVE embeddings and explore the performance with or without training it further. (Our objective  is not to find the best model on the source / target domain by itself, but rather the model that lends best to transferability from  source to target).\n",
    "\n",
    "Measure: Comparative Accuracy of predicting rating from reviews when using one domain versus another. This will define the baseline for transfer learning for part 2.\n",
    "\n",
    "Part 2: Main part of the project. We will examine and develop techniques to improve transfer learning from one domain to another. \n",
    "\n",
    "Examples include:\n",
    "\n",
    "a. Using domain similarity (source, target domain) to pick the source domain for transfer learning.     (Ref: 2) We will use  JS Divergence as the metric for domain similarity / difference.\n",
    "\n",
    "b. Evaluate whether adding a small number of labeled target instances can help us get to desired accuracy in target domain.\n",
    "\n",
    "1st Metric:  Nts/Nt where\n",
    "\n",
    "Nts = # labeled instances from target domain needed when starting from a model built on a source domain, to get to similar accuracy as a model built entirely on the target domain.\n",
    "\n",
    "Nt = ~ minimum number of labeled instances from target domain to reach maximum possible accuracy when building a model directly on the target domain only.\n",
    "              \t\n",
    "2nd Metric: - Tts/Tt where \n",
    "\n",
    "Tts = Epochs needed to get to similar accuracy when starting with a model trained on source             domain\n",
    "\n",
    " Tt = Minimum Epochs needed to reach maximum accuracy with a model trained on the target domain         only.\n",
    " \n",
    "c. Try to determine which examples from target domain would be most helpful in improving accuracy of the model (eg those most different from the source domain)\n",
    "Sample methods to pick examples from target domain : \n",
    "- Instances in the target domain with the most new vocabulary words vs the source domain.\n",
    "- Target domain instances with the highest JS Divergence vs the source domain.\n",
    "Metric:  Same as in 2b, and vs the result in 2b.\n",
    "\n",
    "d.  Stretch goal: If time permits, and we develop sufficient expertise to write the algorithms, we will attempt using Attention based neural networks (Ref: 11), or Auto-Encoders (Ref : 8) to see if they help improve accuracy of transfer learning, and how much fewer training instances are needed with those models.\n",
    "    \n",
    "### Details on JS Divergence :\n",
    "JS Divergence is similar to KL Divergence, but better suited to comparing domains with different vocabularies.\n",
    "\n",
    "KL Divergence = $$ D_{KL}(P\\ ||\\ Q) = \\sum_{x} P(x) \\log_2 {P(x)/{Q(x)}} $$\n",
    "\n",
    "JS Divergence = $$ D_{JS}(P\\ ||\\ Q) = 1/2 * (D_{KL}(P\\ ||\\ M) + D_{KL}(Q\\ ||\\ M)) $$\n",
    "\n",
    "where M = 1/2(Q+R)\n",
    "\n",
    "JS Divergence has the benefit that it is defined even when Q(x) is 0 for a given word, whereaas KL Divergence is not.\n",
    "\n",
    "Part 3: Stretch goal\n",
    "If time permits, we would like to also examine the effectiveness of transfer learning with different tasks within sentiment analysis. Specifically, we would like to see how transfer learning works in Aspect Based Sentiment Analysis. For this, we would be using the data from SemEval 2016, and a CNN / RNN to predict the aspects and associated sentiments.\n",
    "\n",
    "Data: Labeled data for fine grained sentiment analysis in two categories in English : restaurants, and laptops. Paper describing the data(Ref 3). Sample analysis paper (Ref 6)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and discussion \n",
    "(for your baseline model, though feel free to include material for anything else you’ve done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps section for work you plan to do before submitting the final version\n",
    "(you’ll remove this section and replace it with your conclusions, final results and analysis in your final report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Book : Sentiment Analysis and Opinion Mining, Bing Liu.\n",
    "\n",
    "2.  Genre and Domain Dependencies in Sentiment Analysis (PhD Thesis)\n",
    "http://www.qucosa.de/fileadmin/data/qucosa/documents/16543/dissertation_rremus_angenommen_20150423.pdf\n",
    "\n",
    "3.  SemEval 2016, Task 5, paper describing the data: https://www.researchgate.net/publication/305334494_SemEval-2016_Task_5_Aspect_Based_Sentiment_Analysis)\n",
    "\n",
    "4. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering\n",
    "R. He, J. McAuley\n",
    "WWW, 2016\n",
    "Pdf\n",
    "\n",
    "5. Image-based recommendations on styles and substitutes\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "SIGIR, 2015\n",
    "pdf\n",
    "6.  INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis. https://arxiv.org/abs/1609.02748v2\n",
    "7.  How Transferable are Neural Networks in NLP Applications? https://arxiv.org/pdf/1603.06111.pdf\n",
    "8. Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach http://www.icml-2011.org/papers/342_icmlpaper.pdf\n",
    "9. Frustratingly Easy Domain Adaptation: Hal Daum´e III. www.umiacs.umd.edu/~hal/docs/daume07easyadapt.pdf\n",
    "10. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification  http://john.blitzer.com/papers/sentiment_domain.pdf\n",
    "11. ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs http://www.aclweb.org/anthology/Q16-1019\n",
    "12. Learning Attitudes and Attributes from Multi-Aspect Reviews http://i.stanford.edu/~julian/pdfs/icdm2012.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
