{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import time\n",
    "# Install a few python packages using pip\n",
    "from common import utils\n",
    "utils.require_package('nltk')\n",
    "utils.require_package(\"wget\")      # for fetching dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard python helper libraries.\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys, time\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "\n",
    "# NLTK is the Natural Language Toolkit, and contains several language datasets\n",
    "# as well as implementations of many popular NLP algorithms.\n",
    "# HINT: You should look at what is available here when thinking about your project!\n",
    "import nltk\n",
    "\n",
    "# Helper libraries (see the corresponding py files in this notebook's directory).\n",
    "from common import utils, vocabulary\n",
    "import segment\n",
    "\n",
    "utils.require_package(\"tqdm\")  # for nice progress bars\n",
    "from tqdm import tqdm as ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 129.83230781555176\n",
      "end getDF\n",
      "time taken to load data =  129.83265256881714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def parse(path):\n",
    "  print('start parse')\n",
    "  start_parse = time.time()\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "  end_parse = time.time()\n",
    "  print('end parse with time for parse',end_parse - start_parse)\n",
    "\n",
    "def getDF(path):\n",
    "  print('start getDF')\n",
    "  start = time.time()\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  print('end getDF')\n",
    "  end = time.time()\n",
    "  print('time taken to load data = ',end-start)\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "df = getDF('reviews_Toys_and_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2252771, 9)\n",
      "Index(['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',\n",
      "       'overall', 'summary', 'unixReviewTime', 'reviewTime'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMEVO2LY6VEJA</td>\n",
       "      <td>0000191639</td>\n",
       "      <td>Nicole Soeder</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Great product, thank you! Our son loved the pu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Puzzles</td>\n",
       "      <td>1388016000</td>\n",
       "      <td>12 26, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3C9CSW3TJITGT</td>\n",
       "      <td>0005069491</td>\n",
       "      <td>Renee</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I love these felt nursery rhyme characters and...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Charming characters but busy work required</td>\n",
       "      <td>1377561600</td>\n",
       "      <td>08 27, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A31POTIYCKSZ9G</td>\n",
       "      <td>0076561046</td>\n",
       "      <td>So CA Teacher</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I see no directions for its use. Therefore I h...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No directions for use...</td>\n",
       "      <td>1404864000</td>\n",
       "      <td>07 9, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2GGHHME9B6W4O</td>\n",
       "      <td>0131358936</td>\n",
       "      <td>Dalilah G.</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a great tool for any teacher using the...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great CD-ROM</td>\n",
       "      <td>1382400000</td>\n",
       "      <td>10 22, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1FSLDH43ORWZP</td>\n",
       "      <td>0133642984</td>\n",
       "      <td>Dayna English</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Although not as streamlined as the Algebra I m...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Algebra II -- presentation materials</td>\n",
       "      <td>1374278400</td>\n",
       "      <td>07 20, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A3NXHTSEBX9YHB</td>\n",
       "      <td>0279515766</td>\n",
       "      <td>marlenetbueras</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>i am glad to get it after 25 yr of waiting for...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>my strawberry shortcake</td>\n",
       "      <td>1363564800</td>\n",
       "      <td>03 18, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AYVR1MQCTNU5D</td>\n",
       "      <td>0375829695</td>\n",
       "      <td>annie</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>What a great theme for a puzzle book. My daugh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>So cute!!</td>\n",
       "      <td>1291939200</td>\n",
       "      <td>12 10, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A3CJHKFHHQJP2K</td>\n",
       "      <td>0375829695</td>\n",
       "      <td>Beth Sharo \"bookmom\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>My son got this book for his birthday.  He lov...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Disappointing Puzzle Book</td>\n",
       "      <td>1297209600</td>\n",
       "      <td>02 9, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A3638FINP26E8N</td>\n",
       "      <td>0375829695</td>\n",
       "      <td>C. Boykin</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Love the book format. My 4yr old grandson love...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>frustrating puzzle pieces</td>\n",
       "      <td>1282521600</td>\n",
       "      <td>08 23, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUT7723O49VMN</td>\n",
       "      <td>0375829695</td>\n",
       "      <td>Cindy Lindy</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>I'm a 2nd grade teacher.  My students found th...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>challenging and fun</td>\n",
       "      <td>1237766400</td>\n",
       "      <td>03 23, 2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin          reviewerName helpful  \\\n",
       "0   AMEVO2LY6VEJA  0000191639         Nicole Soeder  [0, 0]   \n",
       "1  A3C9CSW3TJITGT  0005069491                 Renee  [0, 0]   \n",
       "2  A31POTIYCKSZ9G  0076561046         So CA Teacher  [0, 0]   \n",
       "3  A2GGHHME9B6W4O  0131358936            Dalilah G.  [0, 0]   \n",
       "4  A1FSLDH43ORWZP  0133642984         Dayna English  [0, 0]   \n",
       "5  A3NXHTSEBX9YHB  0279515766        marlenetbueras  [0, 0]   \n",
       "6   AYVR1MQCTNU5D  0375829695                 annie  [0, 0]   \n",
       "7  A3CJHKFHHQJP2K  0375829695  Beth Sharo \"bookmom\"  [0, 0]   \n",
       "8  A3638FINP26E8N  0375829695             C. Boykin  [1, 1]   \n",
       "9   AUT7723O49VMN  0375829695           Cindy Lindy  [1, 1]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  Great product, thank you! Our son loved the pu...      5.0   \n",
       "1  I love these felt nursery rhyme characters and...      4.0   \n",
       "2  I see no directions for its use. Therefore I h...      3.0   \n",
       "3  This is a great tool for any teacher using the...      5.0   \n",
       "4  Although not as streamlined as the Algebra I m...      5.0   \n",
       "5  i am glad to get it after 25 yr of waiting for...      5.0   \n",
       "6  What a great theme for a puzzle book. My daugh...      5.0   \n",
       "7  My son got this book for his birthday.  He lov...      1.0   \n",
       "8  Love the book format. My 4yr old grandson love...      3.0   \n",
       "9  I'm a 2nd grade teacher.  My students found th...      4.0   \n",
       "\n",
       "                                      summary  unixReviewTime   reviewTime  \n",
       "0                                     Puzzles      1388016000  12 26, 2013  \n",
       "1  Charming characters but busy work required      1377561600  08 27, 2013  \n",
       "2                    No directions for use...      1404864000   07 9, 2014  \n",
       "3                                Great CD-ROM      1382400000  10 22, 2013  \n",
       "4        Algebra II -- presentation materials      1374278400  07 20, 2013  \n",
       "5                     my strawberry shortcake      1363564800  03 18, 2013  \n",
       "6                                   So cute!!      1291939200  12 10, 2010  \n",
       "7                   Disappointing Puzzle Book      1297209600   02 9, 2011  \n",
       "8                   frustrating puzzle pieces      1282521600  08 23, 2010  \n",
       "9                         challenging and fun      1237766400  03 23, 2009  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         reviewerID     asin  reviewerName  helpful  reviewText  summary  \\\n",
      "overall                                                                    \n",
      "1.0          192993   192993        192435   192993      192993   192993   \n",
      "2.0          115801   115801        115416   115801      115801   115801   \n",
      "3.0          193941   193941        193195   193941      193941   193941   \n",
      "4.0          407884   407884        406255   407884      407884   407884   \n",
      "5.0         1342152  1342152       1333623  1342152     1342152  1342152   \n",
      "\n",
      "         unixReviewTime  reviewTime  \n",
      "overall                              \n",
      "1.0              192993      192993  \n",
      "2.0              115801      115801  \n",
      "3.0              193941      193941  \n",
      "4.0              407884      407884  \n",
      "5.0             1342152     1342152  \n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('overall').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMEVO2LY6VEJA\n",
      "Great product, thank you! Our son loved the puzzles.  They have large pieces yet they are still challenging for a 4 year old.\n",
      "A3C9CSW3TJITGT\n",
      "I love these felt nursery rhyme characters and scenes.  The quality of the felt is good, and the illustrations are detailed and pretty.  As noted, the figures and scenes are printed on 2 large sheets of flannel and each individual item needs to be cut out.  This process took me 2 hours of tiny cutting.  To me it does not lend itself to a book form but rather laying out the scenes separately or for use on a flannel board.  However, I love the quiet play it offers for my toddler, and as a former Kindergarten teacher, I understand the value of learning rhyme and its connection to future reading.  Overall, delightful product with some work involved.\n",
      "A31POTIYCKSZ9G\n",
      "I see no directions for its use. Therefore I have to make up the games, unfortunately.\n",
      "A2GGHHME9B6W4O\n",
      "This is a great tool for any teacher using the Pre Calculus Enhanced with Graphing Utilities book (of any edition). Easy to use with a very extensive test bank. Excellent!\n",
      "A1FSLDH43ORWZP\n",
      "Although not as streamlined as the Algebra I materials .. this is extremely helpful for first time teachers ...bulk of materials are prepared for presentations.\n",
      "A3NXHTSEBX9YHB\n",
      "i am glad to get it after 25 yr of waiting for her i ended up with 4 of her\n",
      "AYVR1MQCTNU5D\n",
      "What a great theme for a puzzle book. My daughter loves both Dr. Seuss AND puzzle books, so this was a hit!\n",
      "A3CJHKFHHQJP2K\n",
      "My son got this book for his birthday.  He loves puzzles and Dr. Seuss and I felt that this would be a winning combination.  The pieces are all the same shape and can easily be put together in the wrong place.  I worked with him to complete the puzzle and found it frustrating as an adult.  I would not recommend this book to anyone.\n",
      "A3638FINP26E8N\n",
      "Love the book format. My 4yr old grandson loves puzzles and Dr. Seuss was an added plus. BUT several of the pieces are cut the same, causing unnecessary confusion and frustration for both of us.\n",
      "AUT7723O49VMN\n",
      "I'm a 2nd grade teacher.  My students found the jigsaw puzzle book challenging and fun.  We ended up cutting it up to allow more to use the puzzles at one time.  A great activity for children when they have finished their work and or when a class is celebrating Dr. Seuss' Birthday!\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df['reviewerID'].iloc[i])\n",
    "    print(df['reviewText'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1351662, 9) (450554, 9) (450555, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_toys,devtest = train_test_split(df, test_size=0.4)\n",
    "dev_toys,test_toys = train_test_split(devtest,test_size = 0.5)\n",
    "print(train_toys.shape,dev_toys.shape,test_toys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arunima/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews with ratings != 3 in train set 1235735\n",
      "Number of reviews with ratings != 3 in dev set 411501\n",
      "(1200000, 9)\n",
      "         reviewerID    asin  reviewerName  helpful  reviewText  summary  \\\n",
      "overall                                                                   \n",
      "1.0          112494  112494        112179   112494      112494   112494   \n",
      "2.0           67561   67561         67331    67561       67561    67561   \n",
      "4.0          237627  237627        236669   237627      237627   237627   \n",
      "5.0          782318  782318        777382   782318      782318   782318   \n",
      "\n",
      "         unixReviewTime  reviewTime  \n",
      "overall                              \n",
      "1.0              112494      112494  \n",
      "2.0               67561       67561  \n",
      "4.0              237627      237627  \n",
      "5.0              782318      782318  \n",
      "(360000, 9)\n",
      "         reviewerID    asin  reviewerName  helpful  reviewText  summary  \\\n",
      "overall                                                                   \n",
      "1.0           33967   33967         33864    33967       33967    33967   \n",
      "2.0           20152   20152         20090    20152       20152    20152   \n",
      "4.0           71547   71547         71264    71547       71547    71547   \n",
      "5.0          234334  234334        232858   234334      234334   234334   \n",
      "\n",
      "         unixReviewTime  reviewTime  \n",
      "overall                              \n",
      "1.0               33967       33967  \n",
      "2.0               20152       20152  \n",
      "4.0               71547       71547  \n",
      "5.0              234334      234334  \n",
      "             reviewerID        asin      reviewerName helpful  \\\n",
      "392707    A71CHKJIS7LH6  B000EULXA2    Lee S. Patillo  [0, 0]   \n",
      "1253479  A1ZGSL120D3TVZ  B003ZQ41Z6  JoAnn Pennington  [0, 0]   \n",
      "304445   A2F2IDS1PX8CET  B000931TNA                 K  [4, 4]   \n",
      "\n",
      "                                                reviewText  overall  \\\n",
      "392707   My 3 year old loves this toy. It is durable, e...      5.0   \n",
      "1253479  I got this for my 1 month old granddaughter.  ...      5.0   \n",
      "304445   Questions barely pertain to the Bible, only in...      1.0   \n",
      "\n",
      "                                                   summary  unixReviewTime  \\\n",
      "392707                                           toy train      1199232000   \n",
      "1253479                                          Adorable!      1355184000   \n",
      "304445   Disappointed!  It's a stretch that most of the...      1341532800   \n",
      "\n",
      "          reviewTime  \n",
      "392707    01 2, 2008  \n",
      "1253479  12 11, 2012  \n",
      "304445    07 6, 2012  \n"
     ]
    }
   ],
   "source": [
    "size_train = 1200000\n",
    "len_max_train = train_toys[train_toys.overall!=3].shape[0] #max possible length of train data set taking out the 3 ratings.\n",
    "print(\"Number of reviews with ratings != 3 in train set\",len_max_train)\n",
    "temp_size_train = min(len_max_train,size_train)\n",
    "\n",
    "len_max_dev = dev_toys[dev_toys.overall!=3].shape[0]\n",
    "print(\"Number of reviews with ratings != 3 in dev set\",len_max_dev)\n",
    "temp_size_dev = min(len_max_dev,int(0.3*temp_size_train)) #making the dev set about 0.3 times the train set.\n",
    "\n",
    "temp_train_toys = train_toys[train_toys.overall != 3][:temp_size_train]\n",
    "print(temp_train_toys.shape)\n",
    "print(temp_train_toys.groupby('overall').count())\n",
    "#print(temp_train_toys[:5])\n",
    "\n",
    "temp_dev_toys = dev_toys[dev_toys.overall!=3][:temp_size_dev]\n",
    "print(temp_dev_toys.shape)\n",
    "print(temp_dev_toys.groupby('overall').count())\n",
    "print(temp_dev_toys[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 10000\n",
      "done 20000\n",
      "done 30000\n",
      "done 40000\n",
      "done 50000\n",
      "done 60000\n",
      "done 70000\n",
      "done 80000\n",
      "done 90000\n",
      "done 100000\n",
      "done 110000\n",
      "done 120000\n",
      "done 130000\n",
      "done 140000\n",
      "done 150000\n",
      "done 160000\n",
      "done 170000\n",
      "done 180000\n",
      "done 190000\n",
      "done 200000\n",
      "done 210000\n",
      "done 220000\n",
      "done 230000\n",
      "done 240000\n",
      "done 250000\n",
      "done 260000\n",
      "done 270000\n",
      "done 280000\n",
      "done 290000\n",
      "done 300000\n",
      "done 310000\n",
      "done 320000\n",
      "done 330000\n",
      "done 340000\n",
      "done 350000\n",
      "done 360000\n",
      "done 370000\n",
      "done 380000\n",
      "done 390000\n",
      "done 400000\n",
      "done 410000\n",
      "done 420000\n",
      "done 430000\n",
      "done 440000\n",
      "done 450000\n",
      "done 460000\n",
      "done 470000\n",
      "done 480000\n",
      "done 490000\n",
      "done 500000\n",
      "done 510000\n",
      "done 520000\n",
      "done 530000\n",
      "done 540000\n",
      "done 550000\n",
      "done 560000\n",
      "done 570000\n",
      "done 580000\n",
      "done 590000\n",
      "done 600000\n",
      "done 610000\n",
      "done 620000\n",
      "done 630000\n",
      "done 640000\n",
      "done 650000\n",
      "done 660000\n",
      "done 670000\n",
      "done 680000\n",
      "done 690000\n",
      "done 700000\n",
      "done 710000\n",
      "done 720000\n",
      "done 730000\n",
      "done 740000\n",
      "done 750000\n",
      "done 760000\n",
      "done 770000\n",
      "done 780000\n",
      "done 790000\n",
      "done 800000\n",
      "done 810000\n",
      "done 820000\n",
      "done 830000\n",
      "done 840000\n",
      "done 850000\n",
      "done 860000\n",
      "done 870000\n",
      "done 880000\n",
      "done 890000\n",
      "done 900000\n",
      "done 910000\n",
      "done 920000\n",
      "done 930000\n",
      "done 940000\n",
      "done 950000\n",
      "done 960000\n",
      "done 970000\n",
      "done 980000\n",
      "done 990000\n",
      "done 1000000\n",
      "done 1010000\n",
      "done 1020000\n",
      "done 1030000\n",
      "done 1040000\n",
      "done 1050000\n",
      "done 1060000\n",
      "done 1070000\n",
      "done 1080000\n",
      "done 1090000\n",
      "done 1100000\n",
      "done 1110000\n",
      "done 1120000\n",
      "done 1130000\n",
      "done 1140000\n",
      "done 1150000\n",
      "done 1160000\n",
      "done 1170000\n",
      "done 1180000\n",
      "done 1190000\n",
      "total time taken to canonicalize -1114.5482907295227\n",
      "100 most common words in the dataset: [('.', 4243924), ('the', 3722460), (',', 2385705), ('and', 2383583), ('it', 2323741), ('a', 1981957), ('to', 1965490), ('i', 1849205), ('is', 1351503), ('this', 1320145), ('for', 1295128), ('of', 1158097), ('my', 947474), ('!', 869437), ('with', 829146), ('in', 753569), ('that', 720334), ('was', 681966), ('you', 566573), ('but', 555112), ('on', 548951), ('are', 524760), ('have', 501739), (\"'s\", 488938), ('DG', 480620), ('not', 476724), ('as', 469048), ('so', 449762), ('they', 442535), (\"n't\", 440228), ('we', 387411), ('he', 378990), ('very', 371460), ('one', 363083), ('great', 349800), ('be', 345730), ('all', 314045), (')', 312722), ('she', 308834), ('DGDG', 301623), ('them', 293315), ('(', 278587), ('would', 275337), ('just', 268937), ('can', 267137), ('old', 267101), ('at', 259299), ('like', 258842), ('has', 255424), ('game', 253997), ('toy', 249403), ('do', 245968), ('when', 241344), ('her', 235104), ('up', 234943), ('or', 229940), ('if', 228277), ('little', 226781), ('had', 226368), ('out', 224751), ('these', 219339), ('will', 218761), ('loves', 215988), ('fun', 213018), ('get', 206435), ('love', 204421), ('more', 200487), ('play', 199154), ('from', 198586), ('year', 196711), ('well', 193956), ('good', 193443), ('&', 190418), ('bought', 190261), ('really', 189709), ('time', 178572), ('son', 175632), (';', 175520), ('his', 172270), ('...', 171381), ('only', 168088), ('an', 159311), ('there', 158515), ('kids', 155510), ('got', 155086), ('about', 153217), ('other', 153009), ('were', 152553), ('daughter', 152173), ('than', 149905), ('set', 145775), ('much', 142614), ('our', 141481), ('also', 139924), ('your', 138866), ('did', 138542), ('some', 137892), ('what', 137420), ('does', 135574), ('because', 134918)]\n"
     ]
    }
   ],
   "source": [
    "cnt = collections.Counter()\n",
    "#print(ds.vocab.num_unigrams)\n",
    "x_tokens_list = []\n",
    "start = time.time()\n",
    "for i in range(temp_train_toys.shape[0]):\n",
    "#for i in range(5):\n",
    "    x_tokens = word_tokenize(temp_train_toys.reviewText.iloc[i])\n",
    "    #print(train_toys.reviewText.iloc[i],x_tokens)\n",
    "    #print(type(x_tokens),len(x_tokens))\n",
    "    x_tokens_canonical = utils.canonicalize_words(x_tokens)\n",
    "    #print(x_tokens_canonical,type(x_tokens_canonical),len(x_tokens_canonical))\n",
    "    x_tokens_list.append(x_tokens_canonical)\n",
    "    #print('list',x_tokens_list)\n",
    "    #print(x_tokens_list.shape)\n",
    "    for word in x_tokens_canonical:\n",
    "        cnt[word]+=1\n",
    "    if i%25000 == 0:\n",
    "        #print('x_tokens',x_tokens,'x_tokens_canonical',x_tokens_canonical)\n",
    "        print('done',i)\n",
    "end = time.time()\n",
    "print('total time taken to canonicalize',start-end)\n",
    "\n",
    "print('100 most common words in the dataset:',cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200000\n",
      "[['would', 'have', 'had', 'more', 'fun', 'with', 'him', 'if', 'more', 'people', 'knew', 'who', 'he', 'was', '.', 'they', 'will', 'learn', ',', 'when', 'the', 'stars', 'are', 'right', '!'], ['i', 'had', 'this', 'figure', 'as', 'a', 'kid', 'and', 'just', 'recently', 'i', 'decided', 'to', 'relive', 'some', 'of', 'my', 'childhood', 'and', 'found', 'one', 'on', 'here', 'for', 'a', 'good', 'price', '.', 'figure', 'is', 'just', 'like', 'i', 'remember', ',', 'simple', 'but', 'yet', 'really', 'awesome', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!']]\n",
      "['i', 'am']\n"
     ]
    }
   ],
   "source": [
    "print(len(x_tokens_list))\n",
    "print(x_tokens_list[:2])\n",
    "print(x_tokens_canonical[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 10000\n",
      "done 20000\n",
      "done 30000\n",
      "done 40000\n",
      "done 50000\n",
      "done 60000\n",
      "done 70000\n",
      "done 80000\n",
      "done 90000\n",
      "done 100000\n",
      "done 110000\n",
      "done 120000\n",
      "done 130000\n",
      "done 140000\n",
      "done 150000\n",
      "done 160000\n",
      "done 170000\n",
      "done 180000\n",
      "done 190000\n",
      "done 200000\n",
      "done 210000\n",
      "done 220000\n",
      "done 230000\n",
      "done 240000\n",
      "done 250000\n",
      "done 260000\n",
      "done 270000\n",
      "done 280000\n",
      "done 290000\n",
      "done 300000\n",
      "done 310000\n",
      "done 320000\n",
      "done 330000\n",
      "done 340000\n",
      "done 350000\n",
      "total time taken to canonicalize dev set -339.69965076446533\n",
      "100 most common words in the dataset: [('.', 5515451), ('the', 4837349), (',', 3103855), ('and', 3099236), ('it', 3019358), ('a', 2575808), ('to', 2552900), ('i', 2402632), ('is', 1757049), ('this', 1715868), ('for', 1683699), ('of', 1504924), ('my', 1231933), ('!', 1130799), ('with', 1077799), ('in', 978953), ('that', 935978), ('was', 885707), ('you', 737209), ('but', 721487), ('on', 713808), ('are', 683218), ('have', 651538), (\"'s\", 636324), ('DG', 625355), ('not', 619587), ('as', 611063), ('so', 583818), ('they', 575303), (\"n't\", 571841), ('we', 504222), ('he', 491747), ('very', 483348), ('one', 471937), ('great', 454262), ('be', 449318), ('all', 408473), (')', 405679), ('she', 401490), ('DGDG', 391092), ('them', 381278), ('(', 361572), ('would', 357256), ('just', 348920), ('can', 348020), ('old', 347236), ('at', 336834), ('like', 336179), ('has', 331586), ('game', 330565), ('toy', 324293), ('do', 319154), ('when', 313682), ('her', 305692), ('up', 305368), ('or', 298433), ('if', 296701), ('little', 295011), ('had', 293912), ('out', 291720), ('these', 285210), ('will', 284236), ('loves', 281368), ('fun', 277101), ('get', 268774), ('love', 265526), ('more', 261048), ('play', 258740), ('from', 258198), ('year', 255810), ('well', 252613), ('good', 251402), ('&', 248348), ('bought', 247179), ('really', 246751), ('time', 231746), (';', 228800), ('son', 228332), ('his', 223680), ('...', 222986), ('only', 218394), ('an', 206813), ('there', 205782), ('kids', 201945), ('got', 200979), ('other', 199049), ('about', 198959), ('daughter', 198599), ('were', 198139), ('than', 194508), ('set', 190058), ('much', 185136), ('our', 183641), ('also', 182296), ('your', 180965), ('did', 180028), ('some', 178892), ('what', 178379), ('does', 176511), ('because', 175192)]\n"
     ]
    }
   ],
   "source": [
    "cnt_dev = collections.Counter()\n",
    "#print(ds.vocab.num_unigrams)\n",
    "x_tokens_list_dev = []\n",
    "start = time.time()\n",
    "for i in range(temp_dev_toys.shape[0]):\n",
    "#for i in range(5):\n",
    "    x_tokens = word_tokenize(temp_dev_toys.reviewText.iloc[i])\n",
    "    #print(train_toys.reviewText.iloc[i],x_tokens)\n",
    "    #print(type(x_tokens),len(x_tokens))\n",
    "    x_tokens_canonical = utils.canonicalize_words(x_tokens)\n",
    "    #print(x_tokens_canonical,type(x_tokens_canonical),len(x_tokens_canonical))\n",
    "    x_tokens_list_dev.append(x_tokens_canonical)\n",
    "    #print('list',x_tokens_list)\n",
    "    #print(x_tokens_list.shape)\n",
    "    for word in x_tokens_canonical:\n",
    "        cnt[word]+=1\n",
    "    if i%25000 == 0:\n",
    "        #print('x_tokens',x_tokens,'x_tokens_canonical',x_tokens_canonical)\n",
    "        print('done',i)\n",
    "end = time.time()\n",
    "print('total time taken to canonicalize dev set',start-end)\n",
    "\n",
    "print('100 most common words in the dataset:',cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 674074\n",
      "100 most common words in the dataset: [('.', 5515451), ('the', 4837349), (',', 3103855), ('and', 3099236), ('it', 3019358), ('a', 2575808), ('to', 2552900), ('i', 2402632), ('is', 1757049), ('this', 1715868), ('for', 1683699), ('of', 1504924), ('my', 1231933), ('!', 1130799), ('with', 1077799), ('in', 978953), ('that', 935978), ('was', 885707), ('you', 737209), ('but', 721487), ('on', 713808), ('are', 683218), ('have', 651538), (\"'s\", 636324), ('DG', 625355), ('not', 619587), ('as', 611063), ('so', 583818), ('they', 575303), (\"n't\", 571841), ('we', 504222), ('he', 491747), ('very', 483348), ('one', 471937), ('great', 454262), ('be', 449318), ('all', 408473), (')', 405679), ('she', 401490), ('DGDG', 391092), ('them', 381278), ('(', 361572), ('would', 357256), ('just', 348920), ('can', 348020), ('old', 347236), ('at', 336834), ('like', 336179), ('has', 331586), ('game', 330565), ('toy', 324293), ('do', 319154), ('when', 313682), ('her', 305692), ('up', 305368), ('or', 298433), ('if', 296701), ('little', 295011), ('had', 293912), ('out', 291720), ('these', 285210), ('will', 284236), ('loves', 281368), ('fun', 277101), ('get', 268774), ('love', 265526), ('more', 261048), ('play', 258740), ('from', 258198), ('year', 255810), ('well', 252613), ('good', 251402), ('&', 248348), ('bought', 247179), ('really', 246751), ('time', 231746), (';', 228800), ('son', 228332), ('his', 223680), ('...', 222986), ('only', 218394), ('an', 206813), ('there', 205782), ('kids', 201945), ('got', 200979), ('other', 199049), ('about', 198959), ('daughter', 198599), ('were', 198139), ('than', 194508), ('set', 190058), ('much', 185136), ('our', 183641), ('also', 182296), ('your', 180965), ('did', 180028), ('some', 178892), ('what', 178379), ('does', 176511), ('because', 175192)]\n",
      "id list shape 1200000\n",
      "doing dev set\n",
      "dev list shape 360000\n"
     ]
    }
   ],
   "source": [
    "#Create vocabulary on train data set.\n",
    "vocab = vocabulary.Vocabulary(cnt, size=None)\n",
    "print('vocab size',vocab.size)\n",
    "print('100 most common words in the dataset:',cnt.most_common(100))\n",
    "\n",
    "x_tokens_id_list = []\n",
    "for i in range(temp_train_toys.shape[0]):\n",
    "#for i in range(5):\n",
    "    x_tokens_ids = vocab.words_to_ids(x_tokens_list[i])\n",
    "    #print (x_tokens_ids)\n",
    "    x_tokens_id_list.append(x_tokens_ids)\n",
    "    #print(x_tokens_id_list)\n",
    "\n",
    "print('id list shape',len(x_tokens_id_list))\n",
    "\n",
    "print('doing dev set')\n",
    "x_tokens_id_list_dev = []\n",
    "for i in range(temp_dev_toys.shape[0]):\n",
    "#for i in range(5):\n",
    "    x_tokens_ids = vocab.words_to_ids(x_tokens_list_dev[i])\n",
    "    #print (x_tokens_ids)\n",
    "    x_tokens_id_list_dev.append(x_tokens_ids)\n",
    "    #print(x_tokens_id_list_dev)\n",
    "print('dev list shape',len(x_tokens_id_list_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360000,)\n",
      "[ 1.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.\n",
      "  1.  1.] 392707     5.0\n",
      "1253479    5.0\n",
      "304445     1.0\n",
      "865041     4.0\n",
      "1457898    5.0\n",
      "799874     5.0\n",
      "1977200    1.0\n",
      "2243413    5.0\n",
      "2094383    5.0\n",
      "341215     5.0\n",
      "1337643    4.0\n",
      "222962     5.0\n",
      "798888     5.0\n",
      "2088004    5.0\n",
      "1251549    5.0\n",
      "1420047    4.0\n",
      "2055081    4.0\n",
      "2175931    2.0\n",
      "570459     4.0\n",
      "291418     5.0\n",
      "Name: overall, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Converting ratings to binary\n",
    "temp_train_y = np.zeros(temp_train_toys.shape[0])\n",
    "temp_train_y[temp_train_toys.overall > 3] = 1\n",
    "#print(temp_train_y.shape,temp_train_y[:20],temp_train_toys.overall[:20])\n",
    "temp_dev_y = np.zeros(temp_dev_toys.shape[0])\n",
    "temp_dev_y[temp_dev_toys.overall>3] = 1\n",
    "print(temp_dev_y.shape)\n",
    "print(temp_dev_y[:20],temp_dev_toys.overall[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: x = (1200000, 674074) sparse, y = (1200000,)\n",
      "Test set:     x = (360000, 674074) sparse, y = (360000,)\n"
     ]
    }
   ],
   "source": [
    "# x = utils.id_lists_to_sparse_bow(df['ids'], self.vocab.size)\n",
    "# y = np.array(df.label, dtype=np.int32)\n",
    "    \n",
    "train_x_csr = utils.id_lists_to_sparse_bow(x_tokens_id_list, vocab.size)\n",
    "train_y = np.array(temp_train_toys.overall, dtype=np.int32)\n",
    "train_y_b = temp_train_y\n",
    "dev_x_csr = utils.id_lists_to_sparse_bow(x_tokens_id_list_dev, vocab.size)\n",
    "dev_y = np.array(temp_dev_toys.overall, dtype=np.int32)\n",
    "dev_y_b = temp_dev_y\n",
    "\n",
    "print(\"Training set: x = {:s} sparse, y = {:s}\".format(str(train_x_csr.shape), \n",
    "                                                str(train_y.shape)))\n",
    "print(\"Test set:     x = {:s} sparse, y = {:s}\".format(str(dev_x_csr.shape), \n",
    "                                                str(dev_y.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dev set for binary prediction: 91.92%\n",
      "Accuracy on dev set for 4 level (1,2,4,5) prediction: 71.24%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Naive bayes for binary prediction\n",
    "nb_b = MultinomialNB()\n",
    "nb_b.fit(train_x_csr,train_y_b)\n",
    "y_pred = nb_b.predict(dev_x_csr)\n",
    "\n",
    "acc = accuracy_score(dev_y_b, y_pred)\n",
    "print(\"Accuracy on dev set for binary prediction: {:.02%}\".format(acc))\n",
    "\n",
    "#Naive bayes for 4 level rating prediction, excluding the 3s\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_x_csr,train_y)\n",
    "y_pred = nb.predict(dev_x_csr)\n",
    "\n",
    "acc = accuracy_score(dev_y, y_pred)\n",
    "print(\"Accuracy on dev set for 4 level (1,2,4,5) prediction: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative features - binary prediction:\n",
      "  jpl-discount (-4.70)\n",
      "  disgraceful (-4.57)\n",
      "  .poor (-4.41)\n",
      "  gbcpro (-4.16)\n",
      "  99cent (-4.16)\n",
      "  poor.the (-4.16)\n",
      "  reimbursement (-4.09)\n",
      "  disatisfied (-4.09)\n",
      "  riddance (-4.09)\n",
      "  bambin (-4.09)\n",
      "  browsers (-4.09)\n",
      "  v.motion (-4.01)\n",
      "  returing (-4.01)\n",
      "  3mp (-4.01)\n",
      "  surge= (-4.01)\n",
      "  gyp (-3.92)\n",
      "  merchsource (-3.92)\n",
      "  .total (-3.92)\n",
      "  slaughterhouse (-3.92)\n",
      "  aviva (-3.92)\n",
      "\n",
      "Most positive features: binary prediction:\n",
      "  gatherings (3.09)\n",
      "  sweetest (3.11)\n",
      "  ball-jointed (3.14)\n",
      "  manatee (3.15)\n",
      "  must-have (3.16)\n",
      "  nitpick (3.19)\n",
      "  samus (3.24)\n",
      "  it.great (3.25)\n",
      "  pullip (3.31)\n",
      "  quality,10 (3.31)\n",
      "  exia (3.31)\n",
      "  please.the (3.33)\n",
      "  meeple (3.44)\n",
      "  non-gamers (3.44)\n",
      "  ella (3.53)\n",
      "  liger (3.55)\n",
      "  seaside (3.65)\n",
      "  excelente (3.69)\n",
      "  gn (3.86)\n",
      "  ball-hinged (4.00)\n",
      "\n",
      "Most negative features - (1,2,4,5) prediction:\n",
      "  carnivorous (-3.13)\n",
      "  booker (-3.04)\n",
      "  fraudulent (-2.98)\n",
      "  nst (-2.94)\n",
      "  v.motion (-2.79)\n",
      "  jpl-discount (-2.79)\n",
      "  flexpak (-2.65)\n",
      "  phd (-2.65)\n",
      "  saddest (-2.60)\n",
      "  hermit (-2.60)\n",
      "\n",
      "Most positive features: (1,2,4,5) prediction:\n",
      "  kamen (2.79)\n",
      "  kems (2.88)\n",
      "  dhs (2.96)\n",
      "  f-14 (2.96)\n",
      "  sarlacc (2.96)\n",
      "  synonyms (3.03)\n",
      "  zw (3.10)\n",
      "  travellers (3.10)\n",
      "  ahss (3.44)\n",
      "  jinn (3.61)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Pulling out and printing most positive and negative features - binary prediction\n",
    "linear_weights = nb_b.feature_log_prob_[1,] - nb_b.feature_log_prob_[0,]  # populate this with actual values\n",
    "top_negative_features = np.argsort(linear_weights)[:20]\n",
    "top_positive_features = np.argsort(linear_weights)[-20:]\n",
    "\n",
    "\n",
    "print(\"Most negative features - binary prediction:\")\n",
    "for idx in top_negative_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))\n",
    "print(\"\")\n",
    "print(\"Most positive features: binary prediction:\")\n",
    "for idx in top_positive_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))\n",
    "    \n",
    "#Pulling out and printing most positive and negative features - 4 way prediction\n",
    "linear_weights = nb.feature_log_prob_[1,] - nb.feature_log_prob_[0,]  # populate this with actual values\n",
    "top_negative_features = np.argsort(linear_weights)[:10]\n",
    "top_positive_features = np.argsort(linear_weights)[-10:]\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Most negative features - (1,2,4,5) prediction:\")\n",
    "for idx in top_negative_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))\n",
    "print(\"\")\n",
    "print(\"Most positive features: (1,2,4,5) prediction:\")\n",
    "for idx in top_positive_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping track of results from test runs\n",
    "With number in train set = 10000 (excl 3 ratings)    \n",
    "    Accuracy on dev set for binary prediction: 88.74%\n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 67.16%\n",
    "    Vocab Size : 38696\n",
    "    \n",
    "With number in train set = 50000 (excl 3 ratings)   \n",
    "    Accuracy on dev set for binary prediction: 91.33%   \n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 69.33% \n",
    "    Vocab Size : ~ 104000\n",
    "    \n",
    "With number in train set = 100000 (excl 3 ratings)\n",
    "    Accuracy on dev set for binary prediction: 91.72%\n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 70.25%\n",
    "    Vocab Size : 164897\n",
    "\n",
    "With number in train set = 500000, dev set = 150000 (excl 3 ratings)    \n",
    "    Accuracy on dev set for binary prediction: 91.92%\n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 71.24%\n",
    "    vocab size 367096\n",
    "    \n",
    "With number in train set = 1200000, dev set = 360000 (excl 3 ratings)    \n",
    "    Accuracy on dev set for binary prediction: 91.92%\n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 71.24%\n",
    "    vocab size 674074"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
