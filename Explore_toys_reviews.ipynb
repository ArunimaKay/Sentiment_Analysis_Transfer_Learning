{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import time\n",
    "# Install a few python packages using pip\n",
    "from common import utils\n",
    "utils.require_package('nltk')\n",
    "utils.require_package(\"wget\")      # for fetching dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard python helper libraries.\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys, time\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "\n",
    "# NLTK is the Natural Language Toolkit, and contains several language datasets\n",
    "# as well as implementations of many popular NLP algorithms.\n",
    "# HINT: You should look at what is available here when thinking about your project!\n",
    "import nltk\n",
    "\n",
    "# Helper libraries (see the corresponding py files in this notebook's directory).\n",
    "from common import utils, vocabulary\n",
    "import segment\n",
    "\n",
    "utils.require_package(\"tqdm\")  # for nice progress bars\n",
    "from tqdm import tqdm as ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 88.33158898353577\n",
      "end getDF\n",
      "time taken to load data =  88.33197402954102\n",
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 127.22161197662354\n",
      "end getDF\n",
      "time taken to load data =  127.22227787971497\n"
     ]
    }
   ],
   "source": [
    "def parse(path):\n",
    "  print('start parse')\n",
    "  start_parse = time.time()\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "  end_parse = time.time()\n",
    "  print('end parse with time for parse',end_parse - start_parse)\n",
    "\n",
    "def getDF(path):\n",
    "  print('start getDF')\n",
    "  start = time.time()\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  print('end getDF')\n",
    "  end = time.time()\n",
    "  print('time taken to load data = ',end-start)\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "#df = getDF('reviews_Toys_and_Games.json.gz')\n",
    "df_vid = getDF('reviews_Video_Games.json.gz')\n",
    "df_toys = getDF('reviews_Toys_and_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 69.44895935058594\n",
      "end getDF\n",
      "time taken to load data =  69.44927167892456\n"
     ]
    }
   ],
   "source": [
    "df_aut = getDF('reviews_Automotive.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 232.55289697647095\n",
      "end getDF\n",
      "time taken to load data =  232.5537166595459\n"
     ]
    }
   ],
   "source": [
    "df_hnk = getDF('reviews_Home_and_Kitchen.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys reviews summary\n",
      "(2252771, 9)\n",
      "Index(['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',\n",
      "       'overall', 'summary', 'unixReviewTime', 'reviewTime'],\n",
      "      dtype='object')\n",
      "video games reviews summary\n",
      "(1324753, 9)\n",
      "Index(['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',\n",
      "       'overall', 'summary', 'unixReviewTime', 'reviewTime'],\n",
      "      dtype='object')\n",
      "Auto reviews summary\n",
      "(1373768, 9)\n",
      "Index(['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',\n",
      "       'overall', 'summary', 'unixReviewTime', 'reviewTime'],\n",
      "      dtype='object')\n",
      "Home and Kitchen reviews summary\n",
      "(4253926, 9)\n",
      "Index(['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',\n",
      "       'overall', 'summary', 'unixReviewTime', 'reviewTime'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A210NOCSTBT4OD</td>\n",
       "      <td>0076144011</td>\n",
       "      <td>Sheila</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Have you ever thought about how you met your b...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Lovely</td>\n",
       "      <td>1349308800</td>\n",
       "      <td>10 4, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A28ILV4TOG8BH2</td>\n",
       "      <td>0130350591</td>\n",
       "      <td>ccjensen</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>The butter dish is serving us well, and keepin...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Nice looking, and keeps the butter fresh</td>\n",
       "      <td>1300752000</td>\n",
       "      <td>03 22, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A31B4D7URW4DNZ</td>\n",
       "      <td>0307394530</td>\n",
       "      <td>3Gigi3</td>\n",
       "      <td>[11, 16]</td>\n",
       "      <td>I anxiously waited for the book I had pre orde...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mother of the Bride</td>\n",
       "      <td>1214784000</td>\n",
       "      <td>06 30, 2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2HU0RPDRZZOP1</td>\n",
       "      <td>0307394530</td>\n",
       "      <td>Alexey Leontev</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Fantastic book, a lot of good, original recipe...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>amazing book</td>\n",
       "      <td>1277337600</td>\n",
       "      <td>06 24, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A7J0XOW7DYBBD</td>\n",
       "      <td>0307394530</td>\n",
       "      <td>Allan Mar Cariaso</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Can't wait to try all the amazing techniques. ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very helpful</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>02 23, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin       reviewerName   helpful  \\\n",
       "0  A210NOCSTBT4OD  0076144011             Sheila    [0, 0]   \n",
       "1  A28ILV4TOG8BH2  0130350591           ccjensen    [0, 0]   \n",
       "2  A31B4D7URW4DNZ  0307394530             3Gigi3  [11, 16]   \n",
       "3  A2HU0RPDRZZOP1  0307394530     Alexey Leontev    [0, 0]   \n",
       "4   A7J0XOW7DYBBD  0307394530  Allan Mar Cariaso    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  Have you ever thought about how you met your b...      4.0   \n",
       "1  The butter dish is serving us well, and keepin...      5.0   \n",
       "2  I anxiously waited for the book I had pre orde...      2.0   \n",
       "3  Fantastic book, a lot of good, original recipe...      5.0   \n",
       "4  Can't wait to try all the amazing techniques. ...      5.0   \n",
       "\n",
       "                                    summary  unixReviewTime   reviewTime  \n",
       "0                                    Lovely      1349308800   10 4, 2012  \n",
       "1  Nice looking, and keeps the butter fresh      1300752000  03 22, 2011  \n",
       "2                       Mother of the Bride      1214784000  06 30, 2008  \n",
       "3                              amazing book      1277337600  06 24, 2010  \n",
       "4                              Very helpful      1393113600  02 23, 2014  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('toys reviews summary')\n",
    "print(df_toys.shape)\n",
    "print(df_toys.columns)\n",
    "df_toys.head(5)\n",
    "print('video games reviews summary')\n",
    "print(df_vid.shape)\n",
    "print(df_vid.columns)\n",
    "df_vid.head(5)\n",
    "print('Auto reviews summary')\n",
    "print(df_aut.shape)\n",
    "print(df_aut.columns)\n",
    "df_aut.head(5)\n",
    "print('Home and Kitchen reviews summary')\n",
    "print(df_hnk.shape)\n",
    "print(df_hnk.columns)\n",
    "df_hnk.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings distribution for toys          reviewerID     asin  reviewerName  helpful  reviewText  summary  \\\n",
      "overall                                                                    \n",
      "1.0          192993   192993        192435   192993      192993   192993   \n",
      "2.0          115801   115801        115416   115801      115801   115801   \n",
      "3.0          193941   193941        193195   193941      193941   193941   \n",
      "4.0          407884   407884        406255   407884      407884   407884   \n",
      "5.0         1342152  1342152       1333623  1342152     1342152  1342152   \n",
      "\n",
      "         unixReviewTime  reviewTime  \n",
      "overall                              \n",
      "1.0              192993      192993  \n",
      "2.0              115801      115801  \n",
      "3.0              193941      193941  \n",
      "4.0              407884      407884  \n",
      "5.0             1342152     1342152  \n",
      "Ratings distribution for video games          reviewerID    asin  reviewerName  helpful  reviewText  summary  \\\n",
      "overall                                                                   \n",
      "1.0          152840  152840        149509   152840      152840   152840   \n",
      "2.0           77513   77513         76692    77513       77513    77513   \n",
      "3.0          124370  124370        122959   124370      124370   124370   \n",
      "4.0          260260  260260        256782   260260      260260   260260   \n",
      "5.0          709770  709770        692626   709770      709770   709770   \n",
      "\n",
      "         unixReviewTime  reviewTime  \n",
      "overall                              \n",
      "1.0              152840      152840  \n",
      "2.0               77513       77513  \n",
      "3.0              124370      124370  \n",
      "4.0              260260      260260  \n",
      "5.0              709770      709770  \n",
      "Ratings distribution for automobiles          reviewerID    asin  reviewerName  helpful  reviewText  summary  \\\n",
      "overall                                                                   \n",
      "1.0          122160  122160        121495   122160      122160   122160   \n",
      "2.0           64112   64112         63784    64112       64112    64112   \n",
      "3.0          103857  103857        103222   103857      103857   103857   \n",
      "4.0          230293  230293        228871   230293      230293   230293   \n",
      "5.0          853346  853346        847959   853346      853346   853346   \n",
      "\n",
      "         unixReviewTime  reviewTime  \n",
      "overall                              \n",
      "1.0              122160      122160  \n",
      "2.0               64112       64112  \n",
      "3.0              103857      103857  \n",
      "4.0              230293      230293  \n",
      "5.0              853346      853346  \n",
      "Ratings distribution for home and kitchen          reviewerID     asin  reviewerName  helpful  reviewText  summary  \\\n",
      "overall                                                                    \n",
      "1.0          418381   418381        416013   418381      418381   418381   \n",
      "2.0          242048   242048        240272   242048      242048   242048   \n",
      "3.0          345094   345094        342420   345094      345094   345094   \n",
      "4.0          740864   740864        734679   740864      740864   740864   \n",
      "5.0         2507539  2507539       2487340  2507539     2507539  2507539   \n",
      "\n",
      "         unixReviewTime  reviewTime  \n",
      "overall                              \n",
      "1.0              418381      418381  \n",
      "2.0              242048      242048  \n",
      "3.0              345094      345094  \n",
      "4.0              740864      740864  \n",
      "5.0             2507539     2507539  \n"
     ]
    }
   ],
   "source": [
    "#Count by ratings to determine skew in sample.\n",
    "print('Ratings distribution for toys',df_toys.groupby('overall').count())\n",
    "print('Ratings distribution for video games',df_vid.groupby('overall').count())\n",
    "print('Ratings distribution for automobiles',df_aut.groupby('overall').count())\n",
    "print('Ratings distribution for home and kitchen',df_hnk.groupby('overall').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys reviews examples\n",
      "\n",
      "AMEVO2LY6VEJA\n",
      "Great product, thank you! Our son loved the puzzles.  They have large pieces yet they are still challenging for a 4 year old.\n",
      "A3C9CSW3TJITGT\n",
      "I love these felt nursery rhyme characters and scenes.  The quality of the felt is good, and the illustrations are detailed and pretty.  As noted, the figures and scenes are printed on 2 large sheets of flannel and each individual item needs to be cut out.  This process took me 2 hours of tiny cutting.  To me it does not lend itself to a book form but rather laying out the scenes separately or for use on a flannel board.  However, I love the quiet play it offers for my toddler, and as a former Kindergarten teacher, I understand the value of learning rhyme and its connection to future reading.  Overall, delightful product with some work involved.\n",
      "A31POTIYCKSZ9G\n",
      "I see no directions for its use. Therefore I have to make up the games, unfortunately.\n",
      "A2GGHHME9B6W4O\n",
      "This is a great tool for any teacher using the Pre Calculus Enhanced with Graphing Utilities book (of any edition). Easy to use with a very extensive test bank. Excellent!\n",
      "A1FSLDH43ORWZP\n",
      "Although not as streamlined as the Algebra I materials .. this is extremely helpful for first time teachers ...bulk of materials are prepared for presentations.\n",
      "\n",
      "video games reviews examples\n",
      "\n",
      "AB9S9279OZ3QO\n",
      "I haven't gotten around to playing the campaign but the multiplayer is solid and pretty fun. Includes Zero Dark Thirty pack, an Online Pass, and the all powerful Battlefield 4 Beta access.\n",
      "A24SSUT5CSW8BH\n",
      "I want to start off by saying I have never played the Call of Duty games. This is only the second first person shooter game that I have own. I think it is a lot of fun. Has good graphics and nice story line. It does take some skill to get through the levels. I think all players can enjoy this game. There are three levels to choose from based on your skill level. If your looking for first person shooter game that has current military type play than this is a good buy.\n",
      "AK3V0HEBJMQ7J\n",
      "this will be my second medal of honor I love how the incorporate real life military stories in the game great\n",
      "A10BECPH7W8HM7\n",
      "great game when it first came out, and still a great game\n",
      "A2PRV9OULX1TWP\n",
      "this is the first need for speed I bought years and years ago.  I lost it so I bought this for a trip down memory lane.  Pretty tame by todays games.  It brought back memories of fun times.\n",
      "\n",
      "automobile reviews examples\n",
      "\n",
      "A108J5O7DG2WIM\n",
      "I loved the look and the great improvement at night drivingA bit expensive but work great.Installation took me about 3 hours.\n",
      "A1QBLUSZW281TA\n",
      "Put these on my 2011 can am outlander 800xt, easy to install, and being oem everything just plugged right in , even like the power switch they came with\n",
      "A3B40ZIZJ3HEP7\n",
      "Don't buy this item  , its not a 4 window roll up , its look nothing like the picture , they sent me some crap,  and the seller is charging me 20 % restocking fee plus I have to pay for shipping , I will never buy from them again , YOU HAVE BEEN WARNED ...\n",
      "A1DUAXYX5WHSX1\n",
      "Nice. A decent model, and at a better price on Amazon than anywhere else. Warning: This model starts the new despicable business practice of selling each vehicle with only one weapons configuration, instead of the multiple weapons we're used to. Which makes the MSRP price hike even more despicable. But this walker is worth getting - it puts German anti-tank on near par with Sov anti-tank capability, for fewer points - as long as you're not paying Battlefront's despicable prices.\n",
      "A1VS7YWE0NCAS3\n",
      "I used this rating because I liked the key fob very much but I haven't had achance to install it yet. I don't know yet how it will work or how difficult it will be to install or how well it will work. I think it will be OK though when I do install it..\n",
      "\n",
      "Home and Kitchen reviews examples\n",
      "\n",
      "A210NOCSTBT4OD\n",
      "Have you ever thought about how you met your best friend? Was it normal, or was it wacky - like how Elias met Shohei? Pulling a boa constrictor snake named Mathilda out of your backpack can make a remarkable first impression! This book is about three best friends Elias, Honoria, and Shohei, who are united against \"That Which Is The Peshtigo School\". Their goal is to make it through the annual school science fair, but things don't always go as planned.Elias is part of a family made up of science fanatics who would do anything to win a science fair. Elias isn't exactly what you'd call the ambitious type, especially when it comes to science fairs. So he becomes like Galileo and \"retests\" one of his sibling's past projects. Honoria loves to be ambitious, especially when it comes to being a legal counsel extraordinaire. But when she faces a bigger challenge than beating Goliath Reed or getting a piranha to become vegetarian, she doesn't know if she can make it. Shohei is an all around slacker who tries to mooch off Elias instead of creating something on his own. His adoptive parents are constantly encouraging him to start \"hearing\" his ancestors. His mom has even turned Shohei's room into what looks like a walk-in Japanese museum exhibit!This book is laugh out loud hilarious and the more you read, the more exciting and unexpected it gets. I love the title on this book because it really made me laugh and want to read the book. I also like how people so different from one another can be such close friends. There is not much excitement in the beginning, but it builds up very quickly. So if you like that type of story, then this is the book for you.\n",
      "A28ILV4TOG8BH2\n",
      "The butter dish is serving us well, and keeping the butter fresh and healthy. Couldn't be happier with it, and the color is a pleasing green.\n",
      "A31B4D7URW4DNZ\n",
      "I anxiously waited for the book I had pre ordered.  Pics were beautiful, but...If you don't want a cake with fondant your pretty much out of luck. As most guests want something yummy to eat as well as pleasing to the eye, fondant just doesn't do it.  Sure it holds color better and makes a great presentation, it just isn't a pleaseing thing to eat.We did take away some ideas, we did use some fondant flowers on the lower layers (least likely to be eaten), and used butterccream for the really good and most enjoyed layers.Good Job Martha, maybe next time you could show how to make butter cream just as attractive as fondant.MOTB\n",
      "A2HU0RPDRZZOP1\n",
      "Fantastic book, a lot of good, original recipes. All very nicely decorated. This book will be the best gift for many.\n",
      "A7J0XOW7DYBBD\n",
      "Can't wait to try all the amazing techniques. I just browsed and I can say that this is the book I am looking for.\n"
     ]
    }
   ],
   "source": [
    "#Looking at a few examples\n",
    "print('toys reviews examples\\n')\n",
    "for i in range(5):\n",
    "    print(df_toys['reviewerID'].iloc[i])\n",
    "    print(df_toys['reviewText'].iloc[i])\n",
    "\n",
    "print('\\nvideo games reviews examples\\n')\n",
    "for i in range(5):\n",
    "    print(df_vid['reviewerID'].iloc[i])\n",
    "    print(df_vid['reviewText'].iloc[i])\n",
    "    \n",
    "print('\\nautomobile reviews examples\\n')\n",
    "for i in range(5):\n",
    "    print(df_aut['reviewerID'].iloc[i])\n",
    "    print(df_aut['reviewText'].iloc[i])\n",
    "    \n",
    "print('\\nHome and Kitchen reviews examples\\n')\n",
    "for i in range(5):\n",
    "    print(df_hnk['reviewerID'].iloc[i])\n",
    "    print(df_hnk['reviewText'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique products in toys asin    327698\n",
      "0       327698\n",
      "dtype: int64\n",
      "total number of products with at least 5 reviews\n",
      " count of unique products:\n",
      " asin    68782\n",
      "0       68782\n",
      "dtype: int64 sum of their reviews asin    0375829695043985589604398935770470182318048645...\n",
      "0                                                 1775109\n",
      "dtype: object\n",
      "total number of products with at least 20 reviews\n",
      " count of unique products:\n",
      " asin    19992\n",
      "0       19992\n",
      "dtype: int64 sum of their reviews asin    043985589604398935770470182318048645195X054534...\n",
      "0                                                 1275698\n",
      "dtype: object\n",
      "total number of unique products in videos asin    50210\n",
      "0       50210\n",
      "dtype: int64\n",
      "total number of products with at least 5 reviews\n",
      " count of unique products:\n",
      " asin    23866\n",
      "0       23866\n",
      "dtype: int64 sum of their reviews asin    043940133X043959136807000266570700099867075853...\n",
      "0                                                 1266698\n",
      "dtype: object\n",
      "total number of products with at least 20 reviews\n",
      " count of unique products:\n",
      " asin    10904\n",
      "0       10904\n",
      "dtype: int64 sum of their reviews asin    0700026657070009986738668116596050036071710002...\n",
      "0                                                 1124236\n",
      "dtype: object\n",
      "total number of unique products in automobiles asin    320112\n",
      "0       320112\n",
      "dtype: int64\n",
      "total number of products with at least 5 reviews\n",
      " count of unique products:\n",
      " asin    42052\n",
      "0       42052\n",
      "dtype: int64 sum of their reviews asin    5926025338592702632X74628721618016868193953975...\n",
      "0                                                  912369\n",
      "dtype: object\n",
      "total number of products with at least 20 reviews\n",
      " count of unique products:\n",
      " asin    10218\n",
      "0       10218\n",
      "dtype: int64 sum of their reviews asin    9539751047B00000JD3OB00002243XB00002243ZB00002...\n",
      "0                                                  593687\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Get the count by unique product id\n",
    "tempcnt = df_toys.groupby('asin').size().reset_index()\n",
    "print('total number of unique products in toys',tempcnt.count())\n",
    "print('total number of products with at least 5 reviews\\n','count of unique products:\\n',tempcnt[tempcnt.iloc[:,1] > 5].count(),'sum of their reviews',tempcnt[tempcnt.iloc[:,1] > 5].sum())\n",
    "print('total number of products with at least 20 reviews\\n','count of unique products:\\n',tempcnt[tempcnt.iloc[:,1] > 20].count(),'sum of their reviews',tempcnt[tempcnt.iloc[:,1] > 20].sum())\n",
    "\n",
    "tempcnt = df_vid.groupby('asin').size().reset_index()\n",
    "print('total number of unique products in videos',tempcnt.count())\n",
    "print('total number of products with at least 5 reviews\\n','count of unique products:\\n',tempcnt[tempcnt.iloc[:,1] > 5].count(),'sum of their reviews',tempcnt[tempcnt.iloc[:,1] > 5].sum())\n",
    "print('total number of products with at least 20 reviews\\n','count of unique products:\\n',tempcnt[tempcnt.iloc[:,1] > 20].count(),'sum of their reviews',tempcnt[tempcnt.iloc[:,1] > 20].sum())\n",
    "\n",
    "tempcnt = df_aut.groupby('asin').size().reset_index()\n",
    "print('total number of unique products in automobiles',tempcnt.count())\n",
    "print('total number of products with at least 5 reviews\\n','count of unique products:\\n',tempcnt[tempcnt.iloc[:,1] > 5].count(),'sum of their reviews',tempcnt[tempcnt.iloc[:,1] > 5].sum())\n",
    "print('total number of products with at least 20 reviews\\n','count of unique products:\\n',tempcnt[tempcnt.iloc[:,1] > 20].count(),'sum of their reviews',tempcnt[tempcnt.iloc[:,1] > 20].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1351662, 9) (450554, 9) (450555, 9)\n",
      "(794851, 9) (264951, 9) (264951, 9)\n",
      "(824260, 9) (274754, 9) (274754, 9)\n",
      "(2552355, 9) (850785, 9) (850786, 9)\n"
     ]
    }
   ],
   "source": [
    "#Create train,dev,test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_toys,devtest = train_test_split(df_toys, test_size=0.4)\n",
    "dev_toys,test_toys = train_test_split(devtest,test_size = 0.5)\n",
    "print(train_toys.shape,dev_toys.shape,test_toys.shape)\n",
    "\n",
    "#For Video games reviews\n",
    "train_vid,devtest = train_test_split(df_vid, test_size=0.4)\n",
    "dev_vid,test_vid = train_test_split(devtest,test_size = 0.5)\n",
    "print(train_vid.shape,dev_vid.shape,test_vid.shape)\n",
    "\n",
    "#For Auto reviews\n",
    "train_aut,devtest = train_test_split(df_aut, test_size=0.4)\n",
    "dev_aut,test_aut = train_test_split(devtest,test_size = 0.5)\n",
    "print(train_aut.shape,dev_aut.shape,test_aut.shape)\n",
    "\n",
    "#For Home and Kitchen reviews\n",
    "train_hnk,devtest = train_test_split(df_hnk, test_size=0.4)\n",
    "dev_hnk,test_hnk = train_test_split(devtest,test_size = 0.5)\n",
    "print(train_hnk.shape,dev_hnk.shape,test_hnk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_df_size(size,data_train,data_dev):\n",
    "    size_train = size\n",
    "    len_max_train = data_train[data_train.overall!=3].shape[0] #max possible length of train data set taking out the 3 ratings.\n",
    "    print(\"Number of reviews with ratings != 3 in train set\",len_max_train)\n",
    "    temp_size_train = min(len_max_train,size_train)\n",
    "\n",
    "    len_max_dev = data_dev[data_dev.overall!=3].shape[0]\n",
    "    print(\"Number of reviews with ratings != 3 in dev set\",len_max_dev)\n",
    "    temp_size_dev = min(len_max_dev,int(0.3*temp_size_train)) #making the dev set about 0.3 times the train set.\n",
    "\n",
    "    temp_train_data = data_train[data_train.overall != 3][:temp_size_train]\n",
    "    print('Size of train data',temp_train_data.shape)\n",
    "    #print(temp_train_data.groupby('overall').count())\n",
    "    #print(temp_train_toys[:5])\n",
    "\n",
    "    temp_dev_data = data_dev[data_dev.overall!=3][:temp_size_dev]\n",
    "    print('Size of dev data',temp_dev_data.shape)\n",
    "    #print(temp_dev_data.groupby('overall').count())\n",
    "    #print(temp_dev_data[:2])\n",
    "    \n",
    "    #Binarize ratings\n",
    "    temp_train_y = np.zeros(temp_size_train)\n",
    "    temp_train_y[temp_train_data.overall > 3] = 1\n",
    "    temp_dev_y = np.zeros(temp_size_dev)\n",
    "    temp_dev_y[temp_dev_data.overall>3] = 1\n",
    "    print('binarized y shape',temp_train_y.shape,temp_dev_y.shape)\n",
    "    #print(temp_dev_y[:20],data_dev.overall[:20])\n",
    "    return temp_train_data,temp_dev_data,temp_train_y,temp_dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys reviews\n",
      "\n",
      "Number of reviews with ratings != 3 in train set 1235008\n",
      "Number of reviews with ratings != 3 in dev set 411966\n",
      "Size of train data (100000, 9)\n",
      "Size of dev data (30000, 9)\n",
      "binarized y shape (100000,) (30000,)\n",
      "\n",
      "video games reviews\n",
      "\n",
      "Number of reviews with ratings != 3 in train set 720082\n",
      "Number of reviews with ratings != 3 in dev set 240212\n",
      "Size of train data (100000, 9)\n",
      "Size of dev data (30000, 9)\n",
      "binarized y shape (100000,) (30000,)\n",
      "\n",
      "automobiles reviews\n",
      "\n",
      "Number of reviews with ratings != 3 in train set 762084\n",
      "Number of reviews with ratings != 3 in dev set 253791\n",
      "Size of train data (100000, 9)\n",
      "Size of dev data (30000, 9)\n",
      "binarized y shape (100000,) (30000,)\n",
      "done\n",
      "Number of reviews with ratings != 3 in train set 2345119\n",
      "Number of reviews with ratings != 3 in dev set 781620\n",
      "Size of train data (100000, 9)\n",
      "Size of dev data (30000, 9)\n",
      "binarized y shape (100000,) (30000,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "size_train = 100000\n",
    "print('toys reviews\\n')\n",
    "temp_train_toys, temp_dev_toys,temp_train_toys_y,temp_dev_toys_y = set_df_size(size_train,train_toys,dev_toys)\n",
    "print('\\nvideo games reviews\\n')\n",
    "temp_train_vid, temp_dev_vid,temp_train_vid_y,temp_dev_vid_y = set_df_size(size_train,train_vid,dev_vid)\n",
    "print('\\nautomobiles reviews\\n')\n",
    "temp_train_aut, temp_dev_aut,temp_train_aut_y,temp_dev_aut_y = set_df_size(size_train,train_aut,dev_aut)\n",
    "print('done')\n",
    "temp_train_hnk, temp_dev_hnk,temp_train_hnk_y,temp_dev_hnk_y = set_df_size(size_train,train_hnk,dev_hnk)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 25000\n",
      "done 50000\n",
      "done 75000\n",
      "done 100000\n",
      "done 125000\n",
      "done 150000\n",
      "done 175000\n",
      "done 200000\n",
      "done 225000\n",
      "done 250000\n",
      "done 275000\n",
      "done 300000\n",
      "done 325000\n",
      "done 350000\n",
      "done 375000\n",
      "done 400000\n",
      "done 425000\n",
      "done 450000\n",
      "done 475000\n",
      "total time taken to canonicalize -442.5564184188843\n",
      "100 most common words in the dataset: [('.', 1766866), ('the', 1548962), (',', 994173), ('and', 990792), ('it', 968089), ('a', 824326), ('to', 816861), ('i', 770345), ('is', 562166), ('this', 548846), ('for', 538379), ('of', 481605), ('my', 395452), ('!', 361257), ('with', 344227), ('in', 313419), ('that', 299537), ('was', 284199), ('you', 236122), ('but', 231972), ('on', 228584), ('are', 218913), ('have', 209179), (\"'s\", 203357), ('DG', 200948), ('not', 198800), ('as', 195126), ('so', 187108), ('they', 183769), (\"n't\", 183235), ('we', 161394), ('he', 157594), ('very', 154850), ('one', 151223), ('great', 146363), ('be', 143362), ('all', 130528), (')', 129631), ('she', 128389), ('DGDG', 125484), ('them', 121653), ('(', 115468), ('would', 114432), ('just', 112185), ('old', 111766), ('can', 111204), ('at', 107540), ('like', 107451), ('game', 106187), ('has', 106167), ('toy', 104604), ('do', 102128), ('when', 100761), ('up', 97644), ('her', 96785), ('or', 95880), ('if', 95190), ('had', 94482), ('little', 94272), ('out', 93241), ('these', 91685), ('will', 90799), ('loves', 90324), ('fun', 88443), ('get', 86652), ('love', 84538), ('more', 83718), ('play', 83232), ('from', 82367), ('year', 82228), ('&', 82021), ('well', 80726), ('good', 80669), ('really', 79262), ('bought', 79030), (';', 75926), ('time', 74329), ('son', 73273), ('...', 71889), ('his', 71462), ('only', 69919), ('an', 66037), ('there', 65743), ('kids', 64622), ('got', 64109), ('other', 63766), ('daughter', 63664), ('about', 63499), ('were', 63202), ('than', 62508), ('set', 61186), ('our', 59117), ('much', 58724), ('also', 58593), ('your', 57853), ('did', 57755), ('some', 57448), ('#', 56924), ('what', 56894), ('does', 56721)]\n"
     ]
    }
   ],
   "source": [
    "cnt = collections.Counter()\n",
    "#print(ds.vocab.num_unigrams)\n",
    "x_tokens_list = []\n",
    "start = time.time()\n",
    "for i in range(temp_train_toys.shape[0]):\n",
    "#for i in range(5):\n",
    "    x_tokens = word_tokenize(temp_train_toys.reviewText.iloc[i])\n",
    "    #print(train_toys.reviewText.iloc[i],x_tokens)\n",
    "    #print(type(x_tokens),len(x_tokens))\n",
    "    x_tokens_canonical = utils.canonicalize_words(x_tokens)\n",
    "    #print(x_tokens_canonical,type(x_tokens_canonical),len(x_tokens_canonical))\n",
    "    x_tokens_list.append(x_tokens_canonical)\n",
    "    #print('list',x_tokens_list)\n",
    "    #print(x_tokens_list.shape)\n",
    "    for word in x_tokens_canonical:\n",
    "        cnt[word]+=1\n",
    "    if i%25000 == 0:\n",
    "        #print('x_tokens',x_tokens,'x_tokens_canonical',x_tokens_canonical)\n",
    "        print('done',i)\n",
    "end = time.time()\n",
    "print('total time taken to canonicalize',start-end)\n",
    "\n",
    "print('100 most common words in the dataset:',cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "[['seriously', ',', 'this', 'little', 'cutie', 'pie', 'looks', 'like', 'bird', 'sitting', 'beside', 'my', 'computer', '.', 'i', 'bought', 'the', 'green', 'and', 'white', 'one', 'on', 'a', 'lark', '(', 'no', 'pun', 'intended', '--', 'okay', ',', 'maybe', 'a', 'little', 'pun', ')', 'and', 'then', 'had', 'to', 'have', 'the', 'blue', 'and', 'yellow', 'one', 'as', 'a', 'companion', '.', 'they', 'sit', 'on', 'top', 'of', 'my', 'big', 'computer', 'cpu', 'guarding', 'the', 'on/off', 'button', 'from', 'the', 'paws', 'of', 'my', 'cat', '.', 'well', ',', 'it', 'works', 'for', 'us', '.', 'again', ',', 'these', 'are', 'beautiful', ',', 'very', 'realistic', 'stuffed', 'birds', 'that', 'stand', 'up', 'without', 'any', 'problem', '.', ':', ')'], ['this', 'was', 'a', 'wonderful', 'find', '--', '--', '--', 'terrific', 'price', ',', 'sturdy', 'metal', 'carrying', 'tin', ',', 'up', 'to', 'DG', 'players', ',', 'DG', 'train', 'markers', '(', 'DG', 'for', 'each', 'player', 'plus', 'DG', 'for', 'the', 'mexican', 'train', ')', ',', 'thick/heavy', 'dominoes', ',', 'easy', 'to', 'put', 'away', ',', 'great', 'train', 'hub', ',', 'clear', 'instructions', ',', 'plus', 'a', 'whistling', 'sound', 'and', 'a', 'light', '.', 'the', 'sound', 'and', 'light', 'can', 'be', 'used', 'if', 'you', 'want', ',', 'or', 'you', 'can', 'play', 'the', 'game', 'without', 'pushing', 'the', 'whistle', 'button', '(', 'nice', 'option', ')', '.', 'we', \"'re\", 'big', 'game', 'players', 'and', 'loved', 'this', 'set', '.', 'it', \"'s\", 'also', 'a', 'perfect', 'gift', '.', 'lots', 'of', 'fun', '...', 'you', 'ca', \"n't\", 'go', 'wrong', '.']]\n",
      "['ever', 'since']\n"
     ]
    }
   ],
   "source": [
    "print(len(x_tokens_list))\n",
    "print(x_tokens_list[:2])\n",
    "print(x_tokens_canonical[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 25000\n",
      "done 50000\n",
      "done 75000\n",
      "done 100000\n",
      "done 125000\n",
      "total time taken to canonicalize dev set -131.94238710403442\n",
      "100 most common words in the dev dataset: [('.', 1766866), ('the', 1548962), (',', 994173), ('and', 990792), ('it', 968089), ('a', 824326), ('to', 816861), ('i', 770345), ('is', 562166), ('this', 548846), ('for', 538379), ('of', 481605), ('my', 395452), ('!', 361257), ('with', 344227), ('in', 313419), ('that', 299537), ('was', 284199), ('you', 236122), ('but', 231972), ('on', 228584), ('are', 218913), ('have', 209179), (\"'s\", 203357), ('DG', 200948), ('not', 198800), ('as', 195126), ('so', 187108), ('they', 183769), (\"n't\", 183235), ('we', 161394), ('he', 157594), ('very', 154850), ('one', 151223), ('great', 146363), ('be', 143362), ('all', 130528), (')', 129631), ('she', 128389), ('DGDG', 125484), ('them', 121653), ('(', 115468), ('would', 114432), ('just', 112185), ('old', 111766), ('can', 111204), ('at', 107540), ('like', 107451), ('game', 106187), ('has', 106167), ('toy', 104604), ('do', 102128), ('when', 100761), ('up', 97644), ('her', 96785), ('or', 95880), ('if', 95190), ('had', 94482), ('little', 94272), ('out', 93241), ('these', 91685), ('will', 90799), ('loves', 90324), ('fun', 88443), ('get', 86652), ('love', 84538), ('more', 83718), ('play', 83232), ('from', 82367), ('year', 82228), ('&', 82021), ('well', 80726), ('good', 80669), ('really', 79262), ('bought', 79030), (';', 75926), ('time', 74329), ('son', 73273), ('...', 71889), ('his', 71462), ('only', 69919), ('an', 66037), ('there', 65743), ('kids', 64622), ('got', 64109), ('other', 63766), ('daughter', 63664), ('about', 63499), ('were', 63202), ('than', 62508), ('set', 61186), ('our', 59117), ('much', 58724), ('also', 58593), ('your', 57853), ('did', 57755), ('some', 57448), ('#', 56924), ('what', 56894), ('does', 56721)]\n"
     ]
    }
   ],
   "source": [
    "cnt_dev = collections.Counter()\n",
    "#print(ds.vocab.num_unigrams)\n",
    "x_tokens_list_dev = []\n",
    "start = time.time()\n",
    "for i in range(temp_dev_toys.shape[0]):\n",
    "#for i in range(5):\n",
    "    x_tokens = word_tokenize(temp_dev_toys.reviewText.iloc[i])\n",
    "    #print(train_toys.reviewText.iloc[i],x_tokens)\n",
    "    #print(type(x_tokens),len(x_tokens))\n",
    "    x_tokens_canonical = utils.canonicalize_words(x_tokens)\n",
    "    #print(x_tokens_canonical,type(x_tokens_canonical),len(x_tokens_canonical))\n",
    "    x_tokens_list_dev.append(x_tokens_canonical)\n",
    "    #print('list',x_tokens_list)\n",
    "    #print(x_tokens_list.shape)\n",
    "    for word in x_tokens_canonical:\n",
    "        cnt_dev[word]+=1\n",
    "    if i%25000 == 0:\n",
    "        #print('x_tokens',x_tokens,'x_tokens_canonical',x_tokens_canonical)\n",
    "        print('done',i)\n",
    "end = time.time()\n",
    "print('total time taken to canonicalize dev set',start-end)\n",
    "\n",
    "print('100 most common words in the dev dataset:',cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 305505\n",
      "100 most common words in the dataset: [('.', 1766866), ('the', 1548962), (',', 994173), ('and', 990792), ('it', 968089), ('a', 824326), ('to', 816861), ('i', 770345), ('is', 562166), ('this', 548846), ('for', 538379), ('of', 481605), ('my', 395452), ('!', 361257), ('with', 344227), ('in', 313419), ('that', 299537), ('was', 284199), ('you', 236122), ('but', 231972), ('on', 228584), ('are', 218913), ('have', 209179), (\"'s\", 203357), ('DG', 200948), ('not', 198800), ('as', 195126), ('so', 187108), ('they', 183769), (\"n't\", 183235), ('we', 161394), ('he', 157594), ('very', 154850), ('one', 151223), ('great', 146363), ('be', 143362), ('all', 130528), (')', 129631), ('she', 128389), ('DGDG', 125484), ('them', 121653), ('(', 115468), ('would', 114432), ('just', 112185), ('old', 111766), ('can', 111204), ('at', 107540), ('like', 107451), ('game', 106187), ('has', 106167), ('toy', 104604), ('do', 102128), ('when', 100761), ('up', 97644), ('her', 96785), ('or', 95880), ('if', 95190), ('had', 94482), ('little', 94272), ('out', 93241), ('these', 91685), ('will', 90799), ('loves', 90324), ('fun', 88443), ('get', 86652), ('love', 84538), ('more', 83718), ('play', 83232), ('from', 82367), ('year', 82228), ('&', 82021), ('well', 80726), ('good', 80669), ('really', 79262), ('bought', 79030), (';', 75926), ('time', 74329), ('son', 73273), ('...', 71889), ('his', 71462), ('only', 69919), ('an', 66037), ('there', 65743), ('kids', 64622), ('got', 64109), ('other', 63766), ('daughter', 63664), ('about', 63499), ('were', 63202), ('than', 62508), ('set', 61186), ('our', 59117), ('much', 58724), ('also', 58593), ('your', 57853), ('did', 57755), ('some', 57448), ('#', 56924), ('what', 56894), ('does', 56721)]\n",
      "id list shape 500000\n",
      "doing dev set\n",
      "dev list shape 150000\n"
     ]
    }
   ],
   "source": [
    "#Create vocabulary on train data set.\n",
    "vocab = vocabulary.Vocabulary(cnt, size=None)\n",
    "print('vocab size',vocab.size)\n",
    "print('100 most common words in the dataset:',cnt.most_common(100))\n",
    "\n",
    "x_tokens_id_list = []\n",
    "for i in range(temp_train_toys.shape[0]):\n",
    "#for i in range(5):\n",
    "    x_tokens_ids = vocab.words_to_ids(x_tokens_list[i])\n",
    "    #print (x_tokens_ids)\n",
    "    x_tokens_id_list.append(x_tokens_ids)\n",
    "    #print(x_tokens_id_list)\n",
    "\n",
    "print('id list shape',len(x_tokens_id_list))\n",
    "\n",
    "print('doing dev set')\n",
    "x_tokens_id_list_dev = []\n",
    "for i in range(temp_dev_toys.shape[0]):\n",
    "#for i in range(5):\n",
    "    x_tokens_ids = vocab.words_to_ids(x_tokens_list_dev[i])\n",
    "    #print (x_tokens_ids)\n",
    "    x_tokens_id_list_dev.append(x_tokens_ids)\n",
    "    #print(x_tokens_id_list_dev)\n",
    "print('dev list shape',len(x_tokens_id_list_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = utils.id_lists_to_sparse_bow(df['ids'], self.vocab.size)\n",
    "# y = np.array(df.label, dtype=np.int32)\n",
    "    \n",
    "#train_x_csr = utils.id_lists_to_sparse_bow(x_tokens_id_list, vocab.size)\n",
    "#train_y = np.array(temp_train_toys.overall, dtype=np.int32)\n",
    "train_toys_yb = temp_train_toys_y\n",
    "#dev_x_csr = utils.id_lists_to_sparse_bow(x_tokens_id_list_dev, vocab.size)\n",
    "#dev_y = np.array(temp_dev_toys.overall, dtype=np.int32)\n",
    "dev_toys_yb = temp_dev_toys_y\n",
    "\n",
    "#print(\"Training set: x = {:s} sparse, y = {:s}\".format(str(train_x_csr.shape), \n",
    " #                                               str(train_y.shape)))\n",
    "#print(\"Test set:     x = {:s} sparse, y = {:s}\".format(str(dev_x_csr.shape), \n",
    " #                                               str(dev_y.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number words in training corpus for toys: 63984\n",
      "toys dataset id shapes (100000, 63984) (30000, 63984)\n",
      "number words in training corpus for video games: 98899\n",
      "videos dataset id shapes (100000, 98899) (30000, 98899)\n",
      "number words in training corpus for automobiles: 59468\n",
      "automobile dataset id shapes (100000, 59468) (30000, 59468)\n",
      "number words in training corpus for home and kitchen: 57884\n",
      "home and kitchen dataset id shapes (100000, 57884) (30000, 57884)\n"
     ]
    }
   ],
   "source": [
    "#converting ratings to tokenized word id counts as a sparse matrix\n",
    "vect_toys = CountVectorizer() #vectorizer specific to toys\n",
    "vect_vid = CountVectorizer()  #vectorizer specific to videos\n",
    "vect_aut = CountVectorizer() #vectorizer specific to automobiles\n",
    "vect_hnk = CountVectorizer() #vectorizer specific to automobiles\n",
    "\n",
    "# tokenize train and test text data for toys\n",
    "train_toys_ids = vect_toys.fit_transform(temp_train_toys['reviewText'])\n",
    "dev_toys_ids = vect_toys.transform(temp_dev_toys['reviewText'])\n",
    "print(\"number words in training corpus for toys:\", len(vect_toys.get_feature_names()))\n",
    "print('toys dataset id shapes',train_toys_ids.shape,dev_toys_ids.shape)\n",
    "\n",
    "#tokenize train and test text data for videos\n",
    "train_vid_ids = vect_vid.fit_transform(temp_train_vid['reviewText'])\n",
    "dev_vid_ids = vect_vid.transform(temp_dev_vid['reviewText'])\n",
    "print(\"number words in training corpus for video games:\", len(vect_vid.get_feature_names()))\n",
    "print('videos dataset id shapes',train_vid_ids.shape,dev_vid_ids.shape)\n",
    "\n",
    "#tokenize train and test text data for automobiles\n",
    "train_aut_ids = vect_aut.fit_transform(temp_train_aut['reviewText'])\n",
    "dev_aut_ids = vect_aut.transform(temp_dev_aut['reviewText'])\n",
    "print(\"number words in training corpus for automobiles:\", len(vect_aut.get_feature_names()))\n",
    "print('automobile dataset id shapes',train_aut_ids.shape,dev_aut_ids.shape)\n",
    "\n",
    "#tokenize train and test text data for home and kitchen\n",
    "train_hnk_ids = vect_hnk.fit_transform(temp_train_hnk['reviewText'])\n",
    "dev_hnk_ids = vect_hnk.transform(temp_dev_hnk['reviewText'])\n",
    "print(\"number words in training corpus for home and kitchen:\", len(vect_hnk.get_feature_names()))\n",
    "print('home and kitchen dataset id shapes',train_hnk_ids.shape,dev_hnk_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "videos dataset using toy count vectorizer, id shapes (100000, 63984) (30000, 63984)\n",
      "autos dataset using toy count vectorizer, id shapes (100000, 63984) (30000, 63984)\n",
      "autos dataset using toy count vectorizer, id shapes (100000, 63984) (30000, 63984)\n",
      "toys dataset using video count vectorizer, id shapes (100000, 98899) (30000, 98899)\n",
      "autos dataset using video count vectorizer, id shapes (100000, 98899) (30000, 98899)\n",
      "toys dataset using autos count vectorizer, id shapes (100000, 59468) (30000, 59468)\n",
      "videos dataset using autos count vectorizer, id shapes (100000, 59468) (30000, 59468)\n"
     ]
    }
   ],
   "source": [
    "#Cross-tokenization(for comparison of accuracy using transfer learning):\n",
    "\n",
    "#tokenize for videos using the count_vect for toys \n",
    "train_toys_vid_ids = vect_toys.transform(temp_train_vid['reviewText'])\n",
    "dev_toys_vid_ids = vect_toys.transform(temp_dev_vid['reviewText'])\n",
    "print('videos dataset using toy count vectorizer, id shapes',train_toys_vid_ids.shape,dev_toys_vid_ids.shape)\n",
    "\n",
    "#tokenize for autos using the count_vect for toys \n",
    "train_toys_aut_ids = vect_toys.transform(temp_train_aut['reviewText'])\n",
    "dev_toys_aut_ids = vect_toys.transform(temp_dev_aut['reviewText'])\n",
    "print('autos dataset using toy count vectorizer, id shapes',train_toys_aut_ids.shape,dev_toys_aut_ids.shape)\n",
    "\n",
    "#tokenize for home and kitchen using the count_vect for toys \n",
    "train_toys_hnk_ids = vect_toys.transform(temp_train_hnk['reviewText'])\n",
    "dev_toys_hnk_ids = vect_toys.transform(temp_dev_hnk['reviewText'])\n",
    "print('autos dataset using toy count vectorizer, id shapes',train_toys_hnk_ids.shape,dev_toys_hnk_ids.shape)\n",
    "\n",
    "#tokenize for toys using the count_vect for videos \n",
    "train_vid_toys_ids = vect_vid.transform(temp_train_toys['reviewText'])\n",
    "dev_vid_toys_ids = vect_vid.transform(temp_dev_toys['reviewText'])\n",
    "print('toys dataset using video count vectorizer, id shapes',train_vid_toys_ids.shape,dev_vid_toys_ids.shape)\n",
    "\n",
    "#tokenize for autos using the count_vect for videos \n",
    "train_vid_aut_ids = vect_vid.transform(temp_train_aut['reviewText'])\n",
    "dev_vid_aut_ids = vect_vid.transform(temp_dev_aut['reviewText'])\n",
    "print('autos dataset using video count vectorizer, id shapes',train_vid_aut_ids.shape,dev_vid_aut_ids.shape)\n",
    "\n",
    "#tokenize for toys using the count_vect for autos \n",
    "train_aut_toys_ids = vect_aut.transform(temp_train_toys['reviewText'])\n",
    "dev_aut_toys_ids = vect_aut.transform(temp_dev_toys['reviewText'])\n",
    "print('toys dataset using autos count vectorizer, id shapes',train_aut_toys_ids.shape,dev_aut_toys_ids.shape)\n",
    "\n",
    "#tokenize for videos using the count_vect for autos \n",
    "train_aut_vid_ids = vect_aut.transform(temp_train_vid['reviewText'])\n",
    "dev_aut_vid_ids = vect_aut.transform(temp_dev_vid['reviewText'])\n",
    "print('videos dataset using autos count vectorizer, id shapes',train_aut_vid_ids.shape,dev_aut_vid_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 24706) (30000, 24706)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(min_df=5, stop_words='english')\n",
    "tfidf_train_toys = tfidf_vect.fit_transform(temp_train_toys['reviewText'])\n",
    "tfidf_dev_toys = tfidf_vect.transform(temp_dev_toys['reviewText'])\n",
    "print(tfidf_train_toys.shape,tfidf_dev_toys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8499\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline prediction for toys using most common class: 84.99%\n",
      "Accuracy on toys dev set for binary prediction with toys naive bayes model: 92.23%\n",
      "Corresponding classification report              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.74      0.74      0.74      4503\n",
      "        1.0       0.95      0.95      0.95     25497\n",
      "\n",
      "avg / total       0.92      0.92      0.92     30000\n",
      "\n",
      "Baseline prediction for video games using most common class:80.92%\n",
      "Accuracy on video games dev set for binary prediction with video games naive bayes model: 89.16%\n",
      "Corresponding classification report              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.71      0.71      5725\n",
      "        1.0       0.93      0.93      0.93     24275\n",
      "\n",
      "avg / total       0.89      0.89      0.89     30000\n",
      "\n",
      "Baseline prediction for autos using most common class:85.59%\n",
      "Accuracy on autos dev set for binary prediction with autos naive bayes model: 91.93%\n",
      "Corresponding classification report              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.61      0.69      4323\n",
      "        1.0       0.94      0.97      0.95     25677\n",
      "\n",
      "avg / total       0.91      0.92      0.92     30000\n",
      "\n",
      "Baseline prediction for home and kitchen using most common class:83.09%\n",
      "Accuracy on home and kitchen dev set for binary prediction with home and kitchen naive bayes model: 91.37%\n",
      "Corresponding classification report              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.71      0.73      5072\n",
      "        1.0       0.94      0.96      0.95     24928\n",
      "\n",
      "avg / total       0.91      0.91      0.91     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Naive bayes for binary prediction\n",
    "# nb_b = MultinomialNB()\n",
    "# nb_b.fit(train_x_csr,train_y_b)\n",
    "# y_pred = nb_b.predict(dev_x_csr)\n",
    "\n",
    "# acc = accuracy_score(dev_y_b, y_pred)\n",
    "# print(\"Accuracy on dev set for binary prediction: {:.02%}\".format(acc))\n",
    "# print('classification report naive bayes binary classification \\n',classification_report(dev_y_b, y_pred))\n",
    "\n",
    "#Naive bayes for binary prediction with count_vectorizer for toys\n",
    "print('Baseline prediction for toys using most common class: {:.02%}'.format(len(temp_dev_toys_y[temp_dev_toys_y==1])/len(temp_dev_toys_y)))\n",
    "nb_toys_b = MultinomialNB()\n",
    "nb_toys_b.fit(train_toys_ids,temp_train_toys_y)\n",
    "y_pred_toys_b_cv = nb_toys_b.predict(dev_toys_ids)\n",
    "\n",
    "acc = accuracy_score(temp_dev_toys_y, y_pred_toys_b_cv)\n",
    "print(\"Accuracy on toys dev set for binary prediction with toys naive bayes model: {:.02%}\".format(acc))\n",
    "print('Corresponding classification report',classification_report(temp_dev_toys_y, y_pred_toys_b_cv))\n",
    "\n",
    "#Naive bayes for binary prediction with count_vectorizer for video games\n",
    "print('Baseline prediction for video games using most common class:{:.02%}'.format(len(temp_dev_vid_y[temp_dev_vid_y==1])/len(temp_dev_vid_y)))\n",
    "nb_vid_b = MultinomialNB()\n",
    "nb_vid_b.fit(train_vid_ids,temp_train_vid_y)\n",
    "y_pred_vid_b_cv = nb_vid_b.predict(dev_vid_ids)\n",
    "\n",
    "acc = accuracy_score(temp_dev_vid_y, y_pred_vid_b_cv)\n",
    "print(\"Accuracy on video games dev set for binary prediction with video games naive bayes model: {:.02%}\".format(acc))\n",
    "print('Corresponding classification report',classification_report(temp_dev_vid_y, y_pred_vid_b_cv))\n",
    "\n",
    "#Naive bayes for binary prediction with count_vectorizer for automobiles\n",
    "print('Baseline prediction for autos using most common class:{:.02%}'.format(len(temp_dev_aut_y[temp_dev_aut_y==1])/len(temp_dev_aut_y)))\n",
    "nb_aut_b = MultinomialNB()\n",
    "nb_aut_b.fit(train_aut_ids,temp_train_aut_y)\n",
    "y_pred_aut_b_cv = nb_aut_b.predict(dev_aut_ids)\n",
    "\n",
    "acc = accuracy_score(temp_dev_aut_y, y_pred_aut_b_cv)\n",
    "print(\"Accuracy on autos dev set for binary prediction with autos naive bayes model: {:.02%}\".format(acc))\n",
    "print('Corresponding classification report',classification_report(temp_dev_aut_y, y_pred_aut_b_cv))\n",
    "\n",
    "#Naive bayes for binary prediction with count_vectorizer for home and kitchen\n",
    "print('Baseline prediction for home and kitchen using most common class:{:.02%}'.format(len(temp_dev_hnk_y[temp_dev_hnk_y==1])/len(temp_dev_hnk_y)))\n",
    "nb_hnk_b = MultinomialNB()\n",
    "nb_hnk_b.fit(train_hnk_ids,temp_train_hnk_y)\n",
    "y_pred_hnk_b_cv = nb_hnk_b.predict(dev_hnk_ids)\n",
    "\n",
    "acc = accuracy_score(temp_dev_hnk_y, y_pred_hnk_b_cv)\n",
    "print(\"Accuracy on home and kitchen dev set for binary prediction with home and kitchen naive bayes model: {:.02%}\".format(acc))\n",
    "print('Corresponding classification report',classification_report(temp_dev_hnk_y, y_pred_hnk_b_cv))\n",
    "\n",
    "#Naive bayes for binary prediction with tfidf \n",
    "# nb_bt = MultinomialNB()\n",
    "# nb_bt.fit(tfidf_train_toys,train_y_b)\n",
    "# y_pred_bt = nb_bt.predict(tfidf_dev_toys)\n",
    "\n",
    "# acc = accuracy_score(dev_y_b, y_pred_bt)\n",
    "# print(\"Accuracy on dev set for binary prediction with tfidf: {:.02%}\".format(acc))\n",
    "# print('classification report naive bayes binary classification with tfidf \\n',classification_report(dev_y_b, y_pred_bt))\n",
    "\n",
    "#Naive bayes for 4 level rating prediction, excluding the 3s\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(train_x_csr,train_y)\n",
    "# y_pred_mult = nb.predict(dev_x_csr)\n",
    "\n",
    "# acc = accuracy_score(dev_y, y_pred_mult)\n",
    "# print(\"Accuracy on dev set for 4 level (1,2,4,5) prediction: {:.02%}\".format(acc))\n",
    "# print('classification report naive bayes multinomial classification with tfidf \\n',classification_report(dev_y, y_pred_mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on video games dev set for binary prediction with toys naive bayes model: 86.99%\n",
      "Corresponding classification report              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.65      0.66      5725\n",
      "        1.0       0.92      0.92      0.92     24275\n",
      "\n",
      "avg / total       0.87      0.87      0.87     30000\n",
      "\n",
      "Accuracy on automobiles dev set for binary prediction with toys naive bayes model: 76.06%\n",
      "Corresponding classification report              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.36      0.88      0.51      4323\n",
      "        1.0       0.97      0.74      0.84     25677\n",
      "\n",
      "avg / total       0.88      0.76      0.79     30000\n",
      "\n",
      "Accuracy on home and kitchen dev set for binary prediction with toys naive bayes model: 85.78%\n",
      "Corresponding classification report              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.85      0.67      5072\n",
      "        1.0       0.97      0.86      0.91     24928\n",
      "\n",
      "avg / total       0.90      0.86      0.87     30000\n",
      "\n",
      "Accuracy on toys dev set for binary prediction with video games naive bayes model: 91.53%\n",
      "Corresponding classification report              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.63      0.69      4503\n",
      "        1.0       0.94      0.97      0.95     25497\n",
      "\n",
      "avg / total       0.91      0.92      0.91     30000\n",
      "\n",
      "Accuracy on automobiles dev set for binary prediction with video games naive bayes model: 80.50%\n",
      "Corresponding classification report              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.41      0.77      0.53      4323\n",
      "        1.0       0.96      0.81      0.88     25677\n",
      "\n",
      "avg / total       0.88      0.81      0.83     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Transfer learning\n",
    "#Naive bayes prediction of video games with toys model\n",
    "y_pred_vidwithtoys_dev = nb_toys_b.predict(dev_toys_vid_ids)\n",
    "acc = accuracy_score(temp_dev_vid_y, y_pred_vidwithtoys_dev)\n",
    "print(\"Accuracy on video games dev set for binary prediction with toys naive bayes model: {:.02%}\".format(acc))\n",
    "print('Corresponding classification report',classification_report(temp_dev_vid_y, y_pred_vidwithtoys_dev))\n",
    "\n",
    "#Naive bayes prediction of autos games with toys model\n",
    "y_pred_autwithtoys_dev = nb_toys_b.predict(dev_toys_aut_ids)\n",
    "acc = accuracy_score(temp_dev_aut_y, y_pred_autwithtoys_dev)\n",
    "print(\"Accuracy on automobiles dev set for binary prediction with toys naive bayes model: {:.02%}\".format(acc))\n",
    "print('Corresponding classification report',classification_report(temp_dev_aut_y, y_pred_autwithtoys_dev))\n",
    "\n",
    "#Naive bayes prediction of home and kitchen games with toys model\n",
    "y_pred_hnkwithtoys_dev = nb_toys_b.predict(dev_toys_hnk_ids)\n",
    "acc = accuracy_score(temp_dev_hnk_y, y_pred_hnkwithtoys_dev)\n",
    "print(\"Accuracy on home and kitchen dev set for binary prediction with toys naive bayes model: {:.02%}\".format(acc))\n",
    "print('Corresponding classification report',classification_report(temp_dev_hnk_y, y_pred_hnkwithtoys_dev))\n",
    "\n",
    "#Naive bayes prediction of toys with videos model\n",
    "y_pred_toyswithvid_dev = nb_vid_b.predict(dev_vid_toys_ids)\n",
    "acc = accuracy_score(temp_dev_toys_y, y_pred_toyswithvid_dev)\n",
    "print(\"Accuracy on toys dev set for binary prediction with video games naive bayes model: {:.02%}\".format(acc))\n",
    "print('Corresponding classification report',classification_report(temp_dev_toys_y, y_pred_toyswithvid_dev))\n",
    "\n",
    "#Naive bayes prediction of autos games with videos model\n",
    "y_pred_autwithvid_dev = nb_vid_b.predict(dev_vid_aut_ids)\n",
    "acc = accuracy_score(temp_dev_aut_y, y_pred_autwithvid_dev)\n",
    "print(\"Accuracy on automobiles dev set for binary prediction with video games naive bayes model: {:.02%}\".format(acc))\n",
    "print('Corresponding classification report',classification_report(temp_dev_aut_y, y_pred_autwithvid_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-01e678972a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Pulling out and printing most positive and negative features - binary prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinear_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnb_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# populate this with actual values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtop_negative_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtop_positive_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nb_b' is not defined"
     ]
    }
   ],
   "source": [
    "#Not updated for multi category\n",
    "#Pulling out and printing most positive and negative features - binary prediction\n",
    "linear_weights = nb_b.feature_log_prob_[1,] - nb_b.feature_log_prob_[0,]  # populate this with actual values\n",
    "top_negative_features = np.argsort(linear_weights)[:20]\n",
    "top_positive_features = np.argsort(linear_weights)[-20:]\n",
    "\n",
    "\n",
    "print(\"Most negative features - binary prediction:\")\n",
    "for idx in top_negative_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))\n",
    "print(\"\")\n",
    "print(\"Most positive features: binary prediction:\")\n",
    "for idx in top_positive_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))\n",
    "    \n",
    "#Pulling out and printing most positive and negative features - 4 way prediction\n",
    "linear_weights = nb.feature_log_prob_[1,] - nb.feature_log_prob_[0,]  # populate this with actual values\n",
    "top_negative_features = np.argsort(linear_weights)[:10]\n",
    "top_positive_features = np.argsort(linear_weights)[-10:]\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Most negative features - (1,2,4,5) prediction:\")\n",
    "for idx in top_negative_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))\n",
    "print(\"\")\n",
    "print(\"Most positive features: (1,2,4,5) prediction:\")\n",
    "for idx in top_positive_features:\n",
    "    print(\"  {:s} ({:.02f})\".format(vocab.id_to_word[idx], \n",
    "                                    linear_weights[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# instantiate and train SVM model, kernel=rbf \n",
    "\n",
    "svm_model = svm.SVC()\n",
    "start =time.time()\n",
    "\n",
    "svm_model.fit(tfidf_train_toys,train_y_b)\n",
    "\n",
    "# evaulate model\n",
    "y_pred_svm = svm_model.predict(tfidf_dev_toys)\n",
    "\n",
    "acc = accuracy_score(dev_y_b, y_pred_svm)\n",
    "print(\"Accuracy on dev set for binary prediction: {:.02%}\".format(acc))\n",
    "\n",
    "#target_names = ['polarity 0', 'polarity 1']\n",
    "print('classification report svm',classification_report(dev_y_b, y_pred_svm))\n",
    "stop = time.time()\n",
    "print('time taken for SVM', stop-start)\n",
    "\n",
    "#Does not look like the negative class is correct. Is it because of highly unbalanced negative class.\n",
    "#Lookup http://scikit-learn.org/stable/modules/svm.html#unbalanced-problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping track of results from test runs\n",
    "With number in train set = 10000 (excl 3 ratings)    \n",
    "    Accuracy on dev set for binary prediction: 88.74%\n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 67.16%\n",
    "    Vocab Size : 38696\n",
    "    \n",
    "With number in train set = 50000 (excl 3 ratings)   \n",
    "    Accuracy on dev set for binary prediction: 91.33%   \n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 69.33% \n",
    "    Vocab Size : ~ ..\n",
    "    \n",
    "With number in train set = 100000 (excl 3 ratings)\n",
    "    Accuracy on dev set for binary prediction: 91.56%   \n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 70.42%\n",
    "    Vocab Size : 105304\n",
    "\n",
    "With number in train set = 500000, dev set = 150000 (excl 3 ratings)    \n",
    "    Accuracy on dev set for binary prediction: 91.73%\n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 70.95%\n",
    "    vocab size 307822\n",
    "    \n",
    "With number in train set = 1200000, dev set = 360000 (excl 3 ratings)    \n",
    "    Accuracy on dev set for binary prediction: 91.92%\n",
    "    Accuracy on dev set for 4 level (1,2,4,5) prediction: 71.24%\n",
    "    vocab size 674074 (not repeated with correction for vocab)\n",
    "    \n",
    "### Output from trying different pre-processing with the toys review set.\n",
    " \n",
    " Accuracy on dev set for binary prediction: 91.69%\n",
    "classification report naive bayes binary classification \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.70      0.77      0.74     22472\n",
    "        1.0       0.96      0.94      0.95    127528\n",
    "\n",
    "avg / total       0.92      0.92      0.92    150000\n",
    "\n",
    "Accuracy on dev set for binary prediction with count vectorizer: 91.92%\n",
    "classification report naive bayes binary classification with count vectorizer \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.71      0.79      0.75     22472\n",
    "        1.0       0.96      0.94      0.95    127528\n",
    "\n",
    "avg / total       0.92      0.92      0.92    150000\n",
    "\n",
    "Accuracy on dev set for binary prediction with tfidf: 90.13%\n",
    "classification report naive bayes binary classification with tfidf \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.90      0.38      0.54     22472\n",
    "        1.0       0.90      0.99      0.94    127528\n",
    "\n",
    "avg / total       0.90      0.90      0.88    150000\n",
    "\n",
    "Accuracy on dev set for 4 level (1,2,4,5) prediction: 70.91%\n",
    "classification report naive bayes multinomial classification with tfidf \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          1       0.60      0.74      0.66     13975\n",
    "          2       0.32      0.05      0.09      8497\n",
    "          4       0.42      0.34      0.37     29733\n",
    "          5       0.80      0.87      0.83     97795\n",
    "\n",
    "avg / total       0.68      0.71      0.68    150000\n",
    "\n",
    "### Output from simple ratings prediction with video games review set.\n",
    "\n",
    "train set size : 10000, dev set size : 3000\n",
    "Accuracy on dev set for binary prediction with count vectorizer: 88.93%\n",
    "classification report naive bayes binary classification with count vectorizer \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.77      0.54      0.64       534\n",
    "        1.0       0.91      0.96      0.93      2466\n",
    "\n",
    "avg / total       0.88      0.89      0.88      3000\n",
    "\n",
    "Accuracy on dev set for binary prediction with tfidf: 84.93%\n",
    "classification report naive bayes binary classification with tfidf \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.95      0.16      0.28       534\n",
    "        1.0       0.85      1.00      0.92      2466\n",
    "\n",
    "avg / total       0.86      0.85      0.80      3000\n",
    "\n",
    "Using SVM, with Count Vectorizer pre-processing:\n",
    "Accuracy on dev set for binary prediction: 82.20%\n",
    "classification report svm              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.00      0.00      0.00       534\n",
    "        1.0       0.82      1.00      0.90      2466\n",
    "\n",
    "avg / total       0.68      0.82      0.74      3000\n",
    "\n",
    "time taken for SVM 48.42102265357971\n",
    "\n",
    "Using SVM with TFIDF pre-processing:\n",
    "Accuracy on dev set for binary prediction: 82.20%\n",
    "classification report svm              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.00      0.00      0.00       534\n",
    "        1.0       0.82      1.00      0.90      2466\n",
    "\n",
    "avg / total       0.68      0.82      0.74      3000\n",
    "\n",
    "train set size : 100000, dev set size : 30000\n",
    "Accuracy on dev set for binary prediction with count vectorizer: 89.12%\n",
    "classification report naive bayes binary classification with count vectorizer \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.72      0.71      0.71      5728\n",
    "        1.0       0.93      0.93      0.93     24272\n",
    "\n",
    "avg / total       0.89      0.89      0.89     30000\n",
    "\n",
    "Accuracy on dev set for binary prediction with tfidf: 86.04%\n",
    "classification report naive bayes binary classification with tfidf \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.91      0.30      0.45      5728\n",
    "        1.0       0.86      0.99      0.92     24272\n",
    "\n",
    "avg / total       0.87      0.86      0.83     30000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for transfer learning from toys to video games\n",
    "number words in training corpus for toys: 63984\n",
    "toys dataset id shapes (100000, 63984) (30000, 63984)\n",
    "number words in training corpus for video games: 98899\n",
    "videos dataset id shapes (100000, 98899) (30000, 98899)\n",
    "number words in training corpus for automobiles: 59468\n",
    "automobile dataset id shapes (100000, 59468) (30000, 59468)\n",
    "number words in training corpus for home and kitchen: 57884\n",
    "home and kitchen dataset id shapes (100000, 57884) (30000, 57884)\n",
    "\n",
    "Accuracy on toys dev set for binary prediction with toys naive bayes model: 92.23%   \n",
    "Corresponding classification report              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.74      0.74      0.74      4503\n",
    "        1.0       0.95      0.95      0.95     25497\n",
    "\n",
    "avg / total       0.92      0.92      0.92     30000\n",
    "\n",
    "Accuracy on video games dev set for binary prediction with video games naive bayes model: 89.16%   \n",
    "Corresponding classification report              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.72      0.71      0.71      5725\n",
    "        1.0       0.93      0.93      0.93     24275\n",
    "\n",
    "avg / total       0.89      0.89      0.89     30000\n",
    "\n",
    "Accuracy on autos dev set for binary prediction with autos naive bayes model: 91.93%   \n",
    "Corresponding classification report              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.78      0.61      0.69      4323\n",
    "        1.0       0.94      0.97      0.95     25677\n",
    "\n",
    "avg / total       0.91      0.92      0.92     30000\n",
    "\n",
    "Accuracy on home and kitchen dev set for binary prediction with home and kitchen naive bayes model: 91.37%   \n",
    "Corresponding classification report              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.76      0.71      0.73      5072\n",
    "        1.0       0.94      0.96      0.95     24928\n",
    "\n",
    "avg / total       0.91      0.91      0.91     30000\n",
    "\n",
    "### Transfer learning:\n",
    "\n",
    "Accuracy on video games dev set for binary prediction with toys naive bayes model: 86.99%   \n",
    "Corresponding classification report              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.66      0.65      0.66      5725\n",
    "        1.0       0.92      0.92      0.92     24275\n",
    "\n",
    "avg / total       0.87      0.87      0.87     30000\n",
    "\n",
    "Accuracy on automobiles dev set for binary prediction with toys naive bayes model: 76.06%   \n",
    "Corresponding classification report              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.36      0.88      0.51      4323\n",
    "        1.0       0.97      0.74      0.84     25677\n",
    "\n",
    "avg / total       0.88      0.76      0.79     30000\n",
    "\n",
    "Accuracy on home and kitchen dev set for binary prediction with toys naive bayes model: 85.78%   \n",
    "Corresponding classification report              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.55      0.85      0.67      5072\n",
    "        1.0       0.97      0.86      0.91     24928\n",
    "\n",
    "avg / total       0.90      0.86      0.87     30000\n",
    "\n",
    "Accuracy on toys dev set for binary prediction with video games naive bayes model: 91.53%   \n",
    "Corresponding classification report              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.76      0.63      0.69      4503\n",
    "        1.0       0.94      0.97      0.95     25497\n",
    "\n",
    "avg / total       0.91      0.92      0.91     30000\n",
    "\n",
    "Accuracy on automobiles dev set for binary prediction with video games naive bayes model: 80.50%   \n",
    "Corresponding classification report              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.41      0.77      0.53      4323\n",
    "        1.0       0.96      0.81      0.88     25677\n",
    "\n",
    "avg / total       0.88      0.81      0.83     30000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
